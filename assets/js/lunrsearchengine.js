
var documents = [{
    "id": 0,
    "url": "https://bright-softwares.com/about",
    "title": "Mediumish Template for Jekyll",
    "body": "This website is built with Jekyll and Mediumish template for Jekyll. It's for demonstration purposes, no real content can be found. Mediumish template for Jekyll is compatible with Github pages, in fact even this demo is created with Github Pages and hosted with Github.  Documentation: Please, read the docs here. Questions or bug reports?: Head over to our Github repository! Buy me a coffeeThank you for your support! Your donation helps me to maintain and improve Mediumish . Buy me a coffee Documentation"
    }, {
    "id": 1,
    "url": "https://bright-softwares.com/.well-known/assetlinks.json",
    "title": "",
    "body": "[{   “relation”: [“delegate_permission/common. handle_all_urls”],   “target” : { “namespace”: “android_app”, “package_name”: “com. bright_softwares. twa”,          “sha256_cert_fingerprints”: [“60:76:28:70:C6:95:C8:08:B4:C4:FD:87:CD:0F:FD:CE:8A:89:24:26:3E:B9:D4:F7:99:A1:DF:C5:B0:E9:90:29”] }  }] "
    }, {
    "id": 2,
    "url": "https://bright-softwares.com/en/blog.html",
    "title": "Blog",
    "body": ""
    }, {
    "id": 3,
    "url": "https://bright-softwares.com/fr/blog.html",
    "title": "Blog",
    "body": ""
    }, {
    "id": 4,
    "url": "https://bright-softwares.com/blog",
    "title": "Blog",
    "body": ""
    }, {
    "id": 5,
    "url": "https://bright-softwares.com/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 6,
    "url": "https://bright-softwares.com/conditions",
    "title": "Conditions Générales de vente et d'utilisation",
    "body": "CONDITIONS GÉNÉRALES DE VENTE ET D’UTILISATIONVUE D’ENSEMBLE: Ce site web est exploité par Bright Softwares. Sur ce site, les termes “nous”, “notre” et “nos” font référence à Bright Softwares. Bright Softwares propose ce site web, y compris toutes les informations, tous les outils et tous les services qui y sont disponibles pour vous, l’utilisateur, sous réserve de votre acceptation de l’ensemble des modalités, conditions, politiques et avis énoncés ici. En visitant ce site et/ou en achetant un de nos produits, vous vous engagez dans notre “Service” et acceptez d’être lié par les modalités suivantes (“Conditions Générales de Vente”, “Conditions Générales de Vente et d’Utilisation”, “Conditions”), y compris les modalités, conditions et politiques additionnelles auxquelles il est fait référence ici et/ou accessibles par hyperlien. Ces Conditions Générales de Vente et d’Utilisation s’appliquent à tous les utilisateurs de ce site, incluant mais ne se limitant pas, aux utilisateurs qui naviguent sur le site, qui sont des vendeurs, des clients, des marchands, et/ou des contributeurs de contenu. Veuillez lire attentivement ces Conditions Générales de Vente et d’Utilisation avant d’accéder à ou d’utiliser notre site web. En accédant à ou en utilisant une quelconque partie de ce site, vous acceptez d’être lié par ces Conditions Générales de Vente et d’Utilisation. Si vous n’acceptez pas toutes les modalités et toutes les Conditions de cet accord, alors vous ne devez pas accéder au site web ni utiliser les services qui y sont proposés. Si ces Conditions Générales de Vente et d’Utilisation sont considérées comme une offre, l’acceptation se limite expressément à ces Conditions Générales de Vente et d’Utilisation. Toutes les nouvelles fonctionnalités et tous les nouveaux outils qui seront ajoutés ultérieurement à cette boutique seront également assujettis à ces Conditions Générales de Vente et d’Utilisation. Vous pouvez consulter la version la plus récente des Conditions Générales de Vente et d’Utilisation à tout moment sur cette page. Nous nous réservons le droit de mettre à jour, de changer ou de remplacer toute partie de ces Conditions Générales de Vente et d’Utilisation en publiant les mises à jour et/ou les modifications sur notre site web. Il vous incombe de consulter cette page régulièrement pour vérifier si des modifications ont été apportées. Votre utilisation continue du site web ou votre accès à celui-ci après la publication de toute modification constitue une acception de votre part de ces modifications. Notre boutique est hébergée sur Shopify Inc. Ils nous fournissent la plate-forme e-commerce qui nous permet de vous vendre nos produits et services. ARTICLE 1 – CONDITIONS D’UTILISATION DE NOTRE BOUTIQUE EN LIGNE: En acceptant ces Conditions Générales de Vente et d’Utilisation, vous déclarez que vous avez atteint l’âge de la majorité dans votre pays, État ou province de résidence, et que vous nous avez donné votre consentement pour permettre à toute personne d’âge mineur à votre charge d’utiliser ce site web. L’utilisation de nos produits à toute fin illégale ou non autorisée est interdite, et vous ne devez pas non plus, dans le cadre de l’utilisation du Service, violer les lois de votre juridiction (incluant mais ne se limitant pas aux lois relatives aux droits d’auteur). Vous ne devez pas transmettre de vers, de virus ou tout autre code de nature destructive. Toute infraction ou violation des présentes Conditions Générales de Vente et d’Utilisation entraînera la résiliation immédiate de vos Services. ARTICLE 2 – CONDITIONS GÉNÉRALES: Nous nous réservons le droit de refuser à tout moment l’accès aux services à toute personne, et ce, pour quelque raison que ce soit. Vous comprenez que votre contenu (à l’exclusion de vos informations de carte de crédit) pourrait être transféré de manière non chiffrée, et cela sous-entend (a) des transmissions sur divers réseaux; et (b) des changements pour se conformer et s’adapter aux exigences techniques pour la connexion des réseaux ou appareils. Les informations de carte de crédit sont toujours chiffrées pendant la transmission sur les réseaux. Vous acceptez de ne pas reproduire, dupliquer, copier, vendre, revendre ou exploiter une quelconque partie du Service ou utilisation du Service, ou un quelconque accès au Service ou contact sur le site web, par le biais duquel le Service est fourni, sans autorisation écrite expresse préalable de notre part. Les titres utilisés dans cet accord sont inclus pour votre commodité, et ne vont ni limiter ni affecter ces Conditions. ARTICLE 3 – EXACTITUDE, EXHAUSTIVITÉ ET ACTUALITÉ DES INFORMATIONS: Nous ne sommes pas responsables si les informations disponibles sur ce site ne sont pas exactes, complètes ou à jour. Le contenu de ce site est fourni à titre indicatif uniquement et ne devrait pas constituer votre seule source d’information pour prendre des décisions, sans consulter au préalable des sources d’information plus exactes, plus complètes et actualisées. Si vous décidez de vous fier au contenu présenté sur ce site, vous le faites à votre propre risque. Ce site pourrait contenir certaines informations antérieures. Ces informations antérieures, par nature, ne sont pas à jour et sont fournies à titre indicatif seulement. Nous nous réservons le droit de modifier le contenu de ce site à tout moment, mais nous n’avons aucune obligation de mettre à jour les informations sur notre site. Vous acceptez qu’il vous incombe de surveiller les modifications apportées à notre site. ARTICLE 4 – MODIFICATIONS APPORTÉES AU SERVICE ET AUX PRIX: Les prix de nos produits peuvent être modifiés sans préavis. Nous nous réservons le droit à tout moment de modifier ou d’interrompre le Service (ainsi que toute partie ou tout contenu du Service) sans préavis et en tout temps. Nous ne serons pas responsables envers vous ou toute autre tierce partie de toute modification de prix, suspension ou interruption du Service. ARTICLE 5 – PRODUITS OU SERVICES (le cas échéant): Certains produits ou services peuvent être exclusivement disponibles en ligne sur notre site web. Ces produits ou services peuvent être disponibles en quantités limitées et peuvent uniquement faire l’objet de retours ou d’échanges conformément à notre Politique de Retour. Nous avons fait de notre mieux pour afficher aussi clairement que possible les couleurs et images de nos produits qui apparaissent sur notre boutique. Nous ne pouvons pas garantir que l’affichage des couleurs par l’écran de votre ordinateur sera précis. Nous nous réservons le droit, sans toutefois être obligés de le faire, de limiter les ventes de nos produits ou services à toute personne, et dans toute région géographique ou juridiction. Nous pourrions exercer ce droit au cas par cas. Nous nous réservons le droit de limiter les quantités de tout produit ou service que nous offrons. Toutes les descriptions de produits et tous les prix des produits peuvent être modifiés en tout temps sans avis préalable, à notre seule discrétion. Nous nous réservons le droit d’arrêter d’offrir un produit à tout moment. Toute offre de service ou de produit présentée sur ce site est nulle là où la loi l’interdit. Nous ne garantissons pas que la qualité de tous les produits, services, informations, ou toute autre marchandise que vous avez obtenue ou achetée répondra à vos attentes, ni que toute erreur dans le Service sera corrigée. ARTICLE 6 – EXACTITUDE DE LA FACTURATION ET DES INFORMATIONS DE COMPTE: Nous nous réservons le droit de refuser toute commande que vous passez auprès de nous. Nous pourrions, à notre seule discrétion, réduire ou annuler les quantités achetées par personne, par foyer ou par commande. Ces restrictions pourraient inclure des commandes passées par ou depuis le même compte client, la même carte de crédit, et/ou des commandes qui utilisent la même adresse de facturation et/ou d’expédition. Dans le cas où nous modifierions une commande ou si nous venions à l’annuler, nous pourrions tenter de vous avertir en vous contactant à l’e-mail et/ou à l’adresse de facturation/au numéro de téléphone fourni au moment où la commande a été passée. Nous nous réservons le droit de limiter ou d’interdire les commandes qui, à notre seul jugement, pourraient sembler provenir de marchands, de revendeurs ou de distributeurs. Vous acceptez de fournir des informations de commande et de compte à jour, complètes et exactes pour toutes les commandes passées sur notre boutique. Vous vous engagez à mettre à jour rapidement votre compte et vos autres informations, y compris votre adresse e-mail, vos numéros de cartes de crédit et dates d’expiration, pour que nous poussions compléter vos transactions et vous contacter si nécessaire. Pour plus de détails, veuillez consulter notre Politique de Retour. ARTICLE 7 – OUTILS FACULTATIFS: Nous pourrions vous fournir l’accès à des outils de tierces parties sur lesquels nous n’exerçons ni suivi, ni contrôle, ni influence. Vous reconnaissez et acceptez le fait que nous fournissons l’accès à de tels outils “tels quels” et “selon la disponibilité”, sans aucune garantie, représentation ou condition d’aucune sorte et sans aucune approbation. Nous n’aurons aucune responsabilité légale résultant de ou liée à l’utilisation de ces outils facultatifs de tiers. Si vous utilisez les outils facultatifs offerts sur le site, vous le faites à votre propre risque et à votre propre discrétion, et vous devriez consulter les conditions auxquelles ces outils sont offerts par le ou les fournisseurs tiers concerné(s). Nous pourrions aussi, à l’avenir, offrir de nouveaux services et/ou de nouvelles fonctionnalités sur notre site (incluant de nouveaux outils et de nouvelles ressources). Ces nouvelles fonctionnalités et ces nouveaux services seront également assujettis à ces Conditions Générales de Vente et d’Utilisation. ARTICLE 8 – LIENS DE TIERS: Certains contenus, produits et services disponibles par le biais de notre Service pourraient inclure des éléments provenant de tierces parties. Les liens provenant de tierces parties sur ce site pourraient vous rediriger vers des sites web de tiers qui ne sont pas affiliés à nous. Nous ne sommes pas tenus d’examiner ou d’évaluer le contenu ou l’exactitude de ces sites, et nous ne garantissons pas et n’assumons aucune responsabilité quant à tout contenu, site web, produit, service ou autre élément accessible sur ou depuis ces sites tiers. Nous ne sommes pas responsables des préjudices ou dommages liés à l’achat ou à l’utilisation de biens, de services, de ressources, de contenu, ou de toute autre transaction effectuée en rapport avec ces sites web de tiers. Veuillez lire attentivement les politiques et pratiques des tierces parties et assurez-vous de bien les comprendre avant de vous engager dans toute transaction. Les plaintes, réclamations, préoccupations, ou questions concernant les produits de ces tiers doivent être soumises à ces mêmes tiers. ARTICLE 9 – COMMENTAIRES, SUGGESTIONS ET AUTRES PROPOSITIONS D’UTILISATEURS: Si, à notre demande, vous soumettez des contenus spécifiques (par exemple, pour participer à des concours), ou si sans demande de notre part, vous envoyez des idées créatives, des suggestions, des propositions, des plans ou d’autres éléments, que ce soit en ligne, par e-mail, par courrier, ou autrement (collectivement, “commentaires”), vous nous accordez le droit, en tout temps, et sans restriction, d’éditer, de copier, de publier, de distribuer, de traduire et d’utiliser autrement et dans tout média tout commentaire que vous nous envoyez. Nous ne sommes pas et ne devrons pas être tenus (1) de maintenir la confidentialité des commentaires; (2) de payer une compensation à quiconque pour tout commentaire fourni; (3) de répondre aux commentaires. Nous pourrions, mais n’avons aucune obligation de le faire, surveiller, modifier ou supprimer le contenu que nous estimons, à notre seule discrétion, être illégal, offensant, menaçant, injurieux, diffamatoire, pornographique, obscène ou autrement répréhensible, ou qui enfreint toute propriété intellectuelle ou ces Conditions Générales de Vente et d’Utilisation. Vous vous engagez à écrire des commentaires qui ne violent pas les droits de tierces parties, y compris les droits d’auteur, les marques déposées, la confidentialité, la personnalité, ou d’autres droits personnels ou de propriété. Vous convenez également que vos commentaires ne contiendront pas de contenu illégal, diffamatoire, offensif ou obscène, et qu’ils ne contiendront non plus pas de virus informatique ou d’autres logiciels malveillants qui pourraient affecter de quelque manière que ce soit le fonctionnement du Service ou tout autre site web associé. Vous ne pouvez pas utiliser de fausse adresse e-mail, prétendre être quelqu’un que vous n’êtes pas, ou essayer de nous induire nous et/ou les tierces parties en erreur quant à l’origine de vos commentaires. Vous êtes entièrement responsable de tous les commentaires que vous publiez ainsi que de leur exactitude. Nous n’assumons aucune responsabilité et déclinons tout engagement quant à tout commentaire que vous publiez ou que toute autre tierce partie publie. ARTICLE 10 – RENSEIGNEMENTS PERSONNELS: La soumission de vos renseignements personnels sur notre boutique est régie par notre Politique de Confidentialité. Cliquez ici pour consulter notre Politique de Confidentialité. ARTICLE 11 – ERREURS, INEXACTITUDES ET OMISSIONS: Il se pourrait qu’il y ait parfois des informations sur notre site ou dans le Service qui pourraient contenir des erreurs typographiques, des inexactitudes ou des omissions qui pourraient être relatives aux descriptions de produits, aux prix, aux promotions, aux offres, aux frais d’expédition des produits, aux délais de livraison et à la disponibilité. Nous nous réservons le droit de corriger toute erreur, inexactitude, omission, et de changer ou de mettre à jour des informations ou d’annuler des commandes, si une quelconque information dans le Service ou sur tout autre site web associé est inexacte, et ce, en tout temps et sans préavis (y compris après que vous ayez passé votre commande). Nous ne sommes pas tenus de mettre à jour, de modifier ou de clarifier les informations dans le Service ou sur tout autre site web associé, incluant mais ne se limitant pas aux informations sur les prix, sauf si requis par la loi. Aucune date définie de mise à jour ou d’actualisation dans le Service ou sur tout autre site web associé ne devrait être prise en compte pour conclure que les informations dans le Service ou sur tout autre site web associé ont été modifiées ou mises à jour. ARTICLE 12 – UTILISATIONS INTERDITES: En plus des interdictions énoncées dans les Conditions Générales de Vente et d’Utilisation, il vous est interdit d’utiliser le site ou son contenu: (a) à des fins illégales; (b) pour inciter des tiers à réaliser des actes illégaux ou à y prendre part; (c) pour enfreindre toute ordonnance régionale ou toute loi, règle ou régulation internationale, fédérale, provinciale ou étatique; (d) pour porter atteinte à ou violer nos droits de propriété intellectuelle ou ceux de tierces parties; (e) pour harceler, maltraiter, insulter, blesser, diffamer, calomnier, dénigrer, intimider ou discriminer quiconque en fonction du sexe, de l’orientation sexuelle, de la religion, de l’origine ethnique, de la race, de l’âge, de l’origine nationale, ou d’un handicap; (f) pour soumettre des renseignements faux ou trompeurs; (g) pour téléverser ou transmettre des virus ou tout autre type de code malveillant qui sera ou pourrait être utilisé de manière à compromettre la fonctionnalité ou le fonctionnement du Service ou de tout autre site web associé, indépendant, ou d’Internet; (h) pour recueillir ou suivre les renseignements personnels d’autrui; (i) pour polluposter, hameçonner, détourner un domaine, extorquer des informations, parcourir, explorer ou balayer le web (ou toute autre ressource); (j) à des fins obscènes ou immorales; ou (k) pour porter atteinte ou contourner les mesures de sécurité de notre Service, de tout autre site web, ou d’Internet. Nous nous réservons le droit de résilier votre utilisation du Service ou de tout site web connexe pour avoir enfreint les utilisations interdites. ARTICLE 13 – EXCLUSION DE GARANTIES ET LIMITATION DE RESPONSABILITÉ: Nous ne garantissons ni ne prétendons en aucun cas que votre utilisation de notre Service sera ininterrompue, rapide, sécurisée ou sans erreur. Nous ne garantissons pas que les résultats qui pourraient être obtenus par le biais de l’utilisation du Service seront exacts ou fiables. Vous acceptez que de temps à autre, nous puissions supprimer le Service pour des périodes de temps indéfinies ou annuler le Service à tout moment, sans vous avertir au préalable. Vous convenez expressément que votre utilisation du Service, ou votre incapacité à utiliser celui-ci, est à votre seul risque. Le Service ainsi que tous les produits et services qui vous sont fournis par le biais du Service sont (sauf mention expresse du contraire de notre part) fournis “tels quels” et “selon la disponibilité” pour votre utilisation, et ce sans représentation, sans garanties et sans conditions d’aucune sorte, expresses ou implicites, y compris toutes les garanties implicites de commercialisation ou de qualité marchande, d’adaptation à un usage particulier, de durabilité, de titre et d’absence de contrefaçon. Bright Softwares, nos directeurs, responsables, employés, sociétés affiliées, agents, contractants, stagiaires, fournisseurs, prestataires de services et concédants ne peuvent en aucun cas être tenus responsables de toute blessure, perte, réclamation, ou de dommages directs, indirects, accessoires, punitifs, spéciaux, ou dommages consécutifs de quelque nature qu’ils soient, incluant mais ne se limitant pas à la perte de profits, de revenus, d’économies, de données, aux coûts de remplacement ou tous dommages similaires, qu’ils soient contractuels, délictuels (même en cas de négligence), de responsabilité stricte ou autre, résultant de votre utilisation de tout service ou produit provenant de ce Service, ou quant à toute autre réclamation liée de quelque manière que ce soit à votre utilisation du Service ou de tout produit, incluant mais ne se limitant à toute erreur ou omission dans tout contenu, ou à toute perte ou tout dommage de toute sorte découlant de l’utilisation du Service ou de tout contenu (ou produit) publié, transmis, ou autrement rendu disponible par le biais du Service, même si vous avez été avertis de la possibilité qu’ils surviennent. Parce que certains États ou certaines juridictions ne permettent pas d’exclure ou de limiter la responsabilité quant aux dommages consécutifs ou accessoires, notre responsabilité sera limitée dans la mesure maximale permise par la loi. ARTICLE 14 – INDEMNISATION: Vous acceptez d’indemniser, de défendre et de protéger Bright Softwares, notre société-mère, nos filiales, sociétés affiliées, partenaires, responsables, directeurs, agents, contractants, concédants, prestataires de services, sous-traitants, fournisseurs, stagiaires et employés, quant à toute réclamation ou demande, incluant les honoraires raisonnables d’avocat, faite par toute tierce partie à cause de ou découlant de votre violation de ces Conditions Générales de Vente et d’Utilisation ou des documents auxquels ils font référence, ou de votre violation de toute loi ou des droits d’un tiers. ARTICLE 15 – DISSOCIABILITÉ: Dans le cas où une disposition des présentes Conditions Générales de Vente et d’Utilisation serait jugée comme étant illégale, nulle ou inapplicable, cette disposition pourra néanmoins être appliquée dans la pleine mesure permise par la loi, et la partie non applicable devra être considérée comme étant dissociée de ces Conditions Générales de Vente et d’Utilisation, cette dissociation ne devra pas affecter la validité et l’applicabilité de toutes les autres dispositions restantes. ARTICLE 16 – RÉSILIATION: Les obligations et responsabilités engagées par les parties avant la date de résiliation resteront en vigueur après la résiliation de cet accord et ce à toutes les fins. Ces Conditions Générales de Vente et d’Utilisation sont effectives à moins et jusqu’à ce qu’elles soient résiliées par ou bien vous ou non. Vous pouvez résilier ces Conditions Générales de Vente et d’Utilisation à tout moment en nous avisant que vous ne souhaitez plus utiliser nos Services, ou lorsque vous cessez d’utiliser notre site. Si nous jugeons, à notre seule discrétion, que vous échouez, ou si nous soupçonnons que vous avez été incapable de vous conformer aux modalités de ces Conditions Générales de Vente et d’Utilisation, nous pourrions aussi résilier cet accord à tout moment sans vous prévenir à l’avance et vous resterez responsable de toutes les sommes redevables jusqu’à la date de résiliation (celle-ci étant incluse), et/ou nous pourrions vous refuser l’accès à nos Services (ou à toute partie de ceux-ci). ARTICLE 17 – INTÉGRALITÉ DE L’ACCORD: Tout manquement de notre part à l’exercice ou à l’application de tout droit ou de toute disposition des présentes Conditions Générales de Vente et d’Utilisation ne devrait pas constituer une renonciation à ce droit ou à cette disposition. Ces Conditions Générales de Vente et d’Utilisation ou toute autre politique ou règle d’exploitation que nous publions sur ce site ou relativement au Service constituent l’intégralité de l’entente et de l’accord entre vous et nous et régissent votre utilisation du Service, et remplacent toutes les communications, propositions et tous les accords, antérieurs et contemporains, oraux ou écrits, entre vous et nous (incluant, mais ne se limitant pas à toute version antérieure des Conditions Générales de Vente et d’Utilisation). Toute ambiguïté quant à l’interprétation de ces Conditions Générales de Vente et d’Utilisation ne doit pas être interprétée en défaveur de la partie rédactrice. ARTICLE 18 – LOI APPLICABLE: Ces Conditions Générales de Vente et d’Utilisation, ainsi que tout autre accord séparé par le biais duquel nous vous fournissons des Services seront régis et interprétés en vertu des lois en vigueur à 92 Rue Jeanne d’Arc, Paris, J, 75013, France. ARTICLE 19 – MODIFICATIONS APPORTÉES AUX CONDITIONS GÉNÉRALES DE VENTE ET D’UTILISATION: Vous pouvez consulter la version la plus récente des Conditions Générales de Vente et d’Utilisation à tout moment sur cette page. Nous nous réservons le droit, à notre seule discrétion, de mettre à jour, de modifier ou de remplacer toute partie de ces Conditions Générales de Vente et d’Utilisation en publiant les mises à jour et les changements sur notre site. Il vous incombe de visiter notre site régulièrement pour vérifier si des changements ont été apportés. Votre utilisation continue de ou votre accès à notre site après la publication de toute modification apportée à ces Conditions Générales de Vente et d’Utilisation constitue une acceptation de ces modifications. ARTICLE 20 – COORDONNÉES: Les questions concernant les Conditions Générales de Vente et d’Utilisation devraient nous être envoyées à ofijojo@yopmail. com. "
    }, {
    "id": 7,
    "url": "https://bright-softwares.com/fr/",
    "title": "",
    "body": ""
    }, {
    "id": 8,
    "url": "https://bright-softwares.com/en/",
    "title": "",
    "body": ""
    }, {
    "id": 9,
    "url": "https://bright-softwares.com/legal",
    "title": "Mentions Légales",
    "body": "Mentions Légales1 - Édition du site: En vertu de l’article 6 de la loi n° 2004-575 du 21 juin 2004 pour la confiance dans l’économie numérique, il est précisé aux utilisateurs du site internet bright-softwares. com l’identité des différents intervenants dans le cadre de sa réalisation et de son suivi: Propriétaire du site : Bright Softwares - Contact : ofijojo@yopmail. com - Adresse : 92 Rue Jeanne d’Arc, Paris 13e. Identification de l’entreprise : Bright Softwares au capital social de € - SIREN : - RCS ou RM : - Adresse postale : 92 Rue Jeanne d’Arc, Paris 13e. https://bright-softwares. com/conditions Directeur de la publication : - Contact : . Hébergeur : AutreGitHub pages Mountain view, California Délégué à la protection des données : - Autres contributeurs : Les mentions légales sont proposées par le générateur de mentions légales de La Webeuse. 2 - Propriété intellectuelle et contrefaçons. : Bright Softwares est propriétaire des droits de propriété intellectuelle et détient les droits d’usage sur tous les éléments accessibles sur le site internet, notamment les textes, images, graphismes, logos, vidéos, architecture, icônes et sons. Toute reproduction, représentation, modification, publication, adaptation de tout ou partie des éléments du site, quel que soit le moyen ou le procédé utilisé, est interdite, sauf autorisation écrite préalable de Bright Softwares. Toute exploitation non autorisée du site ou de l’un quelconque des éléments qu’il contient sera considérée comme constitutive d’une contrefaçon et poursuivie conformément aux dispositions des articles L. 335-2 et suivants du Code de Propriété Intellectuelle. 3 - Limitations de responsabilité. : Bright Softwares ne pourra être tenu pour responsable des dommages directs et indirects causés au matériel de l’utilisateur, lors de l’accès au site bright-softwares. com. Bright Softwares décline toute responsabilité quant à l’utilisation qui pourrait être faite des informations et contenus présents sur bright-softwares. com. Bright Softwares s’engage à sécuriser au mieux le site bright-softwares. com, cependant sa responsabilité ne pourra être mise en cause si des données indésirables sont importées et installées sur son site à son insu. Des espaces interactifs (espace contact ou commentaires) sont à la disposition des utilisateurs. Bright Softwares se réserve le droit de supprimer, sans mise en demeure préalable, tout contenu déposé dans cet espace qui contreviendrait à la législation applicable en France, en particulier aux dispositions relatives à la protection des données. Le cas échéant, Bright Softwares se réserve également la possibilité de mettre en cause la responsabilité civile et/ou pénale de l’utilisateur, notamment en cas de message à caractère raciste, injurieux, diffamant, ou pornographique, quel que soit le support utilisé (texte, photographie …). 4 - CNIL et gestion des données personnelles. : Conformément aux dispositions de la loi 78-17 du 6 janvier 1978 modifiée, l’utilisateur du site bright-softwares. com dispose d’un droit d’accès, de modification et de suppression des informations collectées. Pour exercer ce droit, envoyez un message à notre Délégué à la Protection des Données : - . Pour plus d’informations sur la façon dont nous traitons vos données (type de données, finalité, destinataire…), lisez notre Politique de Confidentialité. Il est également possible de déposer une réclamation auprès de la CNIL. 5 - Liens hypertextes et cookies: Le site bright-softwares. com contient des liens hypertextes vers d’autres sites et dégage toute responsabilité à propos de ces liens externes ou des liens créés par d’autres sites vers bright-softwares. com. La navigation sur le site bright-softwares. com est susceptible de provoquer l’installation de cookie(s) sur l’ordinateur de l’utilisateur. Un “cookie” est un fichier de petite taille qui enregistre des informations relatives à la navigation d’un utilisateur sur un site. Les données ainsi obtenues permettent d’obtenir des mesures de fréquentation, par exemple. Vous avez la possibilité d’accepter ou de refuser les cookies en modifiant les paramètres de votre navigateur. Aucun cookie ne sera déposé sans votre consentement. Les cookies sont enregistrés pour une durée maximale de 2 mois. Pour plus d’informations sur la façon dont nous faisons usage des cookies, lisez notre Politique de Confidentialité. 6 - Droit applicable et attribution de juridiction. : Tout litige en relation avec l’utilisation du site bright-softwares. com est soumis au droit français. En dehors des cas où la loi ne le permet pas, il est fait attribution exclusive de juridiction aux tribunaux compétents de Paris 13e. "
    }, {
    "id": 10,
    "url": "https://bright-softwares.com/manifest.json",
    "title": "",
    "body": "{ “lang”: “{{ site. language }}”, “dir”: “{{ site. lang_direction }}”, “name”: {{ site. name | smartify | jsonify }}, “short_name”: {{ site. short_name | smartify | jsonify }}, “icons”: [  {   “src”: “\/assets\/favicon. ico\/android-icon-192x192. png”,   “sizes”: “192x192”,   “type”: “image\/png”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-36x36. png”,   “sizes”: “36x36”,   “type”: “image\/png”,   “density”: “0. 75”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-48x48. png”,   “sizes”: “48x48”,   “type”: “image\/png”,   “density”: “1. 0”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-72x72. png”,   “sizes”: “72x72”,   “type”: “image\/png”,   “density”: “1. 5”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-96x96. png”,   “sizes”: “96x96”,   “type”: “image\/png”,   “density”: “2. 0”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-144x144. png”,   “sizes”: “144x144”,   “type”: “image\/png”,   “density”: “3. 0”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-192x192. png”,   “sizes”: “192x192”,   “type”: “image\/png”,   “density”: “4. 0”  } ], “theme_color”: “{{ site. color }}”, “background_color”: “{{ site. color }}”, “start_url”: “{{ site. url }}”, “display”: “standalone”, “orientation”: “natural”} "
    }, {
    "id": 11,
    "url": "https://bright-softwares.com/fr/manifest.json",
    "title": "",
    "body": "{ “lang”: “fr”, “dir”: “{{ site. lang_direction }}”, “name”: {{ site. name | smartify | jsonify }}, “short_name”: {{ site. short_name | smartify | jsonify }}, “icons”: [  {   “src”: “\/assets\/favicon. ico\/android-icon-192x192. png”,   “sizes”: “192x192”,   “type”: “image\/png”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-36x36. png”,   “sizes”: “36x36”,   “type”: “image\/png”,   “density”: “0. 75”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-48x48. png”,   “sizes”: “48x48”,   “type”: “image\/png”,   “density”: “1. 0”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-72x72. png”,   “sizes”: “72x72”,   “type”: “image\/png”,   “density”: “1. 5”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-96x96. png”,   “sizes”: “96x96”,   “type”: “image\/png”,   “density”: “2. 0”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-144x144. png”,   “sizes”: “144x144”,   “type”: “image\/png”,   “density”: “3. 0”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-192x192. png”,   “sizes”: “192x192”,   “type”: “image\/png”,   “density”: “4. 0”  } ], “theme_color”: “{{ site. color }}”, “background_color”: “{{ site. color }}”, “start_url”: “{{ site. url }}”, “display”: “standalone”, “orientation”: “natural”} "
    }, {
    "id": 12,
    "url": "https://bright-softwares.com/en/manifest.json",
    "title": "",
    "body": "{ “lang”: “en”, “dir”: “{{ site. lang_direction }}”, “name”: {{ site. name | smartify | jsonify }}, “short_name”: {{ site. short_name | smartify | jsonify }}, “icons”: [  {   “src”: “\/assets\/favicon. ico\/android-icon-192x192. png”,   “sizes”: “192x192”,   “type”: “image\/png”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-36x36. png”,   “sizes”: “36x36”,   “type”: “image\/png”,   “density”: “0. 75”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-48x48. png”,   “sizes”: “48x48”,   “type”: “image\/png”,   “density”: “1. 0”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-72x72. png”,   “sizes”: “72x72”,   “type”: “image\/png”,   “density”: “1. 5”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-96x96. png”,   “sizes”: “96x96”,   “type”: “image\/png”,   “density”: “2. 0”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-144x144. png”,   “sizes”: “144x144”,   “type”: “image\/png”,   “density”: “3. 0”  },  {   “src”: “\/assets\/favicon. ico\/android-icon-192x192. png”,   “sizes”: “192x192”,   “type”: “image\/png”,   “density”: “4. 0”  } ], “theme_color”: “{{ site. color }}”, “background_color”: “{{ site. color }}”, “start_url”: “{{ site. url }}”, “display”: “standalone”, “orientation”: “natural”} "
    }, {
    "id": 13,
    "url": "https://bright-softwares.com/pay",
    "title": "Payments",
    "body": "&lt;!DOCTYPE html&gt;                                        &lt;link rel= stylesheet  href= {{ baseurl }}{{  /css/stripe/normalize. css  | prepend: site. baseurl }}  /&gt;  &lt;link rel= stylesheet  href= {{ baseurl }}{{  /css/stripe/global. css  | prepend: site. baseurl }}  /&gt;      &lt;script src= {{  /js/stripe-script. js  | prepend: site. url }}  defer&gt;&lt;/script&gt;          &lt;script src= {{  /js/stripe-translation. js  | prepend: site. url }}  defer&gt;&lt;/script&gt;                                         :                                           -                  +      price_1How0GCVId68T2UtGwGmN34p                                                                                          Invoice ID                                                             Password                                  OK              "
    }, {
    "id": 14,
    "url": "https://bright-softwares.com/paiement",
    "title": "Payments",
    "body": "&lt;!DOCTYPE html&gt;                                        &lt;link rel= stylesheet  href= {{ baseurl }}{{  /css/stripe/normalize. css  | prepend: site. baseurl }}  /&gt;  &lt;link rel= stylesheet  href= {{ baseurl }}{{  /css/stripe/global. css  | prepend: site. baseurl }}  /&gt;      &lt;script src= {{  /js/stripe-script. js  | prepend: site. url }}  defer&gt;&lt;/script&gt;          &lt;script src= {{  /js/stripe-translation. js  | prepend: site. url }}  defer&gt;&lt;/script&gt;                                         :                                           -                  +      price_1How0GCVId68T2UtGwGmN34p                                                                                          Invoice ID                                                             Password                                  OK              "
    }, {
    "id": 15,
    "url": "https://bright-softwares.com/pay/canceled",
    "title": "Payment cancelled",
    "body": "&lt;!DOCTYPE html&gt;      Bright Softwares - Payments canceled                                &lt;link rel= stylesheet  href= {{ baseurl }}{{  /css/stripe/normalize. css  |  prepend: site. baseurl }}  /&gt; &lt;link rel= stylesheet  href= {{ baseurl }}{{   /css/stripe/global. css  | prepend: site. baseurl }}  /&gt;                             Your payment was canceled     Home                                            "
    }, {
    "id": 16,
    "url": "https://bright-softwares.com/pay/success",
    "title": "Payment succeeded",
    "body": "&lt;!DOCTYPE html&gt;      Bright Softwares - Payments succeeded                                  &lt;link rel= stylesheet  href= {{ baseurl }}{{  /css/stripe/normalize. css  |  prepend: site. baseurl }}  /&gt; &lt;link rel= stylesheet  href= {{ baseurl }}{{   /css/stripe/global. css  | prepend: site. baseurl }}  /&gt;                             Your payment succeeded     View CheckoutSession response::                              Home                                              "
    }, {
    "id": 17,
    "url": "https://bright-softwares.com/privacy",
    "title": "Declaration de confidentialité",
    "body": "DÉCLARATION DE CONFIDENTIALITÉARTICLE 1 – RENSEIGNEMENTS PERSONNELS RECUEILLIS: Lorsque vous effectuez un achat sur notre boutique, dans le cadre de notre processus d’achat et de vente, nous recueillons les renseignements personnels que vous nous fournissez, tels que votre nom, votre adresse et votre adresse e-mail. Lorsque vous naviguez sur notre boutique, nous recevons également automatiquement l’adresse de protocole Internet (adresse IP) de votre ordinateur, qui nous permet d’obtenir plus de détails au sujet du navigateur et du système d’exploitation que vous utilisez. Marketing par e-mail (le cas échéant): Avec votre permission, nous pourrions vous envoyer des e-mails au sujet de notre boutique, de nouveaux produits et d’autres mises à jour. ARTICLE 2 - CONSENTEMENT: Comment obtenez-vous mon consentement? Lorsque vous nous fournissez vos renseignements personnels pour conclure une transaction, vérifier votre carte de crédit, passer une commande, planifier une livraison ou retourner un achat, nous présumons que vous consentez à ce que nous recueillions vos renseignements et à ce que nous les utilisions à cette fin uniquement. Si nous vous demandons de nous fournir vos renseignements personnels pour une autre raison, à des fins de marketing par exemple, nous vous demanderons directement votre consentement explicite, ou nous vous donnerons la possibilité de refuser. Comment puis-je retirer mon consentement? Si après nous avoir donné votre consentement, vous changez d’avis et ne consentez plus à ce que nous puissions vous contacter, recueillir vos renseignements ou les divulguer, vous pouvez nous en aviser en nous contactant à ofijojo@yopmail. com ou par courrier à: Bright Softwares 92 Rue Jeanne d’Arc, Paris, J, 75013, France ARTICLE 3 – DIVULGATION: Nous pouvons divulguer vos renseignements personnels si la loi nous oblige à le faire ou si vous violez nos Conditions Générales de Vente et d’Utilisation. ARTICLE 4 – SHOPIFY: Notre boutique est hébergée sur Shopify Inc. Ils nous fournissent la plate-forme e-commerce en ligne qui nous permet de vous vendre nos services et produits. Vos données sont stockées dans le système de stockage de données et les bases de données de Shopify, et dans l’application générale de Shopify. Vos données sont conservées sur un serveur sécurisé protégé par un pare-feu. Paiement: Si vous réalisez votre achat par le biais d’une passerelle de paiement direct, dans ce cas Shopify stockera vos renseignements de carte de crédit. Ces renseignements sont chiffrés conformément à la norme de sécurité des données établie par l’industrie des cartes de paiement (norme PCI-DSS). Les renseignements relatifs à votre transaction d’achat sont conservés aussi longtemps que nécessaire pour finaliser votre commande. Une fois votre commande finalisée, les renseignements relatifs à la transaction d’achat sont supprimés. Toutes les passerelles de paiement direct respectent la norme PCI-DSS, gérée par le conseil des normes de sécurité PCI, qui résulte de l’effort conjoint d’entreprises telles que Visa, MasterCard, American Express et Discover. Les exigences de la norme PCI-DSS permettent d’assurer le traitement sécurisé des données de cartes de crédit par notre boutique et par ses prestataires de services. Pour plus d’informations, veuillez consulter les Conditions d’Utilisation de Shopify ici ou la Politique de Confidentialité ici. ARTICLE 5 – SERVICES FOURNIS PAR DES TIERS: De manière générale, les fournisseurs tiers que nous utilisons vont uniquement recueillir, utiliser et divulguer vos renseignements dans la mesure du nécessaire pour pouvoir réaliser les services qu’ils nous fournissent. Cependant, certains tiers fournisseurs de services, comme les passerelles de paiement et autres processeurs de transactions de paiement, possèdent leurs propres politiques de confidentialité quant aux renseignements que nous sommes tenus de leur fournir pour vos transactions d’achat. En ce qui concerne ces fournisseurs, nous vous recommandons de lire attentivement leurs politiques de confidentialité pour que vous puissiez comprendre la manière dont ils traiteront vos renseignements personnels. Il ne faut pas oublier que certains fournisseurs peuvent être situés ou avoir des installations situées dans une juridiction différente de la vôtre ou de la nôtre. Donc si vous décidez de poursuivre une transaction qui requiert les services d’un fournisseur tiers, vos renseignements pourraient alors être régis par les lois de la juridiction dans laquelle ce fournisseur se situe ou celles de la juridiction dans laquelle ses installations sont situées. À titre d’exemple, si vous êtes situé au Canada et que votre transaction est traitée par une passerelle de paiement située aux États-Unis, les renseignements vous appartenant qui ont été utilisés pour conclure la transaction pourraient être divulgués en vertu de la législation des États-Unis, y compris le Patriot Act. Une fois que vous quittez le site de notre boutique ou que vous êtes redirigé vers le site web ou l’application d’un tiers, vous n’êtes plus régi par la présente Politique de Confidentialité ni par les Conditions Générales de Vente et d’Utilisation de notre site web. Liens Vous pourriez être amené à quitter notre site web en cliquant sur certains liens présents sur notre site. Nous n’assumons aucune responsabilité quant aux pratiques de confidentialité exercées par ces autres sites et vous recommandons de lire attentivement leurs politiques de confidentialité. ARTICLE 6 – SÉCURITÉ: Pour protéger vos données personnelles, nous prenons des précautions raisonnables et suivons les meilleures pratiques de l’industrie pour nous assurer qu’elles ne soient pas perdues, détournées, consultées, divulguées, modifiées ou détruites de manière inappropriée. Si vous nous fournissez vos informations de carte de crédit, elles seront chiffrées par le biais de l’utilisation du protocole de sécurisation SSL et conservées avec un chiffrement de type AES-256. Bien qu’aucune méthode de transmission sur Internet ou de stockage électronique ne soit sûre à 100 %, nous suivons toutes les exigences de la norme PCI-DSS et mettons en œuvre des normes supplémentaires généralement reconnues par l’industrie. FICHIERS TÉMOINS (COOKIES) Voici une liste de fichiers témoins que nous utilisons. Nous les avons énumérés ici pour que vous ayez la possibilité de choisir si vous souhaitez les autoriser ou non. _session_id, identificateur unique de session, permet à Shopify de stocker les informations relatives à votre session (référent, page de renvoi, etc. ). _shopify_visit, aucune donnée retenue, persiste pendant 30 minutes depuis la dernière visite. Utilisé par le système interne de suivi des statistiques du fournisseur de notre site web pour enregistrer le nombre de visites. _shopify_uniq, aucune donnée retenue, expire à minuit (selon l’emplacement du visiteur) le jour suivant. Calcule le nombre de visites d’une boutique par client unique. cart, identificateur unique, persiste pendant 2 semaines, stocke l’information relative à votre panier d’achat. _secure_session_id, identificateur unique de session storefront_digest, identificateur unique, indéfini si la boutique possède un mot de passe, il est utilisé pour savoir si le visiteur actuel a accès. ARTICLE 7 – ÂGE DE CONSENTEMENT: En utilisant ce site, vous déclarez que vous avez au moins l’âge de la majorité dans votre État ou province de résidence, et que vous nous avez donné votre consentement pour permettre à toute personne d’âge mineur à votre charge d’utiliser ce site web. ARTICLE 8 – MODIFICATIONS APPORTÉES À LA PRÉSENTE POLITIQUE DE CONFIDENTIALITÉ: Nous nous réservons le droit de modifier la présente politique de confidentialité à tout moment, donc veuillez s’il vous plait la consulter fréquemment. Les changements et les clarifications prendront effet immédiatement après leur publication sur le site web. Si nous apportons des changements au contenu de cette politique, nous vous aviserons ici qu’elle a été mise à jour, pour que vous sachiez quels renseignements nous recueillons, la manière dont nous les utilisons, et dans quelles circonstances nous les divulguons, s’il y a lieu de le faire. Si notre boutique fait l’objet d’une acquisition par ou d’une fusion avec une autre entreprise, vos renseignements pourraient être transférés aux nouveaux propriétaires pour que nous puissions continuer à vous vendre des produits. QUESTIONS ET COORDONNÉES Si vous souhaitez: accéder à, corriger, modifier ou supprimer toute information personnelle que nous avons à votre sujet, déposer une plainte, ou si vous souhaitez simplement avoir plus d’informations, contactez notre agent responsable des normes de confidentialité à ofijojo@yopmail. com ou par courrier à Bright Softwares [Re: Agent des Normes de Confidentialité] [92 Rue Jeanne d’Arc, Paris, J, 75013, France] "
    }, {
    "id": 18,
    "url": "https://bright-softwares.com/style.css",
    "title": "",
    "body": ". portfolio-image { width: 360px; height: 261px;} . en:lang(en),. fr:lang(fr),. zh:lang(zh) { font-weight: bold;} "
    }, {
    "id": 19,
    "url": "https://bright-softwares.com/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 20,
    "url": "https://bright-softwares.com/blog/en/security/log4j-flaw-am-i-impacted-and-how-can-i-protect-myself",
    "title": "Log4j flaw: Am I impacted and how can I protect myself",
    "body": "2021/12/15 - A flaw in Log4j, a Java library developed by the open-source Apache Software Foundation for logging error messages in applications, is the most high-profile security vulnerability on the internet right now and comes with a severity score of 10 out of 10.   The library is developed by the open-source Apache Software Foundation and is a key Java-logging framework. The CERT New Zealand triggered an alert last week that CVE-2021-44228, a remote code execution flaw in Log4j, was already being exploited by attackers. Several national cybersecurity agencies also issed the alert, including the Cybersecurity and Infrastructure Security Agency (CISA) and the UK’s National Cyber Security Centre (NCSC).   Read more at Internet infrastructure provider Cloudflare said Log4j exploits started on December 1.   What devices and applications are at risk?  Here are the criteria:  Your device must be exposed to the internet.  Your device must be running Apache Log4j Apache Log4j version must be between 2. 0 and 2. 14. 1If you have IOT devices connected to the internet, with the conditions above, you are at risk. Probably, Mirai, a botnet that targets all manner of internet-connected (IoT) devices, might be trying to locate your device. Where is Log4j most used?NCSC notes that Log4j version 2 (Log4j2), the affected version, is included in :  Apache Struts2 framework Solr framework Druid framework Flink framework Swift framework. For IBM products, there is Websphere 8. 5 and 9. 0. Big players responseAWS: AWS is working on patching its services that use Log4j and has released mitigations for services like CloudFront. It has also detailed how the flaw impacts its services. AWS has updated its WAF rule set – AWSManagedRulesKnownBadInputsRuleSet AMR – to detect and mitigate Log4j attack attempts and scanning. It also has mitigation options that can be enabled for CloudFront, Application Load Balancer (ALB), API Gateway, and AppSync. It’s also currently updating all Amazon OpenSearch Service to the patched version of Log4j.   IBM: Same for IBM shared that it is **“actively responding”** to the Log4j vulnerability across IBM’s own infrastructure and its products. IBM has confirmed Websphere 8. 5 and 9. 0 are vulnerable.   Oracle has issued a patch for the flaw, too.    “Due to the severity of this vulnerability and the publication of exploit code on various sites, Oracle strongly recommends that customers apply the updates provided by this Security Alert as soon as possible,” it said.   Other playersVendors with popular products known to be still vulnerable include  Atlassian, Amazon, Microsoft Azure, Cisco, Commvault, ESRI, Exact, Fortinet, JetBrains, Nelson, Nutanix, OpenMRS, Oracle, Red Hat, Splunk, -Soft, and VMware. The list is even longer when adding products where a patch has been released.   What you should do: Discover your devices and patch them Part of the challenge will be identifying software harboring the Log4j vulnerability. The Netherland’s Nationaal Cyber Security Centrum (NCSC) has posted a comprehensive and sourced A-Z list on GitHub of all affected products it is aware are either vulnerable, not vulnerable, are under investigation, or where a fix is available. The list of products illustrates how widespread the vulnerability is, spanning cloud services, developer services, security devices, mapping services, and more.     CISA’s main advice is to identify internet-facing devices running Log4j and upgrade them to version 2. 15. 0, or to apply the mitigations provided by vendors “immediately”. But it also recommends setting up alerts for probes or attacks on devices running Log4j.    “To be clear, this vulnerability poses a severe risk,” CISA director Jen Easterly said Sunday. “We will only minimize potential impacts through collaborative efforts between government and the private sector. We urge all organizations to join us in this essential effort and take action. ”   Additional steps recommended by CISA include: enumerating any external facing devices with Log4j installed; ensuring the security operations center actions every alert with Log4j installed; and installing a web application firewall (WAF) with rules to focus on Log4j.   What if I cannot patch or upgrade?It is recommended to upgrade to version 2. 15. 0 of Log4j. There might be situations where upgrading is not immediately possible. Update Log4j’s configuration: NCSC recommends updating to version 2. 15. 0 or later, and – where not possible – mitigating the flaw in Log4j 2. 10 and later by setting system property  log4j2. formatMsgNoLookups  to  true  or removing the JndiLookup class from the classpath.     Setup network rules to detect exploit attempts: NCCGroup has posted several network-detection rules to detect exploitation attempts and indicators of successful exploitation. Is my system compromised?Finally, Microsoft has released its set of indicators of compromise and guidance for preventing attacks on Log4j vulnerability. Examples of the post-exploitation of the flaw that Microsoft has seen include installing coin miners, Cobalt Strike to enable credential theft and lateral movement, and exfiltrating data from compromised systems.   What is Log4j?: Log4J is a widely used Java library for logging error messages in applications. It is used in enterprise software applications, including those custom applications developed in-house by businesses, and forms part of many cloud computing services. Where is Log4j used?: The Log4j 2 library is used in enterprise Java software and according to the UK’s NCSC is included in Apache frameworks such as Apache Struts2, Apache Solr, Apache Druid, Apache Flink, and Apache Swift. Which applications are affected by the Log4j flaw?: Because Log4j is so widely used, the vulnerability may impact a very wide range of software and services from many major vendors. According to NCSC an application is vulnerable “if it consumes untrusted user input and passes this to a vulnerable version of the Log4j logging library. ” How widely is the Log4j flaw being exploited?: Security experts have warned that there are hundreds of thousands of attempts by hackers to find vulnerable devices; over 40 percent of corporate networks have been targeted according to one security company. ConclusionThe primary objective you need to have is first the assesment of your devices. Split your team in 3 groups:  Assesment group : will list all the devices of your infrastructure and check if they are impacted. If they found corrupted devices, take them offline. For the other ones, route them to the tow remaining teams.  Upgrade group : They perform the upgrade of the devices.  Reconfigure group: they process the devices that are not immediately upgradable. "
    }, {
    "id": 21,
    "url": "https://bright-softwares.com/blog/en/docker/docker-tip-inspect-and-jq",
    "title": "Docker tip : inspect and jq",
    "body": "2021/12/15 - Docker inspect and jqThis isn’t so much a docker tip, as it is a jq tip. If you haven’t heard of jq, it is a great tool for parsing JSON from the command line. This also makes it a great tool to see what is happening in a container instead of having to use the –format specifier which I can never remember how to use exactly: Get network information::     $ docker inspect 4c45aea49180 | jq '. []. NetworkSettings. Networks'The output is:   {    bridge : {     EndpointID :  ba1b6efba16de99f260e0fa8892fd4685dbe2f79cba37ac0114195e9fad66075 ,     Gateway :  172. 17. 0. 1 ,     IPAddress :  172. 17. 0. 2 ,     IPPrefixLen : 16,     IPv6Gateway :   ,     GlobalIPv6Address :   ,     GlobalIPv6PrefixLen : 0,     MacAddress :  02:42:ac:11:00:02    }  }  Get the arguments with which the container was started:     $ docker inspect 4c45aea49180 | jq '. []. Args'The output is:   [   -server ,   -advertise ,   192. 168. 99. 100 ,   -bootstrap-expect ,   1   ]Get all the mounted volumes:   $ docker inspect 4c45aea49180 | jq '. []. Mounts'Output   [  {   Name :  a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f ,   Source :  /mnt/sda1/var/lib/docker/volumes/a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f/_data ,   Destination :  /data ,   Driver :  local ,   Mode :   ,   RW : true  }  ]And of course also works great for querying other kinds of (docker-esque) APIs that produce JSON (e. g Marathon, Mesos, Consul etc. ). JQ provides a very extensive API for accessing and processing JSON. More information can be found here: https://stedolan. github. io/jq/ "
    }, {
    "id": 22,
    "url": "https://bright-softwares.com/blog/en/kubernetes/how-to-use-local-docker-images-with-minikube",
    "title": "How to use local docker images with Minikube?",
    "body": "2021/12/14 - I have previously shared a short tutorial on Minikube, and while I was using it, I thought I would reuse my local images directly, without uploading and then downloading them again. Two things I have tried (and didn’t work): Import the images using kubectl: I first cleaned the instances of minikube and start fresh to make sure that the is no collision.   kubectl run hdfs --image=fluxcapacitor/hdfs:latest --port=8989kubectl run hdfs --image=fluxcapacitor/hdfs:latest --port=8989 imagePullPolicy=NeverAnd the output was :   NAME          READY   STATUS       RESTARTS  AGEhdfs-2425930030-q0sdl  0/1    ContainerCreating  0     10mAs you see, it get stuck on some status but it never reach the ready state. Create a local registery: The idea here is to create a local registry and put my images into it. With that, I don’t need to upload then download my images. That didn’t work either. The solution: The solution was to use the eval $(minikube docker-env). The steps are: Step 1 : Set the environment variables: To set your environment variables, use the command :   eval $(minikube docker-env)Step 2: Build the image with the daemon of minikube: To build the image use this command :   docker build -t my-imageOf source, replace my-image with your image name. Step 3: Set the image in the kubernetes pod: Use the tag in the kubernetes pod. Eg: my-image Step 4: Tell kubernetes not to download the image anymore: To achieve that, you need to use the imagePullPolicy to Never. More information here: How to set imagePullPolicy to never. Tips and pilfalls: Run the env command in all your terminals: Make sure you run the eval $(minikube docker-env) in all your terminal. You will have the environment variables in all of them. If not, some of the commands my fail because of the lack of these variables. If you close your terminal, rerun eval $(minikube docker-env): Once you close your terminal, the environment variables are cleared. If you then build your images, they won’t update in minikube. You will think that it is not working, but it is because the environment variables are not there. Exit from minikube?: If you want to exit from minikube, run this command :   eval $(minikube docker-env -u)Conclusion: This tutorial shows you how you can use your local image with minikube without uploading then downloading them. Watchout for the pitfalls. References: This readme This stackoverflow post "
    }, {
    "id": 23,
    "url": "https://bright-softwares.com/blog/en/kubernetes/kubernetes-helm-how-to-show-multi-line-properties",
    "title": "Kubernetes Helm : how to show Multi-line Properties",
    "body": "2021/12/10 - Working with Kubernetes Helm, I went to the documentation. The link to the documentation is here. In Helm’s v3 documentation, in the section Accessing Files Inside Templates, you have an example of 3 properties (toml) files; where each file has only one key/value pair. The configmap. yaml looks like this. It contains one config. toml for simplicity. apiVersion: v1kind: ConfigMapmetadata: name: {{ . Release. Name }}-configdata: {{- $files := . Files }} {{- range tuple  config. toml  }} {{ . }}: |-  {{ $files. Get . }} {{- end }}I was happy with it. Then I add a second line to the config. toml file. config. toml replicaCount=1foo=barThen boom, I get an Error: INSTALLATION FAILED: YAML parse error on deploy/templates/configmap. yaml: error converting YAML to JSON: yaml: line 9: could not find expected ':'Digging in this error, I found a solution. The solution: Helm will read in that file, but as it is a text templating engine, It does not understand that I was trying to compose a YAML file. As a consquence, it was not helping me in the error. That’s actually why you will see so many, many templates in the wild with {{ . thing | indent 8 }} or {{ . otherThing | toYaml }} – because you need to help Helm know in what context it is emitting the text. So, in my specific case, I needed to indent the filter with a value of 4 because mycurrent template has two spaces for the key indent level, and two more spaces for the value block scalar data: {{- $files := . Files }} {{- range tuple  config. toml  }} {{ . }}: |-{{ $files. Get . | indent 4 }}{{/* notice this ^^^ template expression is flush left,because the 'indent' is handling whitespace, not the golang template itself */}} {{- end }}I hope this quick post will help someone in his research. Inpiration from this post. #helm #automation #docker #container "
    }, {
    "id": 24,
    "url": "https://bright-softwares.com/blog/en/slack/how-to-build-a-slackbot-in-python-on-ubuntu-2004",
    "title": "How To Build a Slackbot in Python on Ubuntu 20.04",
    "body": "2021/11/16 - Introduction: Slack is a communication platform designed for workplace productivity. It includes features such as direct messaging, public and private channels, voice and video calls, and bot integrations. A Slackbot is an automated program that can perform a variety of functions in Slack, from sending messages to triggering tasks to alerting on certain events. In this tutorial you will build a Slackbot in the Python programming language. Python is a popular language that prides itself on simplicity and readability. Slack provides a rich Python Slack API for integrating with Slack to perform common tasks such as sending messages, adding emojis to messages, and much more. Slack also provides a Python Slack Events API for integrating with events in Slack, allowing you to perform actions on events such as messages and mentions. As a fun proof-of-concept that will demonstrate the power of Python and its Slack APIs, you will build a CoinBot—a Slackbot that monitors a channel and, when triggered, will flip a coin for you. You can then modify your CoinBot to fulfill any number of slightly more practical applications. Note that this tutorial uses Python 3 and is not compatible with Python 2. Prerequisites: In order to follow this guide, you’ll need:    A Slack Workspace that you have the ability to install applications into. If you created the workspace you have this ability. If you don’t already have one, you can create one on the [Slack website](https://slack. com/create).     (Optional) A server or computer with a public IP address for development. We recommend a fresh installation of Ubuntu 20. 04, a non-root user with sudo privileges, and SSH enabled. You can follow this guide to initialize your server and complete these steps.  You may want to test this tutorial on a server that has a public IP address. Slack will need to be able to send events such as messages to your bot. If you are testing on a local machine you will need to port forward traffic through your firewall to your local system. If you are looking for a way to develop on a cloud server, check out this tutorial on How To Use Visual Studio Code for Remote Development via the Remote-SSH Plugin. Step 1 — Creating the Slackbot in the Slack UI: First create your Slack app in the Slack API Control Panel. Log in to your workspace in Slack via a web browser and navigate to the API Control Panel. Now click on the Create an App button.  Next you’ll be prompted for the name of your app and to select a development Slack workspace. For this tutorial, name your app &lt;span class= highlight &gt;CoinBot&lt;/span&gt; and select a workspace you have admin access to. Once you have done this click on the Create App button.  Once your app is created you’ll be presented with the following default app dashboard. This dashboard is where you manage your app by setting permissions, subscribing to events, installing the app into workspaces, and more.  In order for your app to be able to post messages to a channel you need to grant the app permissions to send messages. To do this, click the Permissions button in the control panel.  When you arrive at the OAuth &amp; Permissions page, scroll down until you find the Scopes section of the page. Then find the Bot Token Scopes subsection in the scope and click on Add an OAuth Scope button.  Click on that button and then type chat:write. Select that permission to add it to your bot. This will allow the app to post messages to channels that it can access. For more information on the available permissions refer to Slack’s Documentation.  Now that you’ve added the appropriate permission it is time to install your app into your Slack workspace. Scroll back up on the OAuth &amp; Permissions page and click the Install App to Workspace button at the top.  Click this button and review the actions that the app can perform in the channel. Once you are satisfied, click the Allow button to finish the installation.  Once the bot is installed you’ll be presented with a Bot User OAuth Access Token for your app to use when attempting to perform actions in the workspace. Go ahead and copy this token; you’ll need it later.  Finally, add your newly installed bot into a channel within your workspace. If you haven’t created a channel yet you can use the #general channel that is created by default in your Slack workspace. Locate the app in the Apps section of the navigation bar in your Slack client and click on it. Once you’ve done that open the Details menu in the top right hand side. If your Slack client isn’t full-screened it will look like an i in a circle.  To finish adding your app to a channel, click on the More button represented by three dots in the details page and select Add this app to a channel…. Type your channel into the modal that appears and click Add.  You’ve now successfully created your app and added it to a channel within your Slack workspace. After you write the code for your app it will be able to post messages in that channel. In the next section you’ll start writing the Python code that will power CoinBot. Step 2 — Setting Up Your Python Developer Environment: First let’s set up your Python environment so you can develop the Slackbot. Open a terminal and install python3 and the relevant tools onto your system:   sudo apt install python3 python3-venv``Next you will create a virtual environment to isolate your Python packages from the system installation of Python. To do this, first create a directory into which you will create your virtual environment. Make a new directory at ``~/. venvs``:``  mkdir ~/. venvs``Now create your Python virtual environment:``  python3 -m venv ~/. venvs/slackbotNext, activate your virtual environment so you can use its Python installation and install packages: ``  source ~/. venvs/slackbot/bin/activate Your shell prompt will now show the virtual environment in parenthesis. It will look something like this:Now use `pip` to install the necessary Python packages into your virtual environment:``  pip install slackclient slackeventsapi Flask```slackclient` and `slackeventsapi` facilitate Python’s interaction with Slack’s APIs. `Flask` is a popular micro web framework that you will use to deploy your app:Now that you have your developer environment set up, you can start writing your Python Slackbot:## Step 3 — Creating the Slackbot Message Class in PythonMessages in Slack are sent via a [specifically formatted JSON payload](https://api. slack. com/reference/surfaces/formatting). This is an example of the JSON that your Slackbot will craft and send as a message:``  {     channel : channel ,     blocks :[     {        type : section ,        text :{         type : mrkdwn ,         text : Sure! Flipping a coin. . . . \n\n        }     },     {        type : section ,        text :{         type : mrkdwn ,         text : *flips coin* The result is Tails.         }     }    ]  }You could manually craft this JSON and send it, but instead let’s build a Python class that not only crafts this payload, but also simulates a coin flip. First use the touch command to create a file named coinbot. py:   touch coinbot. py Next, open this file with nano or your favorite text editor:   nano coinbot. py Now add the following lines of code to import the relevant libraries for your app. The only library you need for this class is the random library from the Python Standard Library. This library will allow us to simulate a coin flip. Add the following lines to coinbot. py to import all of the necessary libraries: &lt;div class= code-label   title= coinbot. py &gt;coinbot. py&lt;/div&gt;  # import the random library to help us generate the random numbers  import randomNext, create your CoinBot class and an instance of this classto craft the message payload. Add the following lines to coinbot. py to create the CoinBot class: `` coinbot. py. . . class CoinBot: ```Now indent by one and create the constants, constructors, and methods necessary for your class. First let’s create the constant that will hold the base of your message payload. This section specifies that this constant is of the section type and that the text is formatted via markdown. It also specifies what text you wish to display. You can read more about the different payload options in the official Slack message payload documentation. Append the following lines to coinbot. py to create the base template for the payload: `` coinbot. py. . .   # Create a constant that contains the default text for the message  COIN_BLOCK = {     type :  section ,     text : {       type :  mrkdwn ,       text : (         Sure! Flipping a coin. . . . \n\n       ),    },  } ```Next create a constructor for your class so that you can create a separate instance of your bot for every request. Don’t worry about memory overhead here; the Python garbage collector will clean up these instances once they are no longer needed. This code sets the recipient channel based on a parameter passed to the constructor. Append the following lines to coinbot. py to create the constructor: `` coinbot. py. . .   # The constructor for the class. It takes the channel name as the a  # parameter and sets it as an instance variable.   def __init__(self, channel):    self. channel = channel ```Now write the code that simulates to flip a coin. We’ll randomly generate a one or zero, representing heads or tails respectively. Append the following lines to coinbot. py to simulate the coin flip and return the crafted payload: `` coinbot. py. . .   # Generate a random number to simulate flipping a coin. Then return the   # crafted slack payload with the coin flip message.   def _flip_coin(self):    rand_int = random. randint(0,1)    if rand_int == 0:      results =  Heads     else:      results =  Tails     text = f The result is {results}     return { type :  section ,  text : { type :  mrkdwn ,  text : text}}, ``Finally, create a method that crafts and returns the entire message payload, including the data from your constructor, by calling your _flip_coin method. Append the following lines to coinbot. py to create the method that will generate the finished payload: `` coinbot. py. . .   # Craft and return the entire message payload as a dictionary.   def get_message_payload(self):    return {       channel : self. channel,       blocks : [        self. COIN_BLOCK,        *self. _flip_coin(),      ],    } ``You are now finished with the CoinBot class and it is ready for testing. Before continuing, verify that your finished file, coinbot. py, contains the following: `` coinbot. py# import the random library to help us generate the random numbersimport random# Create the CoinBot Classclass CoinBot:  # Create a constant that contains the default text for the message  COIN_BLOCK = {     type :  section ,     text : {       type :  mrkdwn ,       text : (         Sure! Flipping a coin. . . . \n\n       ),    },  }  # The constructor for the class. It takes the channel name as the a  # parameter and then sets it as an instance variable  def __init__(self, channel):    self. channel = channel  # Generate a random number to simulate flipping a coin. Then return the  # crafted slack payload with the coin flip message.   def _flip_coin(self):    rand_int = random. randint(0,1)    if rand_int == 0:      results =  Heads     else:      results =  Tails     text = f The result is {results}     return { type :  section ,  text : { type :  mrkdwn ,  text : text}},  # Craft and return the entire message payload as a dictionary.   def get_message_payload(self):    return {       channel : self. channel,       blocks : [        self. COIN_BLOCK,        *self. _flip_coin(),      ],    } ``Save and close the file. Now that you have a Python class ready to do the work for your Slackbot, let’s ensure that this class produces a useful message payload and that you can send it to your workspace. Step 4 — Testing Your Message: Now let’s test that this class produces a proper payload. Create a file namedcoinbot_test. py:   nano coinbot_test. py``Now add the following code. **Be sure to change the channel name in the instantiation of the coinbot class `coin_bot = coinbot( #&lt;span class= highlight &gt;YOUR_CHANNEL_HERE&lt;/span&gt; )`**. This code will create a Slack client in Python that will send a message to the channel you specify that you have already installed the app into:``&lt;div class= code-label   title= coinbot_test. py &gt;coinbot_test. py&lt;/div&gt;  from slack import WebClient  from coinbot import CoinBot  import os  # Create a slack client  slack_web_client = WebClient(token=os. environ. get( SLACK_TOKEN ))  # Get a new CoinBot  coin_bot = CoinBot( #YOUR_CHANNEL_HERE )  # Get the onboarding message payload  message = coin_bot. get_message_payload()  # Post the onboarding message in Slack  slack_web_client. chat_postMessage(\*\*message)Save and close the file. Before you can run this file you will need to export the Slack token that you saved in Step 1 as an environment variable:  export SLACK_TOKEN= your_bot_user_token Now test this file and verify that the payload is produced and sent by running the following script in your terminal. Make sure that your virtual environment is activated. You can verify this by seeing the `(slackbot)` text at the front of your bash prompt. Run this command you will receive a message from your Slackbot with the results of a coin flip:  python coinbot_test. pyCheck the channel that you installed your app into and verify that your bot did indeed send the coin flip message. Your result will be heads or tails. ![Coin Flip Test](https://assets. digitalocean. com/articles/coinbot/NPfnw0k. png)Now that you’ve verified that your Slackbot can flip a coin, create a message, and deliver the message, let’s create a [Flask](https://flask. palletsprojects. com/en/1. 1. x/) to perpetually run this app and make it simulate a coin flip and share the results whenever it sees certain text in messages sent in the channel. ## Step 5 — Creating a Flask Application to Run Your SlackbotNow that you have a functioning application that can send messages to your Slack workspace, you need to create a long running process so your bot can listen to messages sent in the channel and reply to them if the text meets certain criteria. You’re going to use the Python web framework [Flask](https://flask. palletsprojects. com/en/1. 1. x/) to run this process and listen for events in your channel. &lt;span class= note &gt;In this section you will be running your Flask application from a server with a public IP address so that the Slack API can send you events. If you are running this locally on your personal workstation you will need to forward the port from your personal firewall to the port that will be running on your workstation. These ports can be the same, and this tutorial will be set up to use port `3000`.  &lt;/span&gt;First adjust your firewall settings to allow traffic through port `3000`:``  sudo ufw allow 3000``Now check the status of `ufw`:``  sudo ufw status``You will see an output like this:``  OutputStatus: active  To Action From  ---  OpenSSH ALLOW Anywhere  3000 ALLOW Anywhere  OpenSSH (v6) ALLOW Anywhere (v6)  3000 (v6) ALLOW Anywhere (v6)``Now create the file for your Flask app. Name this file `app. py`:``  touch app. py``Next, open this file in your favorite text editor:``  nano app. py``Now add the following import `statements`. You’ll import the following libraries for the following reasons: import os - To access environment variables import logging - To log the events of the app from flask import Flask - To create a Flask app from slack import WebClient - To send messages via Slack from slackeventsapi import SlackEventAdapter - To receive events from Slack and process them from coinbot import CoinBot - To create an instance of your CoinBot and generate the message payload. ```Append the following lines to app. py to import all of the necessary libraries: &lt;div class= code-label   title= app. py &gt;app. py&lt;/div&gt;  import os  import logging  from flask import Flask  from slack import WebClient  from slackeventsapi import SlackEventAdapter  from coinbot import CoinBotNow create your Flask app and register a Slack Event Adapter to your Slack app at the /slack/events endpoint. This will create a route in your Slack app where Slack events will be sent and ingested. To do this you will need to get another token from your Slack app, which you will do later in the tutorial. Once you get this variable you will export it as an environment variable named SLACK_EVENTS_TOKEN. Go ahead and write your code to read it in when creating the SlackEventAdapter, even though you haven’t set the token yet. Append the following lines to app. py to create the Flask app and register the events adapter into this app: `` app. py. . . # Initialize a Flask app to host the events adapterapp = Flask(__name__)# Create an events adapter and register it to an endpoint in the slack app for event ingestion. slack_events_adapter = SlackEventAdapter(os. environ. get( SLACK_EVENTS_TOKEN ),  /slack/events , app) ```Next create a web client object that will allow your app to perform actions in the workspace, specifically to send messages. This is similar to what you did when you tested your coinbot. py file previously. Append the following line to app. py to create this slack_web_client: &lt;div class= code-label   title= app. py &gt;app. py&lt;/div&gt;  . . .   # Initialize a Web API client  slack_web_client = WebClient(token=os. environ. get( SLACK_TOKEN ))Now create a function that can be called that will create an instance of CoinBot, and then use this instance to create a message payload and pass the message payload to the Slack web client for delivery. This function will take in a single parameter, channel, which will specify what channel receives the message. Append the following lines to app. py to create this function: `` app. py. . . def flip_coin(channel):     Craft the CoinBot, flip the coin and send the message to the channel       # Create a new CoinBot  coin_bot = CoinBot(channel)  # Get the onboarding message payload  message = coin_bot. get_message_payload()  # Post the onboarding message in Slack  slack_web_client. chat_postMessage(**message) ```Now that you have created a function to handle the messaging aspects of your app, create one that monitors Slack events for a certain action and then executes your bot. You’re going to configure your app to respond with the results of a simulated coin flip when it sees the phrase “Hey Sammy, Flip a coin”. You’re going to accept any version of this—case won’t prevent the app from responding. First decorate your function with the @slack_events_adapter. on syntax that allows your function to receive events. Specify that you only want the message events and have your function accept a payload parameter containing all of the necessary Slack information. Once you have this payload you will parse out the text and analyze it. Then, if it receives the activation phrase, your app will send the results of a simulated coin flip. Append the following code to app. py to receive, analyze, and act on incoming messages: `` app. py# When a 'message' event is detected by the events adapter, forward that payload# to this function. @slack_events_adapter. on( message )def message(payload):     Parse the message event, and if the activation string is in the text,  simulate a coin flip and send the result.        # Get the event data from the payload  event = payload. get( event , {})  # Get the text from the event that came through  text = event. get( text )  # Check and see if the activation phrase was in the text of the message.   # If so, execute the code to flip a coin.   if  hey sammy, flip a coin  in text. lower():    # Since the activation phrase was met, get the channel ID that the event    # was executed on    channel_id = event. get( channel )    # Execute the flip_coin function and send the results of    # flipping a coin to the channel    return flip_coin(channel_id) ``Finally, create a main section that will create a logger so you can see the internals of your application as well as launch the app on your external IP address on port 3000. In order to ingest the events from Slack, such as when a new message is sent, you must test your application on a public-facing IP address. Append the following lines to app. py to set up your main section: `` app. pyif __name__ ==  __main__ :  # Create the logging object  logger = logging. getLogger()  # Set the log level to DEBUG. This will increase verbosity of logging messages  logger. setLevel(logging. DEBUG)  # Add the StreamHandler as a logging handler  logger. addHandler(logging. StreamHandler())  # Run your app on your externally facing IP address on port 3000 instead of  # running it on localhost, which is traditional for development.   app. run(host='0. 0. 0. 0', port=3000) ```You are now finished with the Flask app and it is ready for testing. Before you move on verify that your finished file, app. py contains the following: `` app. pyimport osimport loggingfrom flask import Flaskfrom slack import WebClientfrom slackeventsapi import SlackEventAdapterfrom coinbot import CoinBot# Initialize a Flask app to host the events adapterapp = Flask(**name**)# Create an events adapter and register it to an endpoint in the slack app for event injestion. slack_events_adapter = SlackEventAdapter(os. environ. get( SLACK_EVENTS_TOKEN ),  /slack/events , app)# Initialize a Web API clientslack_web_client = WebClient(token=os. environ. get( SLACK_TOKEN ))def flip_coin(channel):   Craft the CoinBot, flip the coin and send the message to the channel    # Create a new CoinBotcoin_bot = CoinBot(channel)  # Get the onboarding message payload  message = coin_bot. get_message_payload()  # Post the onboarding message in Slack  slack_web_client. chat_postMessage(**message)# When a 'message' event is detected by the events adapter, forward that payload# to this function. @slack_events_adapter. on( message )def message(payload):   Parse the message event, and if the activation string is in the text,simulate a coin flip and send the result.      # Get the event data from the payload  event = payload. get( event , {})  # Get the text from the event that came through  text = event. get( text )  # Check and see if the activation phrase was in the text of the message.   # If so, execute the code to flip a coin.   if  hey sammy, flip a coin  in text. lower():    # Since the activation phrase was met, get the channel ID that the event    # was executed on    channel_id = event. get( channel )    # Execute the flip_coin function and send the results of    # flipping a coin to the channel    return flip_coin(channel_id)if **name** ==  **main** : # Create the logging objectlogger = logging. getLogger()  # Set the log level to DEBUG. This will increase verbosity of logging messages  logger. setLevel(logging. DEBUG)  # Add the StreamHandler as a logging handler  logger. addHandler(logging. StreamHandler())  # Run our app on our externally facing IP address on port 3000 instead of  # running it on localhost, which is traditional for development.   app. run(host='0. 0. 0. 0', port=3000) ``Save and close the file. Now that your Flask app is ready to serve your application let’s test it out. Step 6 — Running Your Flask App: Finally, bring everything together and execute your app. First, add your running application as an authorized handler for your Slackbot. Navigate to the Basic Information section of your app in the Slack UI. Scroll down until you find the App Credentials section.  Copy the Signing Secret and export it as the environment variable SLACK_EVENTS_TOKEN:   export SLACK_EVENTS_TOKEN= MY_SIGNING_SECRET_TOKEN  With this you have all the necessary API tokens to run your app. Refer to Step 1 if you need a refresher on how to export your SLACK_TOKEN. Now you can start your app and verify that it is indeed running. Ensure that your virtual environment is activated and run the following command to start your Flask app:   python3 app. pyYou will see an output like this: ``  (slackbot) [20:04:03] sammy:coinbot$ python app. py   * Serving Flask app “app” (lazy loading)   * Environment: production    WARNING: This is a development server. Do not use it in a production deployment.    Use a production WSGI server instead.   * Debug mode: off   * Running on http://0. 0. 0. 0:3000/ (Press CTRL+C to quit) To verify that your app is up, open a new terminal window and `curl` the IP address of your server with the correct port at `/slack/events`:curl http://YOUR_IP_ADDRESS:3000/slack/events ```curl will return the following: OutputThese are not the slackbots you're looking for. Receiving the message These are not the slackbots you're looking for. , indicates that your app is up and running. Now leave this Flask application running while you finish configuring your app in the Slack UI. First grant your app the appropriate permissions so that it can listen to messages and respond accordingly. Click on Event Subscriptions in the UI sidebar and toggle the Enable Events radio button.  Once you’ve done that, type in your IP address, port, and /slack/events endpoint into the Request URL field. Don’t forget the HTTP protocol prefix. Slack will make an attempt to connect to your endpoint. Once it has successfully done so you’ll see a green check mark with the word Verified next to it.  Next, expand the Subscribe to bot events and add the message. channels permission to your app. This will allow your app to receive messages from your channel and process them.  Once you’ve done this you will see the event listed in your Subscribe to bot events section. Next click the green Save Changes button in the bottom right hand corner.  Once you do this you’ll see a yellow banner across the top of the screen informing you that you need to reinstall your app for the following changes to apply. Every time you change permissions you’ll need to reinstall your app. Click on the reinstall your app link in this banner to reinstall your app.  You’ll be presented with a confirmation screen summarizing the permissions your bot will have and asking if you want to allow its installation. Click on the green Allow button to finish the installation process.  Now that you’ve done this your app should be ready. Go back to the channel that you installed CoinBot into and send a message containing the phrase Hey Sammy, Flip a coin in it. Your bot will flip a coin and reply with the results. Congrats! You’ve created a Slackbot! Conclusion: Once you are done developing your application and you are ready to move it to production, you’ll need to deploy it to a server. This is necessary because the Flask development server is not a secure production environment. You’ll be better served if you deploy your app using a WSGI and maybe even securing a domain name and giving your server a DNS record. There are many options for deploying Flask applications, some of which are listed below:  Deploy your Flask application to Ubuntu 20. 04 using Gunicorn and Nginx Deploy your Flask application to Ubuntu 20. 04 using uWSGI and Nginx Deploy your Flask Application Using Docker on Ubuntu 18. 04There are many more ways to deploy your application than just these. As always, when it comes to deployments and infrastucture, do what works best for you. In any case, you now have a Slackbot that you can use to flip a coin to help you make decisions, like what to eat for lunch. You can also take this base code and modify it to fit your needs, whether it be automated support, resource management, pictures of cats, or whatever you can think of. You can view the complete Python Slack API docs here. "
    }, {
    "id": 25,
    "url": "https://bright-softwares.com/blog/en/backup-restore/how-do-i-get-my-data-off-an-old-computer",
    "title": "How Do I Get My Data Off an Old Computer?",
    "body": "2021/11/15 - I want to delete everything on my old computer, but…:  “Hello,  I have and older gateway tower but sold the monitor that came with it.  _I have a laptop and a TV and tried using an HDMI to VGA adapter thinking it would show on my TV or laptop but its not working. I would like to go through my old computer and wipe it clean before just recycling it. _  Any suggestions on how I can view the content on my old computer?” Why you can’t get a signal from your old computer: Good news: You have a few solutions to address this. To start, let’s talk about what isn’t going to work. Connecting your desktop to a laptop isn’t going to do anything, as your laptop’s connections are outputs. No matter what connection you use, hooking up your desktop PC to your laptop’s anything isn’t going to display your desktop’s picture on your laptop’s display. It’s a great idea in theory, but one that isn’t going to work, technologically. I’ve never tried using a VGA-to-HDMI adapter—in fact this is the first I’ve ever heard of someone actually using one, given how old that connection type is. My suspicion is that this technique won’t work, or has a high likelihood of failure, because VGA is an analog signal and HDMI is digital. It’s possible to go between the two, but whatever adapter you’re using is going to have to take that VGA signal and convert it. If you just have a simple cable that has a VGA connection on one end and an HDMI connection on the other, that’s probably not going to work. It’s also possible that whatever adapter you’ve picked up sucks, for lack of a better way to phrase it. If you went bargain-bin on your adapter, or bought one that hasn’t already been tested out by plenty of other people, it might not be able to do what you want it to do because that’s how these things sometimes go. An adapter can help, but there’s a better solution: This is a tricky situation, as what I’d typically recommend—a USB-to-DVI adapter, which you’d then connect to a DVI-to-HDMI cable, or a simpler USB-to-HDMI adapter—is probably going to cost more than what you’re willing to spend on a solution. At least, I’d probably ask around and see if any of my friends have an older monitor with a VGA input before I plunked down $30-50 on an adapter.  One thing worth checking is whether your old desktop has an S-Video port. It’s unlikely, but if it does, and your TV has one, you can connect them together that way. It’s a longshot, but it’s worth exploring. Otherwise, I think your best option is to hit up Craigslist for your area and buy someone’s cheap older display. Make sure they can confirm it works with a VGA signal before you pay for it, but this will be the best and surest way to get an image from your system. And, in doing so, you can then start poring over your data and wiping your system. Looking at Craigslist right now for my area, Amazon is selling VGA monitors for around $150-200. You can probably find something as cheap as that where you live; remember, all that matters is that it works, not that it looks pretty, has a giant screen, or anything like that. Hook it up to your computer, save or delete whatever you want, and recycle that along with your old PC. To get at your old data, try removing your hard drive: Messing with displays and monitors is one approach. Since all you care about is the data on your drive, you could also open up your desktop PC, disconnect and remove its hard drive, and slap that hard drive in an external enclosure. I’d recommend one, but I’m not sure if your drive uses an ancient IDE connection or SATA. (You can find enclosures and docking stations that handle both, so it’s almost a moot point. ) Install your old desktop’s hard drive into one of those, connect the enclosure to a new desktop or laptop via USB, and you should be able to access all of your files. Save whatever you want, wipe the drive, and send it—and your old system—off for recycling when you’re done. While this sounds like a process, removing a hard drive from your desktop PC is as easy as disconnecting a few cables and unscrewing four screws. Take care not to jostle the drive too much as you’re taking it out. Don’t turn it upside-down or anything crazy like that, and make sure you’ve touched the site of your (metal) case to discharge of any static electricity before you go fiddling with your system’s insides. Were I you, I’d go that route because it’s easy and it saves you money. Instead of spending $20-30 on a display you’ll never use again, you can pick up a docking station that you* *could definitely use at some future point, whether you’re pulling files off another drive, copying files over to a bare backup drive, et cetera. It’s infinitely more practical than a crappy old display, but it requires slightly more legwork for your situation. You can do it. I have faith. "
    }, {
    "id": 26,
    "url": "https://bright-softwares.com/blog/en/kubernetes/how-to-set-up-the-codeserver-cloud-ide-platform-on-digitalocean-kubernetes",
    "title": "How To Set Up the code-server Cloud IDE Platform on DigitalOcean Kubernetes",
    "body": "2021/11/03 - IntroductionWith developer tools moving to the cloud, creation and adoption of cloud IDE (Integrated Development Environment) platforms is growing. Cloud IDEs allow for real-time collaboration between developer teams to work in a unified development environment that minimizes incompatibilities and enhances productivity. Accessible through web browsers, cloud IDEs are available from every type of modern device. Another advantage of a cloud IDE is the possibility to leverage the power of a cluster, which can greatly exceed the processing power of a single development computer. code-server is Microsoft Visual Studio Code running on a remote server and accessible directly from your browser. Visual Studio Code is a modern code editor with integrated Git support, a code debugger, smart autocompletion, and customizable and extensible features. This means that you can use various devices, running different operating systems, and always have a consistent development environment on hand. In this tutorial, you will set up the code-server cloud IDE platform on your DigitalOcean Kubernetes cluster and expose it at your domain, secured with Let’s Encrypt certificates. In the end, you’ll have Microsoft Visual Studio Code running on your Kubernetes cluster, available via HTTPS and protected by a password. PrerequisitesA DigitalOcean Kubernetes cluster with your connection configured as the kubectl default. Instructions on how to configure kubectl are shown under the Connect to your Cluster step when you create your cluster. To create a Kubernetes cluster on DigitalOcean, see Kubernetes Quickstart. The Helm package manager installed on your local machine, and Tiller installed on your cluster. To do this, complete Steps 1 and 2 of the How To Install Software on Kubernetes Clusters with the Helm Package Manager tutorial. The Nginx Ingress Controller and Cert-Manager installed on your cluster using Helm in order to expose code-server using Ingress Resources. To do this, follow How to Set Up an Nginx Ingress on DigitalOcean Kubernetes Using Helm. A fully registered domain name to host code-server, pointed at the Load Balancer used by the Nginx Ingress. This tutorial will use code-server. your_domain throughout. You can purchase a domain name on Namecheap, get one for free on Freenom, or use the domain registrar of your choice. This domain name must differ from the one used in the How To Set Up an Nginx Ingress on DigitalOcean Kubernetes prerequisite tutorial. Step 1 — Installing And Exposing code-server: In this section, you’ll install code-server to your DigitalOcean Kubernetes cluster and expose it at your domain, using the Nginx Ingress controller. You will also set up a password for admittance. You’ll store the deployment configuration on your local machine, in a file named code-server. yaml. Create it using the following command: nano code-server. yamlAdd the following lines to the file: code-server. yaml apiVersion: v1kind: Namespacemetadata: name: code-server---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: code-server namespace: code-server annotations:  kubernetes. io/ingress. class: nginxspec: rules: - host: code-server. your_domain  http:   paths:   - backend:     serviceName: code-server     servicePort: 80---apiVersion: v1kind: Servicemetadata: name: code-server namespace: code-serverspec: ports: - port: 80  targetPort: 8443 selector:  app: code-server---apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels:  app: code-server name: code-server namespace: code-serverspec: selector:  matchLabels:   app: code-server replicas: 1 template:  metadata:   labels:    app: code-server  spec:   containers:   - image: codercom/code-server    imagePullPolicy: Always    name: code-server    args: [ --allow-http ]    ports:    - containerPort: 8443    env:    - name: PASSWORD     value:  your_password This configuration defines a Namespace, a Deployment, a Service, and an Ingress. The Namespace is called code-server and separates the code-server installation from the rest of your cluster. The Deployment consists of one replica of the codercom/code-server Docker image, and an environment variable named PASSWORD that specifies the password for access. The code-server Service internally exposes the pod (created as a part of the Deployment) at port 80. The Ingress defined in the file specifies that the Ingress Controller is nginx, and that the code-server. your_domain domain will be served from the Service. Remember to replace your_password with your desired password, and code-server. your_domain with your desired domain, pointed to the Load Balancer of the Nginx Ingress Controller. Then, create the configuration in Kubernetes by running the following command: kubectl create -f code-server. yamlYou’ll see the following output:Output namespace/code-server createdingress. extensions/code-server createdservice/code-server createddeployment. extensions/code-server createdYou can watch the code-server pod become available by running: kubectl get pods -w -n code-serverThe output will look like:Output NAME             READY  STATUS       RESTARTS  AGEcode-server-f85d9bfc9-j7hq6  0/1   ContainerCreating  0     1mAs soon as the status becomes Running, code-server has finished installing to your cluster. Navigate to your domain in your browser. You’ll see the login prompt for code-server. Enter the password you set in code-server. yaml and press Enter IDE. You’ll enter code-server and immediately see its editor GUI. You’ve installed code-server to your Kubernetes cluster and made it available at your domain. You have also verified that it requires you to log in with a password. Now, you’ll move on to secure it with free Let’s Encrypt certificates using Cert-Manager. Step 2 — Securing the code-server Deployment: In this section, you will secure your code-server installation by applying Let’s Encrypt certificates to your Ingress, which Cert-Manager will automatically create. After completing this step, your code-server installation will be accessible via HTTPS. Open code-server. yaml for editing: nano code-server. yamlAdd the highlighted lines to your file, making sure to replace the example domain with your own:code-server. yaml apiVersion: v1kind: Namespacemetadata: name: code-server---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: code-server namespace: code-server annotations:  kubernetes. io/ingress. class: nginx  certmanager. k8s. io/cluster-issuer: letsencrypt-prodspec: tls: - hosts:  - code-server. your_domain  secretName: codeserver-prod rules: - host: code-server. your_domain  http:   paths:   - backend:     serviceName: code-server     servicePort: 80. . . First, you specify that the cluster-issuer that this Ingress will use to provision certificates will be letsencrypt-prod, created as a part of the prerequisites. Then, you specify the domains that will be secured under the tls section, as well as your name for the Secret holding them. Apply the changes to your Kubernetes cluster by running the following command: kubectl apply -f code-server. yamlYou’ll need to wait a few minutes for Let’s Encrypt to provision your certificate. In the meantime, you can track its progress by looking at the output of the following command:kubectl describe certificate codeserver-prod -n code-server When it finishes, the end of the output will look similar to this:Output Events: Type  Reason       Age  From     Message ----  ------       ----  ----     ------- Normal Generated      2m49s cert-manager Generated new private key Normal GenerateSelfSigned 2m49s cert-manager Generated temporary self signed certificate Normal OrderCreated    2m49s cert-manager Created Order resource  codeserver-prod-4279678953  Normal OrderComplete    2m14s cert-manager Order  codeserver-prod-4279678953  completed successfully Normal CertIssued     2m14s cert-manager Certificate issued successfullyYou can now refresh your domain in your browser. You’ll see the padlock to the left of the address bar in your browser signifying that the connection is secure. In this step, you have configured the Ingress to secure your code-server deployment. Now, you can review the code-server user interface. Step 3 — Exploring the code-server Interface: In this section, you’ll explore some of the features of the code-server interface. Since code-server is Visual Studio Code running in the cloud, it has the same interface as the standalone desktop edition. On the left-hand side of the IDE, there is a vertical row of six buttons opening the most commonly used features in a side panel known as the Activity Bar. This bar is customizable so you can move these views to a different order or remove them from the bar. By default, the first view opens the Explorer panel that provides tree-like navigation of the project’s structure. You can manage your folders and files here—creating, deleting, moving, and renaming them as necessary. The next view provides access to a search and replace functionality. Following this, in the default order, is your view of the source control systems, like Git. Visual Studio code also supports other source control providers and you can find further instructions for source control workflows with the editor in this documentation. The debugger option on the Activity Bar provides all the common actions for debugging in the panel. Visual Studio Code comes with built-in support for the Node. js runtime debugger and any language that transpiles to Javascript. For other languages you can install extensions for the required debugger. You can save debugging configurations in the launch. json file. The final view in the Activity Bar provides a menu to access available extensions on the Marketplace. The central part of the GUI is your editor, which you can separate by tabs for your code editing. You can change your editing view to a grid system or to side-by-side files. After creating a new file through the File menu, an empty file will open in a new tab, and once saved, the file’s name will be viewable in the Explorer side panel. Creating folders can be done by right clicking on the Explorer sidebar and pressing on New Folder. You can expand a folder by clicking on its name as well as dragging and dropping files and folders to upper parts of the hierarchy to move them to a new location. You can gain access to a terminal by pressing CTRL+SHIFT+, or by pressing on Terminal in the upper menu, and selecting New Terminal. The terminal will open in a lower panel and its working directory will be set to the project’s workspace, which contains the files and folders shown in the Explorer side panel. You’ve explored a high-level overview of the code-server interface and reviewed some of the most commonly used features. ConclusionYou now have code-server, a versatile cloud IDE, installed on your DigitalOcean Kubernetes cluster. You can work on your source code and documents with it individually or collaborate with your team. Running a cloud IDE on your cluster provides more power for testing, downloading, and more thorough or rigorous computing. For further information see the Visual Studio Code documentation on additional features and detailed instructions on other components of code-server. "
    }, {
    "id": 27,
    "url": "https://bright-softwares.com/blog/fr/fax/how-to-send-and-receive-faxes-online-without-a-fax-machine-or-phone-line",
    "title": "How to Send and Receive Faxes Online Without a Fax Machine or Phone Line",
    "body": "2020/09/01 - Some slow-moving businesses and government agencies may not accept documents over email, forcing you to fax them in. If you are forced to send a fax, you can do it from your computer for free. How Fax Machines Work (and Why They’re So Inconvenient): This isn’t as easy as it should be. Fax machines are all connected to the plain old telephone lines. When you use a standard fax machine, that fax machine places a phone call to the number you specify. The fax machine at the destination number answers and the document is transmitted over a telephone call. This process was invented before the Internet and seems laughably archaic at this point. To perform a fax, a person may type up a document, print it out, and scan it into the fax machine which sends it over the phone line. The person receiving the fax may then scan the faxed document and turn it back into a digital file. They’ve come full circle — the document was sent from one computer to another computer with much additional work and lost image quality. Ideally, you’d be able to submit a document via email or a more secure online method. Many businesses consider fax a secure method of transmitting documents, but it really isn’t — if someone was snooping on the phone line, they could easily intercept all the faxed documents. There’s no way to connect to a fax machine directly over the Internet, as the fax machine is only connected to telephone lines. To perform a fax online, we’ll need some sort of gateway that accepts documents via the Internet and transmits the document to a fax machine. That’s where the below services come in. Give them a document and they’ll do the annoying work of dialing up the fax machine and sending your document over the telephone line.  You Could Fax With Just Your Computer, But…: You could skip the below services, of course. Microsoft Windows even contains a Fax and Scan application that allows you to send faxes. The catch is that you’d need your computer connected to the phone line — yes, this means that you’d need a dial-up fax modem. You’d also need a landline telephone connection and you’d have to tell people to stay off the phone when you’re sending faxes, just like in the old dial-up Internet days. Of course, if you were faxing a lot, you could pay for a dedicated fax telephone line — this might even be necessary if you were receiving a lot of faxes. This obviously isn’t ideal. Sure, if you need to send quite a few faxes, go ahead and buy a fax machine or modem and hook it up to your landline. But you probably don’t need to send and receive faxes this often — you hopefully just need to send the occasional fax whenever you bump into an organization that’s stuck in the past. Scan the Document or Use an Existing Digital File: The basic process is simple. First, you’ll need to scan the document you want to fax, just as if you were going to send that document over email. if you don’t have a scanner lying around, you may want to try scanning it with your smartphone. If the document is already a file on your computer, congratulations — you don’t have to scan anything. With the document now in digital form, you can send it along to a service that will do the annoying fax work for you.  Send Faxes Online, Free: There are so many online fax services out there that it’s hard to make an informed decision about which one to choose. The first thing to consider is what kind of a user you are, how often you’ll be faxing, and what features you need. If you are going to be sending sensitive faxes all the time, or you work for a company and you’re trying to choose a service, RingCentral Fax, which is partially owned by Cisco and AT&amp;T, is probably the best choice for your needs, especially since they have a lot of great security features and support for multiple users with separate fax lines. It has all the features you can imagine, including integrations with Outlook, Google Drive, Dropbox, Box, and you can even get a toll-free number. It also has a lot of security features that would be useful for businesses or people that are transmitting secure information. Of course, if you just want to send a few faxes, you can sign up for one of their cheap plans… and then just cancel after a month or two. Occasional User If you do need to send the occasional fax, we recommend signing up for a free trial of MyFax, which will let you send up to 100 pages, which is more pages per month than most people have to fax per year. If you do need to fax frequently, you can upgrade to a regular plan.  Receiving Faxes: If you do need to receive faxes, you’ll have to sign up for a paid service. The service will need to establish a dedicated phone number for your fax line, and that costs money. RingCentral, MyFax, and many other services will do this if you pay. Luckily, you should at least be able to get a free trial — RingCentral offers 30 days of free fax receiving, for example. There are many fax services to use, and if you just need to send the occasional fax, you can manage to do that for free, but if you want to receive a fax, you’ll end up needing to sign up for a trial account. You can always cancel if you want. Image Credit: Matt Jiggins on Flickr, David Voegtle on Flickr "
    }, {
    "id": 28,
    "url": "https://bright-softwares.com/blog/fr/screenshot/comment-prendre-une-capture-ecran-dans-windows",
    "title": "Comment prendre une capture d'écran dans windows",
    "body": "2020/09/01 - WindowsHow do I take a screenshot?: press PrtScn = Windows captures the entire screen and copies it to the (invisible) clipboard. Note: On some notebooks you have to hold Fn and then press PrtScn instead. ### Where can I find that key? PrtScn ScrLk Pause Look for this group of keys at the upper right of your keyboard. Note: Print Screen (PrtScn) might have been abbreviated differently on your keyboard. How do I take a screenshot of a single window?: hold down  Alt  and press  PrtScn = Windows captures only the currently active **window** and copies it to the **clipboard**. How do I take a screenshot of a specific area?: hold down ⊞ and Shift and press S = Use your mouse to draw a rectangle in order to specify what to capture. Windows then copies it to the clipboard. Since: Windows 10 Creators Update. I guess it’s in the clipboard now. How can I paste it into a document or something?: hold down Ctrl and press V = Windows pastes the screenshot (that is in the clipboard) into a document or image you are currently editing. Where should I paste it?: I just need a (graphics) file. Start “Paint”PasteSavetype “paint” into the search field How do I take a screenshot and have it directly as a file?: Hold down ⊞ and press PrtScn = Windows (8 or 10) captures the entire screen and saves it as a file to your Pictures &gt; Screenshots folder. Is there a dedicated program for taking screenshots? How can I find it?: type “snip” into the search field "
    }, {
    "id": 29,
    "url": "https://bright-softwares.com/blog/en/screenshot/how-to-take-a-screenshot-in-windows",
    "title": "How to take a screenshot in Windows",
    "body": "2020/09/01 - WindowsHow do I take a screenshot?: press PrtScn = Windows captures the entire screen and copies it to the (invisible) clipboard. Note: On some notebooks you have to hold Fn and then press PrtScn instead. ### Where can I find that key? PrtScn ScrLk Pause Look for this group of keys at the upper right of your keyboard. Note: Print Screen (PrtScn) might have been abbreviated differently on your keyboard. How do I take a screenshot of a single window?: hold down  Alt  and press  PrtScn = Windows captures only the currently active **window** and copies it to the **clipboard**. How do I take a screenshot of a specific area?: hold down ⊞ and Shift and press S = Use your mouse to draw a rectangle in order to specify what to capture. Windows then copies it to the clipboard. Since: Windows 10 Creators Update. I guess it’s in the clipboard now. How can I paste it into a document or something?: hold down Ctrl and press V = Windows pastes the screenshot (that is in the clipboard) into a document or image you are currently editing. Where should I paste it?: I just need a (graphics) file. Start “Paint”PasteSavetype “paint” into the search field How do I take a screenshot and have it directly as a file?: Hold down ⊞ and press PrtScn = Windows (8 or 10) captures the entire screen and saves it as a file to your Pictures &gt; Screenshots folder. Is there a dedicated program for taking screenshots? How can I find it?: type “snip” into the search field "
    }, {
    "id": 30,
    "url": "https://bright-softwares.com/blog/en/fax/how-to-send-and-receive-faxes-online-without-a-fax-machine-or-phone-line",
    "title": "How to Send and Receive Faxes Online Without a Fax Machine or Phone Line",
    "body": "2020/09/01 - Some slow-moving businesses and government agencies may not accept documents over email, forcing you to fax them in. If you are forced to send a fax, you can do it from your computer for free. How Fax Machines Work (and Why They’re So Inconvenient): This isn’t as easy as it should be. Fax machines are all connected to the plain old telephone lines. When you use a standard fax machine, that fax machine places a phone call to the number you specify. The fax machine at the destination number answers and the document is transmitted over a telephone call. This process was invented before the Internet and seems laughably archaic at this point. To perform a fax, a person may type up a document, print it out, and scan it into the fax machine which sends it over the phone line. The person receiving the fax may then scan the faxed document and turn it back into a digital file. They’ve come full circle — the document was sent from one computer to another computer with much additional work and lost image quality. Ideally, you’d be able to submit a document via email or a more secure online method. Many businesses consider fax a secure method of transmitting documents, but it really isn’t — if someone was snooping on the phone line, they could easily intercept all the faxed documents. There’s no way to connect to a fax machine directly over the Internet, as the fax machine is only connected to telephone lines. To perform a fax online, we’ll need some sort of gateway that accepts documents via the Internet and transmits the document to a fax machine. That’s where the below services come in. Give them a document and they’ll do the annoying work of dialing up the fax machine and sending your document over the telephone line.  You Could Fax With Just Your Computer, But…: You could skip the below services, of course. Microsoft Windows even contains a Fax and Scan application that allows you to send faxes. The catch is that you’d need your computer connected to the phone line — yes, this means that you’d need a dial-up fax modem. You’d also need a landline telephone connection and you’d have to tell people to stay off the phone when you’re sending faxes, just like in the old dial-up Internet days. Of course, if you were faxing a lot, you could pay for a dedicated fax telephone line — this might even be necessary if you were receiving a lot of faxes. This obviously isn’t ideal. Sure, if you need to send quite a few faxes, go ahead and buy a fax machine or modem and hook it up to your landline. But you probably don’t need to send and receive faxes this often — you hopefully just need to send the occasional fax whenever you bump into an organization that’s stuck in the past. Scan the Document or Use an Existing Digital File: The basic process is simple. First, you’ll need to scan the document you want to fax, just as if you were going to send that document over email. if you don’t have a scanner lying around, you may want to try scanning it with your smartphone. If the document is already a file on your computer, congratulations — you don’t have to scan anything. With the document now in digital form, you can send it along to a service that will do the annoying fax work for you.  Send Faxes Online, Free: There are so many online fax services out there that it’s hard to make an informed decision about which one to choose. The first thing to consider is what kind of a user you are, how often you’ll be faxing, and what features you need. If you are going to be sending sensitive faxes all the time, or you work for a company and you’re trying to choose a service, RingCentral Fax, which is partially owned by Cisco and AT&amp;T, is probably the best choice for your needs, especially since they have a lot of great security features and support for multiple users with separate fax lines. It has all the features you can imagine, including integrations with Outlook, Google Drive, Dropbox, Box, and you can even get a toll-free number. It also has a lot of security features that would be useful for businesses or people that are transmitting secure information. Of course, if you just want to send a few faxes, you can sign up for one of their cheap plans… and then just cancel after a month or two. Occasional User If you do need to send the occasional fax, we recommend signing up for a free trial of MyFax, which will let you send up to 100 pages, which is more pages per month than most people have to fax per year. If you do need to fax frequently, you can upgrade to a regular plan.  Receiving Faxes: If you do need to receive faxes, you’ll have to sign up for a paid service. The service will need to establish a dedicated phone number for your fax line, and that costs money. RingCentral, MyFax, and many other services will do this if you pay. Luckily, you should at least be able to get a free trial — RingCentral offers 30 days of free fax receiving, for example. There are many fax services to use, and if you just need to send the occasional fax, you can manage to do that for free, but if you want to receive a fax, you’ll end up needing to sign up for a trial account. You can always cancel if you want. Image Credit: Matt Jiggins on Flickr, David Voegtle on Flickr "
    }, {
    "id": 31,
    "url": "https://bright-softwares.com/blog/fr/kubernetes/travailler-avec-kubernetes-et-minikube",
    "title": "Travailler avec Kubernetes et Minikube",
    "body": "2020/08/13 - Installing Kubernetes with MinikubeMinikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Minikube Features : Minikube supports the following Kubernetes features:  DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, CRI-O, and containerd Enabling CNI (Container Network Interface) IngressInstallation See Installing Minikube. Quickstart : This brief demo guides you on how to start, use, and delete Minikube locally. Follow the steps given below to start and explore Minikube.  Start Minikube and create a cluster:minikube start The output is similar to this: Starting local Kubernetes cluster… Running pre-create checks… Creating machine… Starting local Kubernetes cluster… For more information on starting your cluster on a specific Kubernetes version, VM, or container runtime, see Starting a Cluster.  Now, you can interact with your cluster using kubectl. For more information, see Interacting with Your Cluster. Let’s create a Kubernetes Deployment using an existing image named echoserver, which is a simple HTTP server and expose it on port 8080 using –port. kubectl create deployment hello-minikube –image=k8s. gcr. io/echoserver:1. 10 The output is similar to this: deployment. apps/hello-minikube created 3. To access the hello-minikube Deployment, expose it as a Service: kubectl expose deployment hello-minikube –type=NodePort –port=8080 The option –type=NodePort specifies the type of the Service. The output is similar to this: service/hello-minikube exposed 4. The hello-minikube Pod is now launched but you have to wait until the Pod is up before accessing it via the exposed Service. Check if the Pod is up and running: kubectl get pod If the output shows the STATUS as ContainerCreating, the Pod is still being created: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 0/1 ContainerCreating 0 3s If the output shows the STATUS as Running, the Pod is now up and running: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 1/1 Running 0 13s 5. Get the URL of the exposed Service to view the Service details: minikube service hello-minikube –url 6. To view the details of your local cluster, copy and paste the URL you got as the output, on your browser. The output is similar to this: Hostname: hello-minikube-7c77b68cff-8wdzq Pod Information: -no pod information available- Server values: server_version=nginx: 1. 13. 3 - lua: 10008 Request Information: client_address=172. 17. 0. 1 method=GET real path=/ query= request_version=1. 1 request_scheme=http request_uri=http://192. 168. 99. 100:8080/ Request Headers: accept=/ host=192. 168. 99. 100:30674 user-agent=curl/7. 47. 0 Request Body: -no body in request- If you no longer want the Service and cluster to run, you can delete them.  Delete the hello-minikube Service:kubectl delete services hello-minikube The output is similar to this: service “hello-minikube” deleted 8. Delete the hello-minikube Deployment: kubectl delete deployment hello-minikube The output is similar to this: deployment. extensions “hello-minikube” deleted 9. Stop the local Minikube cluster: minikube stop The output is similar to this: Stopping “minikube”… “minikube” stopped. For more information, see Stopping a Cluster.  Delete the local Minikube cluster:minikube delete The output is similar to this: Deleting “minikube” … The “minikube” cluster has been deleted. For more information, see Deleting a cluster. Managing your Cluster : Starting a Cluster : The minikube start command can be used to start your cluster. This command creates and configures a Virtual Machine that runs a single-node Kubernetes cluster. This command also configures your kubectl installation to communicate with this cluster.  Note:If you are behind a web proxy, you need to pass this information to the minikube start command:  https_proxy= minikube start --docker-env http_proxy= --docker-env https_proxy= --docker-env no_proxy=192. 168. 99. 0/24 Unfortunately, setting the environment variables alone does not work.  Minikube also creates a “minikube” context, and sets it to default in kubectl. To switch back to this context, run this command: kubectl config use-context minikube.  Specifying the Kubernetes version : You can specify the version of Kubernetes for Minikube to use by adding the –kubernetes-version string to the minikube start command. For example, to run version v1. 18. 0, you would run the following: minikube start –kubernetes-version v1. 18. 0 #### Specifying the VM driver You can change the VM driver by adding the –driver= flag to minikube start. For example the command would be. minikube start –driver= Minikube supports the following drivers:  Note: See DRIVERS for details on supported drivers and how to install plugins. * docker (driver installation)  virtualbox (driver installation) podman (driver installation) (EXPERIMENTAL) vmwarefusion kvm2 (driver installation) hyperkit (driver installation) hyperv (driver installation) Note that the IP below is dynamic and can change. It can be retrieved with minikube ip.  vmware (driver installation) (VMware unified driver) parallels (driver installation) none (Runs the Kubernetes components on the host and not in a virtual machine. You need to be running Linux and to have DockerDocker is a software technology providing operating-system-level virtualization also known as containers. installed. ) Caution: If you use the none driver, some Kubernetes components run as privileged containers that have side effects outside of the Minikube environment. Those side effects mean that the none driver is not recommended for personal workstations. #### Starting a cluster on alternative container runtimes You can start Minikube on the following container runtimes.  containerd CRI-OTo use containerd as the container runtime, run:minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=containerd \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=unix:///run/containerd/containerd. sock \ –extra-config=kubelet. image-service-endpoint=unix:///run/containerd/containerd. sock \ –bootstrapper=kubeadm To use CRI-O as the container runtime, run: minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=cri-o \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=/var/run/crio. sock \ –extra-config=kubelet. image-service-endpoint=/var/run/crio. sock \ –bootstrapper=kubeadm $(function(){$(“#container-runtimes”). tabs();});#### Use local images by re-using the Docker daemon When using a single VM for Kubernetes, it’s useful to reuse Minikube’s built-in Docker daemon. Reusing the built-in daemon means you don’t have to build a Docker registry on your host machine and push the image into it. Instead, you can build inside the same Docker daemon as Minikube, which speeds up local experiments.  Note: Be sure to tag your Docker image with something other than latest and use that tag to pull the image. Because :latest is the default value, with a corresponding default image pull policy of Always, an image pull error (ErrImagePull) eventually results if you do not have the Docker image in the default Docker registry (usually DockerHub). To work with the Docker daemon on your Mac/Linux host, run the last line from minikube docker-env. You can now use Docker at the command line of your host Mac/Linux machine to communicate with the Docker daemon inside the Minikube VM: docker ps  Note:On Centos 7, Docker may report the following error:  Could not read CA certificate “/etc/docker/ca. pem”: open /etc/docker/ca. pem: no such file or directory You can fix this by updating /etc/sysconfig/docker to ensure that Minikube’s environment changes are respected:  &lt; DOCKER_CERT_PATH=/etc/docker — &gt; if [ -z “${DOCKER_CERT_PATH}” ]; then &gt; DOCKER_CERT_PATH=/etc/docker &gt; fi ### Configuring Kubernetes Minikube has a “configurator” feature that allows users to configure the Kubernetes components with arbitrary values. To use this feature, you can use the –extra-config flag on the minikube start command. This flag is repeated, so you can pass it several times with several different values to set multiple options. This flag takes a string of the form component. key=value, where component is one of the strings from the below list, key is a value on the configuration struct and value is the value to set. Valid keys can be found by examining the documentation for the Kubernetes componentconfigs for each component. Here is the documentation for each supported configuration:  kubelet apiserver proxy controller-manager etcd schedulerExamples : To change the MaxPods setting to 5 on the Kubelet, pass this flag: –extra-config=kubelet. MaxPods=5. This feature also supports nested structs. To change the LeaderElection. LeaderElect setting to true on the scheduler, pass this flag: –extra-config=scheduler. LeaderElection. LeaderElect=true. To set the AuthorizationMode on the apiserver to RBAC, you can use: –extra-config=apiserver. authorization-mode=RBAC. Stopping a Cluster : The minikube stop command can be used to stop your cluster. This command shuts down the Minikube Virtual Machine, but preserves all cluster state and data. Starting the cluster again will restore it to its previous state. Deleting a Cluster : The minikube delete command can be used to delete your cluster. This command shuts down and deletes the Minikube Virtual Machine. No data or state is preserved. Upgrading Minikube : If you are using macOS and Brew Package Manager is installed run: brew update brew upgrade minikube Interacting with Your Cluster : Kubectl : The minikube start command creates a kubectl context called “minikube”. This context contains the configuration to communicate with your Minikube cluster. Minikube sets this context to default automatically, but if you need to switch back to it in the future, run: kubectl config use-context minikube Or pass the context on each command like this: kubectl get pods –context=minikube Dashboard : To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address: minikube dashboard ### Services To access a Service exposed via a node port, run this command in a shell after starting Minikube to get the address: minikube service [-n NAMESPACE] [–url] NAME Networking : The Minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command. Any services of type NodePort can be accessed over that IP address, on the NodePort. To determine the NodePort for your service, you can use a kubectl command like this: kubectl get service $SERVICE –output=’jsonpath=”{. spec. ports[0]. nodePort}”’ Persistent Volumes : Minikube supports PersistentVolumes of type hostPath. These PersistentVolumes are mapped to a directory inside the Minikube VM. The Minikube VM boots into a tmpfs, so most directories will not be persisted across reboots (minikube stop). However, Minikube is configured to persist files stored under the following host directories:  /data /var/lib/minikube /var/lib/dockerHere is an example PersistentVolume config to persist data in the /data directory:apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: data/pv0001/ Mounted Host Folders : Some drivers will mount a host folder within the VM so that you can easily share files between the VM and host. These are not configurable at the moment and different for the driver and OS you are using.  Note: Host folder sharing is not implemented in the KVM driver yet. DriverOSHostFolderVMVirtualBoxLinux/home/hosthomeVirtualBoxmacOS/Users/UsersVirtualBoxWindowsC://Users/c/UsersVMware FusionmacOS/Users/mnt/hgfs/UsersXhyvemacOS/Users/UsersPrivate Container Registries To access a private container registry, follow the steps on this page. We recommend you use ImagePullSecrets, but if you would like to configure access on the Minikube VM you can place the . dockercfg in the /home/docker directory or the config. json in the /home/docker/. docker directory. Add-ons : In order to have Minikube properly start or restart custom addons, place the addons you wish to be launched with Minikube in the ~/. minikube/addons directory. Addons in this folder will be moved to the Minikube VM and launched each time Minikube is started or restarted. Using Minikube with an HTTP Proxy : Minikube creates a Virtual Machine that includes Kubernetes and a Docker daemon. When Kubernetes attempts to schedule containers using Docker, the Docker daemon may require external network access to pull containers. If you are behind an HTTP proxy, you may need to supply Docker with the proxy settings. To do this, pass the required environment variables as flags during minikube start. For example: minikube start –docker-env http_proxy=http://$YOURPROXY:PORT \ –docker-env https_proxy=https://$YOURPROXY:PORT If your Virtual Machine address is 192. 168. 99. 100, then chances are your proxy settings will prevent kubectl from directly reaching it. To by-pass proxy configuration for this IP address, you should modify your no_proxy settings. You can do so with: export no_proxy=$no_proxy,$(minikube ip) Known Issues : Features that require multiple nodes will not work in Minikube. Design : Minikube uses libmachine for provisioning VMs, and kubeadm to provision a Kubernetes cluster. For more information about Minikube, see the proposal. Additional Links :  Goals and Non-Goals: For the goals and non-goals of the Minikube project, please see our roadmap.  Development Guide: See Contributing for an overview of how to send pull requests.  Building Minikube: For instructions on how to build/test Minikube from source, see the build guide.  Adding a New Dependency: For instructions on how to add a new dependency to Minikube, see the adding dependencies guide.  Adding a New Addon: For instructions on how to add a new addon for Minikube, see the adding an addon guide.  MicroK8s: Linux users wishing to avoid running a virtual machine may consider MicroK8s as an alternative. Community Contributions, questions, and comments are all welcomed and encouraged! Minikube developers hang out on Slack in the #minikube channel (get an invitation here). We also have the kubernetes-dev Google Groups mailing list. If you are posting to the list please prefix your subject with “minikube: “. Feedback: Was this page helpful? Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub repo if you want to report a problem or suggest an improvement. "
    }, {
    "id": 32,
    "url": "https://bright-softwares.com/blog/en/kubernetes/work-with-kubernetes-with-minikube",
    "title": "Work with Kubernetes with Minikube",
    "body": "2020/08/13 - Installing Kubernetes with MinikubeMinikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Minikube Features : Minikube supports the following Kubernetes features:  DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, CRI-O, and containerd Enabling CNI (Container Network Interface) IngressInstallation See Installing Minikube. Quickstart : This brief demo guides you on how to start, use, and delete Minikube locally. Follow the steps given below to start and explore Minikube.  Start Minikube and create a cluster:minikube start The output is similar to this: Starting local Kubernetes cluster… Running pre-create checks… Creating machine… Starting local Kubernetes cluster… For more information on starting your cluster on a specific Kubernetes version, VM, or container runtime, see Starting a Cluster.  Now, you can interact with your cluster using kubectl. For more information, see Interacting with Your Cluster. Let’s create a Kubernetes Deployment using an existing image named echoserver, which is a simple HTTP server and expose it on port 8080 using –port. kubectl create deployment hello-minikube –image=k8s. gcr. io/echoserver:1. 10 The output is similar to this: deployment. apps/hello-minikube created 3. To access the hello-minikube Deployment, expose it as a Service: kubectl expose deployment hello-minikube –type=NodePort –port=8080 The option –type=NodePort specifies the type of the Service. The output is similar to this: service/hello-minikube exposed 4. The hello-minikube Pod is now launched but you have to wait until the Pod is up before accessing it via the exposed Service. Check if the Pod is up and running: kubectl get pod If the output shows the STATUS as ContainerCreating, the Pod is still being created: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 0/1 ContainerCreating 0 3s If the output shows the STATUS as Running, the Pod is now up and running: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 1/1 Running 0 13s 5. Get the URL of the exposed Service to view the Service details: minikube service hello-minikube –url 6. To view the details of your local cluster, copy and paste the URL you got as the output, on your browser. The output is similar to this: Hostname: hello-minikube-7c77b68cff-8wdzq Pod Information: -no pod information available- Server values: server_version=nginx: 1. 13. 3 - lua: 10008 Request Information: client_address=172. 17. 0. 1 method=GET real path=/ query= request_version=1. 1 request_scheme=http request_uri=http://192. 168. 99. 100:8080/ Request Headers: accept=/ host=192. 168. 99. 100:30674 user-agent=curl/7. 47. 0 Request Body: -no body in request- If you no longer want the Service and cluster to run, you can delete them.  Delete the hello-minikube Service:kubectl delete services hello-minikube The output is similar to this: service “hello-minikube” deleted 8. Delete the hello-minikube Deployment: kubectl delete deployment hello-minikube The output is similar to this: deployment. extensions “hello-minikube” deleted 9. Stop the local Minikube cluster: minikube stop The output is similar to this: Stopping “minikube”… “minikube” stopped. For more information, see Stopping a Cluster.  Delete the local Minikube cluster:minikube delete The output is similar to this: Deleting “minikube” … The “minikube” cluster has been deleted. For more information, see Deleting a cluster. Managing your Cluster : Starting a Cluster : The minikube start command can be used to start your cluster. This command creates and configures a Virtual Machine that runs a single-node Kubernetes cluster. This command also configures your kubectl installation to communicate with this cluster.  Note:If you are behind a web proxy, you need to pass this information to the minikube start command:  https_proxy= minikube start --docker-env http_proxy= --docker-env https_proxy= --docker-env no_proxy=192. 168. 99. 0/24 Unfortunately, setting the environment variables alone does not work.  Minikube also creates a “minikube” context, and sets it to default in kubectl. To switch back to this context, run this command: kubectl config use-context minikube.  Specifying the Kubernetes version : You can specify the version of Kubernetes for Minikube to use by adding the –kubernetes-version string to the minikube start command. For example, to run version v1. 18. 0, you would run the following: minikube start –kubernetes-version v1. 18. 0 #### Specifying the VM driver You can change the VM driver by adding the –driver= flag to minikube start. For example the command would be. minikube start –driver= Minikube supports the following drivers:  Note: See DRIVERS for details on supported drivers and how to install plugins. * docker (driver installation)  virtualbox (driver installation) podman (driver installation) (EXPERIMENTAL) vmwarefusion kvm2 (driver installation) hyperkit (driver installation) hyperv (driver installation) Note that the IP below is dynamic and can change. It can be retrieved with minikube ip.  vmware (driver installation) (VMware unified driver) parallels (driver installation) none (Runs the Kubernetes components on the host and not in a virtual machine. You need to be running Linux and to have DockerDocker is a software technology providing operating-system-level virtualization also known as containers. installed. ) Caution: If you use the none driver, some Kubernetes components run as privileged containers that have side effects outside of the Minikube environment. Those side effects mean that the none driver is not recommended for personal workstations. #### Starting a cluster on alternative container runtimes You can start Minikube on the following container runtimes.  containerd CRI-OTo use containerd as the container runtime, run:minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=containerd \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=unix:///run/containerd/containerd. sock \ –extra-config=kubelet. image-service-endpoint=unix:///run/containerd/containerd. sock \ –bootstrapper=kubeadm To use CRI-O as the container runtime, run: minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=cri-o \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=/var/run/crio. sock \ –extra-config=kubelet. image-service-endpoint=/var/run/crio. sock \ –bootstrapper=kubeadm $(function(){$(“#container-runtimes”). tabs();});#### Use local images by re-using the Docker daemon When using a single VM for Kubernetes, it’s useful to reuse Minikube’s built-in Docker daemon. Reusing the built-in daemon means you don’t have to build a Docker registry on your host machine and push the image into it. Instead, you can build inside the same Docker daemon as Minikube, which speeds up local experiments.  Note: Be sure to tag your Docker image with something other than latest and use that tag to pull the image. Because :latest is the default value, with a corresponding default image pull policy of Always, an image pull error (ErrImagePull) eventually results if you do not have the Docker image in the default Docker registry (usually DockerHub). To work with the Docker daemon on your Mac/Linux host, run the last line from minikube docker-env. You can now use Docker at the command line of your host Mac/Linux machine to communicate with the Docker daemon inside the Minikube VM: docker ps  Note:On Centos 7, Docker may report the following error:  Could not read CA certificate “/etc/docker/ca. pem”: open /etc/docker/ca. pem: no such file or directory You can fix this by updating /etc/sysconfig/docker to ensure that Minikube’s environment changes are respected:  &lt; DOCKER_CERT_PATH=/etc/docker — &gt; if [ -z “${DOCKER_CERT_PATH}” ]; then &gt; DOCKER_CERT_PATH=/etc/docker &gt; fi ### Configuring Kubernetes Minikube has a “configurator” feature that allows users to configure the Kubernetes components with arbitrary values. To use this feature, you can use the –extra-config flag on the minikube start command. This flag is repeated, so you can pass it several times with several different values to set multiple options. This flag takes a string of the form component. key=value, where component is one of the strings from the below list, key is a value on the configuration struct and value is the value to set. Valid keys can be found by examining the documentation for the Kubernetes componentconfigs for each component. Here is the documentation for each supported configuration:  kubelet apiserver proxy controller-manager etcd schedulerExamples : To change the MaxPods setting to 5 on the Kubelet, pass this flag: –extra-config=kubelet. MaxPods=5. This feature also supports nested structs. To change the LeaderElection. LeaderElect setting to true on the scheduler, pass this flag: –extra-config=scheduler. LeaderElection. LeaderElect=true. To set the AuthorizationMode on the apiserver to RBAC, you can use: –extra-config=apiserver. authorization-mode=RBAC. Stopping a Cluster : The minikube stop command can be used to stop your cluster. This command shuts down the Minikube Virtual Machine, but preserves all cluster state and data. Starting the cluster again will restore it to its previous state. Deleting a Cluster : The minikube delete command can be used to delete your cluster. This command shuts down and deletes the Minikube Virtual Machine. No data or state is preserved. Upgrading Minikube : If you are using macOS and Brew Package Manager is installed run: brew update brew upgrade minikube Interacting with Your Cluster : Kubectl : The minikube start command creates a kubectl context called “minikube”. This context contains the configuration to communicate with your Minikube cluster. Minikube sets this context to default automatically, but if you need to switch back to it in the future, run: kubectl config use-context minikube Or pass the context on each command like this: kubectl get pods –context=minikube Dashboard : To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address: minikube dashboard ### Services To access a Service exposed via a node port, run this command in a shell after starting Minikube to get the address: minikube service [-n NAMESPACE] [–url] NAME Networking : The Minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command. Any services of type NodePort can be accessed over that IP address, on the NodePort. To determine the NodePort for your service, you can use a kubectl command like this: kubectl get service $SERVICE –output=’jsonpath=”{. spec. ports[0]. nodePort}”’ Persistent Volumes : Minikube supports PersistentVolumes of type hostPath. These PersistentVolumes are mapped to a directory inside the Minikube VM. The Minikube VM boots into a tmpfs, so most directories will not be persisted across reboots (minikube stop). However, Minikube is configured to persist files stored under the following host directories:  /data /var/lib/minikube /var/lib/dockerHere is an example PersistentVolume config to persist data in the /data directory:apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: data/pv0001/ Mounted Host Folders : Some drivers will mount a host folder within the VM so that you can easily share files between the VM and host. These are not configurable at the moment and different for the driver and OS you are using.  Note: Host folder sharing is not implemented in the KVM driver yet. DriverOSHostFolderVMVirtualBoxLinux/home/hosthomeVirtualBoxmacOS/Users/UsersVirtualBoxWindowsC://Users/c/UsersVMware FusionmacOS/Users/mnt/hgfs/UsersXhyvemacOS/Users/UsersPrivate Container Registries To access a private container registry, follow the steps on this page. We recommend you use ImagePullSecrets, but if you would like to configure access on the Minikube VM you can place the . dockercfg in the /home/docker directory or the config. json in the /home/docker/. docker directory. Add-ons : In order to have Minikube properly start or restart custom addons, place the addons you wish to be launched with Minikube in the ~/. minikube/addons directory. Addons in this folder will be moved to the Minikube VM and launched each time Minikube is started or restarted. Using Minikube with an HTTP Proxy : Minikube creates a Virtual Machine that includes Kubernetes and a Docker daemon. When Kubernetes attempts to schedule containers using Docker, the Docker daemon may require external network access to pull containers. If you are behind an HTTP proxy, you may need to supply Docker with the proxy settings. To do this, pass the required environment variables as flags during minikube start. For example: minikube start –docker-env http_proxy=http://$YOURPROXY:PORT \ –docker-env https_proxy=https://$YOURPROXY:PORT If your Virtual Machine address is 192. 168. 99. 100, then chances are your proxy settings will prevent kubectl from directly reaching it. To by-pass proxy configuration for this IP address, you should modify your no_proxy settings. You can do so with: export no_proxy=$no_proxy,$(minikube ip) Known Issues : Features that require multiple nodes will not work in Minikube. Design : Minikube uses libmachine for provisioning VMs, and kubeadm to provision a Kubernetes cluster. For more information about Minikube, see the proposal. Additional Links :  Goals and Non-Goals: For the goals and non-goals of the Minikube project, please see our roadmap.  Development Guide: See Contributing for an overview of how to send pull requests.  Building Minikube: For instructions on how to build/test Minikube from source, see the build guide.  Adding a New Dependency: For instructions on how to add a new dependency to Minikube, see the adding dependencies guide.  Adding a New Addon: For instructions on how to add a new addon for Minikube, see the adding an addon guide.  MicroK8s: Linux users wishing to avoid running a virtual machine may consider MicroK8s as an alternative. Community Contributions, questions, and comments are all welcomed and encouraged! Minikube developers hang out on Slack in the #minikube channel (get an invitation here). We also have the kubernetes-dev Google Groups mailing list. If you are posting to the list please prefix your subject with “minikube: “. Feedback: Was this page helpful? Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub repo if you want to report a problem or suggest an improvement. "
    }, {
    "id": 33,
    "url": "https://bright-softwares.com/blog/fr/kubernetes/play-with-kubernetes-with-minikube",
    "title": "Play with Kubernetes with Minikube",
    "body": "2020/08/12 - Installing Kubernetes with MinikubeMinikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Minikube Features : Minikube supports the following Kubernetes features:  DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, CRI-O, and containerd Enabling CNI (Container Network Interface) IngressInstallation See Installing Minikube. Quickstart : This brief demo guides you on how to start, use, and delete Minikube locally. Follow the steps given below to start and explore Minikube.  Start Minikube and create a cluster:minikube start The output is similar to this: Starting local Kubernetes cluster… Running pre-create checks… Creating machine… Starting local Kubernetes cluster… For more information on starting your cluster on a specific Kubernetes version, VM, or container runtime, see Starting a Cluster.  Now, you can interact with your cluster using kubectl. For more information, see Interacting with Your Cluster. Let’s create a Kubernetes Deployment using an existing image named echoserver, which is a simple HTTP server and expose it on port 8080 using –port. kubectl create deployment hello-minikube –image=k8s. gcr. io/echoserver:1. 10 The output is similar to this: deployment. apps/hello-minikube created 3. To access the hello-minikube Deployment, expose it as a Service: kubectl expose deployment hello-minikube –type=NodePort –port=8080 The option –type=NodePort specifies the type of the Service. The output is similar to this: service/hello-minikube exposed 4. The hello-minikube Pod is now launched but you have to wait until the Pod is up before accessing it via the exposed Service. Check if the Pod is up and running: kubectl get pod If the output shows the STATUS as ContainerCreating, the Pod is still being created: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 0/1 ContainerCreating 0 3s If the output shows the STATUS as Running, the Pod is now up and running: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 1/1 Running 0 13s 5. Get the URL of the exposed Service to view the Service details: minikube service hello-minikube –url 6. To view the details of your local cluster, copy and paste the URL you got as the output, on your browser. The output is similar to this: Hostname: hello-minikube-7c77b68cff-8wdzq Pod Information: -no pod information available- Server values: server_version=nginx: 1. 13. 3 - lua: 10008 Request Information: client_address=172. 17. 0. 1 method=GET real path=/ query= request_version=1. 1 request_scheme=http request_uri=http://192. 168. 99. 100:8080/ Request Headers: accept=/ host=192. 168. 99. 100:30674 user-agent=curl/7. 47. 0 Request Body: -no body in request- If you no longer want the Service and cluster to run, you can delete them.  Delete the hello-minikube Service:kubectl delete services hello-minikube The output is similar to this: service “hello-minikube” deleted 8. Delete the hello-minikube Deployment: kubectl delete deployment hello-minikube The output is similar to this: deployment. extensions “hello-minikube” deleted 9. Stop the local Minikube cluster: minikube stop The output is similar to this: Stopping “minikube”… “minikube” stopped. For more information, see Stopping a Cluster.  Delete the local Minikube cluster:minikube delete The output is similar to this: Deleting “minikube” … The “minikube” cluster has been deleted. For more information, see Deleting a cluster. Managing your Cluster : Starting a Cluster : The minikube start command can be used to start your cluster. This command creates and configures a Virtual Machine that runs a single-node Kubernetes cluster. This command also configures your kubectl installation to communicate with this cluster.  Note:If you are behind a web proxy, you need to pass this information to the minikube start command:  https_proxy= minikube start --docker-env http_proxy= --docker-env https_proxy= --docker-env no_proxy=192. 168. 99. 0/24 Unfortunately, setting the environment variables alone does not work.  Minikube also creates a “minikube” context, and sets it to default in kubectl. To switch back to this context, run this command: kubectl config use-context minikube.  Specifying the Kubernetes version : You can specify the version of Kubernetes for Minikube to use by adding the –kubernetes-version string to the minikube start command. For example, to run version v1. 18. 0, you would run the following: minikube start –kubernetes-version v1. 18. 0 #### Specifying the VM driver You can change the VM driver by adding the –driver= flag to minikube start. For example the command would be. minikube start –driver= Minikube supports the following drivers:  Note: See DRIVERS for details on supported drivers and how to install plugins. * docker (driver installation)  virtualbox (driver installation) podman (driver installation) (EXPERIMENTAL) vmwarefusion kvm2 (driver installation) hyperkit (driver installation) hyperv (driver installation) Note that the IP below is dynamic and can change. It can be retrieved with minikube ip.  vmware (driver installation) (VMware unified driver) parallels (driver installation) none (Runs the Kubernetes components on the host and not in a virtual machine. You need to be running Linux and to have DockerDocker is a software technology providing operating-system-level virtualization also known as containers. installed. ) Caution: If you use the none driver, some Kubernetes components run as privileged containers that have side effects outside of the Minikube environment. Those side effects mean that the none driver is not recommended for personal workstations. #### Starting a cluster on alternative container runtimes You can start Minikube on the following container runtimes.  containerd CRI-OTo use containerd as the container runtime, run:minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=containerd \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=unix:///run/containerd/containerd. sock \ –extra-config=kubelet. image-service-endpoint=unix:///run/containerd/containerd. sock \ –bootstrapper=kubeadm To use CRI-O as the container runtime, run: minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=cri-o \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=/var/run/crio. sock \ –extra-config=kubelet. image-service-endpoint=/var/run/crio. sock \ –bootstrapper=kubeadm $(function(){$(“#container-runtimes”). tabs();});#### Use local images by re-using the Docker daemon When using a single VM for Kubernetes, it’s useful to reuse Minikube’s built-in Docker daemon. Reusing the built-in daemon means you don’t have to build a Docker registry on your host machine and push the image into it. Instead, you can build inside the same Docker daemon as Minikube, which speeds up local experiments.  Note: Be sure to tag your Docker image with something other than latest and use that tag to pull the image. Because :latest is the default value, with a corresponding default image pull policy of Always, an image pull error (ErrImagePull) eventually results if you do not have the Docker image in the default Docker registry (usually DockerHub). To work with the Docker daemon on your Mac/Linux host, run the last line from minikube docker-env. You can now use Docker at the command line of your host Mac/Linux machine to communicate with the Docker daemon inside the Minikube VM: docker ps  Note:On Centos 7, Docker may report the following error:  Could not read CA certificate “/etc/docker/ca. pem”: open /etc/docker/ca. pem: no such file or directory You can fix this by updating /etc/sysconfig/docker to ensure that Minikube’s environment changes are respected:  &lt; DOCKER_CERT_PATH=/etc/docker — &gt; if [ -z “${DOCKER_CERT_PATH}” ]; then &gt; DOCKER_CERT_PATH=/etc/docker &gt; fi ### Configuring Kubernetes Minikube has a “configurator” feature that allows users to configure the Kubernetes components with arbitrary values. To use this feature, you can use the –extra-config flag on the minikube start command. This flag is repeated, so you can pass it several times with several different values to set multiple options. This flag takes a string of the form component. key=value, where component is one of the strings from the below list, key is a value on the configuration struct and value is the value to set. Valid keys can be found by examining the documentation for the Kubernetes componentconfigs for each component. Here is the documentation for each supported configuration:  kubelet apiserver proxy controller-manager etcd schedulerExamples : To change the MaxPods setting to 5 on the Kubelet, pass this flag: –extra-config=kubelet. MaxPods=5. This feature also supports nested structs. To change the LeaderElection. LeaderElect setting to true on the scheduler, pass this flag: –extra-config=scheduler. LeaderElection. LeaderElect=true. To set the AuthorizationMode on the apiserver to RBAC, you can use: –extra-config=apiserver. authorization-mode=RBAC. Stopping a Cluster : The minikube stop command can be used to stop your cluster. This command shuts down the Minikube Virtual Machine, but preserves all cluster state and data. Starting the cluster again will restore it to its previous state. Deleting a Cluster : The minikube delete command can be used to delete your cluster. This command shuts down and deletes the Minikube Virtual Machine. No data or state is preserved. Upgrading Minikube : If you are using macOS and Brew Package Manager is installed run: brew update brew upgrade minikube Interacting with Your Cluster : Kubectl : The minikube start command creates a kubectl context called “minikube”. This context contains the configuration to communicate with your Minikube cluster. Minikube sets this context to default automatically, but if you need to switch back to it in the future, run: kubectl config use-context minikube Or pass the context on each command like this: kubectl get pods –context=minikube Dashboard : To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address: minikube dashboard ### Services To access a Service exposed via a node port, run this command in a shell after starting Minikube to get the address: minikube service [-n NAMESPACE] [–url] NAME Networking : The Minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command. Any services of type NodePort can be accessed over that IP address, on the NodePort. To determine the NodePort for your service, you can use a kubectl command like this: kubectl get service $SERVICE –output=’jsonpath=”{. spec. ports[0]. nodePort}”’ Persistent Volumes : Minikube supports PersistentVolumes of type hostPath. These PersistentVolumes are mapped to a directory inside the Minikube VM. The Minikube VM boots into a tmpfs, so most directories will not be persisted across reboots (minikube stop). However, Minikube is configured to persist files stored under the following host directories:  /data /var/lib/minikube /var/lib/dockerHere is an example PersistentVolume config to persist data in the /data directory:apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: data/pv0001/ Mounted Host Folders : Some drivers will mount a host folder within the VM so that you can easily share files between the VM and host. These are not configurable at the moment and different for the driver and OS you are using.  Note: Host folder sharing is not implemented in the KVM driver yet. DriverOSHostFolderVMVirtualBoxLinux/home/hosthomeVirtualBoxmacOS/Users/UsersVirtualBoxWindowsC://Users/c/UsersVMware FusionmacOS/Users/mnt/hgfs/UsersXhyvemacOS/Users/UsersPrivate Container Registries To access a private container registry, follow the steps on this page. We recommend you use ImagePullSecrets, but if you would like to configure access on the Minikube VM you can place the . dockercfg in the /home/docker directory or the config. json in the /home/docker/. docker directory. Add-ons : In order to have Minikube properly start or restart custom addons, place the addons you wish to be launched with Minikube in the ~/. minikube/addons directory. Addons in this folder will be moved to the Minikube VM and launched each time Minikube is started or restarted. Using Minikube with an HTTP Proxy : Minikube creates a Virtual Machine that includes Kubernetes and a Docker daemon. When Kubernetes attempts to schedule containers using Docker, the Docker daemon may require external network access to pull containers. If you are behind an HTTP proxy, you may need to supply Docker with the proxy settings. To do this, pass the required environment variables as flags during minikube start. For example: minikube start –docker-env http_proxy=http://$YOURPROXY:PORT \ –docker-env https_proxy=https://$YOURPROXY:PORT If your Virtual Machine address is 192. 168. 99. 100, then chances are your proxy settings will prevent kubectl from directly reaching it. To by-pass proxy configuration for this IP address, you should modify your no_proxy settings. You can do so with: export no_proxy=$no_proxy,$(minikube ip) Known Issues : Features that require multiple nodes will not work in Minikube. Design : Minikube uses libmachine for provisioning VMs, and kubeadm to provision a Kubernetes cluster. For more information about Minikube, see the proposal. Additional Links :  Goals and Non-Goals: For the goals and non-goals of the Minikube project, please see our roadmap.  Development Guide: See Contributing for an overview of how to send pull requests.  Building Minikube: For instructions on how to build/test Minikube from source, see the build guide.  Adding a New Dependency: For instructions on how to add a new dependency to Minikube, see the adding dependencies guide.  Adding a New Addon: For instructions on how to add a new addon for Minikube, see the adding an addon guide.  MicroK8s: Linux users wishing to avoid running a virtual machine may consider MicroK8s as an alternative. Community Contributions, questions, and comments are all welcomed and encouraged! Minikube developers hang out on Slack in the #minikube channel (get an invitation here). We also have the kubernetes-dev Google Groups mailing list. If you are posting to the list please prefix your subject with “minikube: “. Feedback: Was this page helpful? Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub repo if you want to report a problem or suggest an improvement. "
    }, {
    "id": 34,
    "url": "https://bright-softwares.com/blog/fr/kubernetes/installing-kubernetes-with-minikube",
    "title": "Installing Kubernetes with Minikube",
    "body": "2020/08/12 - Installing Kubernetes with MinikubeMinikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Minikube Features : Minikube supports the following Kubernetes features:  DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, CRI-O, and containerd Enabling CNI (Container Network Interface) IngressInstallation See Installing Minikube. Quickstart : This brief demo guides you on how to start, use, and delete Minikube locally. Follow the steps given below to start and explore Minikube.  Start Minikube and create a cluster:minikube start The output is similar to this: Starting local Kubernetes cluster… Running pre-create checks… Creating machine… Starting local Kubernetes cluster… For more information on starting your cluster on a specific Kubernetes version, VM, or container runtime, see Starting a Cluster.  Now, you can interact with your cluster using kubectl. For more information, see Interacting with Your Cluster. Let’s create a Kubernetes Deployment using an existing image named echoserver, which is a simple HTTP server and expose it on port 8080 using –port. kubectl create deployment hello-minikube –image=k8s. gcr. io/echoserver:1. 10 The output is similar to this: deployment. apps/hello-minikube created 3. To access the hello-minikube Deployment, expose it as a Service: kubectl expose deployment hello-minikube –type=NodePort –port=8080 The option –type=NodePort specifies the type of the Service. The output is similar to this: service/hello-minikube exposed 4. The hello-minikube Pod is now launched but you have to wait until the Pod is up before accessing it via the exposed Service. Check if the Pod is up and running: kubectl get pod If the output shows the STATUS as ContainerCreating, the Pod is still being created: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 0/1 ContainerCreating 0 3s If the output shows the STATUS as Running, the Pod is now up and running: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 1/1 Running 0 13s 5. Get the URL of the exposed Service to view the Service details: minikube service hello-minikube –url 6. To view the details of your local cluster, copy and paste the URL you got as the output, on your browser. The output is similar to this: Hostname: hello-minikube-7c77b68cff-8wdzq Pod Information: -no pod information available- Server values: server_version=nginx: 1. 13. 3 - lua: 10008 Request Information: client_address=172. 17. 0. 1 method=GET real path=/ query= request_version=1. 1 request_scheme=http request_uri=http://192. 168. 99. 100:8080/ Request Headers: accept=/ host=192. 168. 99. 100:30674 user-agent=curl/7. 47. 0 Request Body: -no body in request- If you no longer want the Service and cluster to run, you can delete them.  Delete the hello-minikube Service:kubectl delete services hello-minikube The output is similar to this: service “hello-minikube” deleted 8. Delete the hello-minikube Deployment: kubectl delete deployment hello-minikube The output is similar to this: deployment. extensions “hello-minikube” deleted 9. Stop the local Minikube cluster: minikube stop The output is similar to this: Stopping “minikube”… “minikube” stopped. For more information, see Stopping a Cluster.  Delete the local Minikube cluster:minikube delete The output is similar to this: Deleting “minikube” … The “minikube” cluster has been deleted. For more information, see Deleting a cluster. Managing your Cluster : Starting a Cluster : The minikube start command can be used to start your cluster. This command creates and configures a Virtual Machine that runs a single-node Kubernetes cluster. This command also configures your kubectl installation to communicate with this cluster.  Note:If you are behind a web proxy, you need to pass this information to the minikube start command:  https_proxy= minikube start --docker-env http_proxy= --docker-env https_proxy= --docker-env no_proxy=192. 168. 99. 0/24 Unfortunately, setting the environment variables alone does not work.  Minikube also creates a “minikube” context, and sets it to default in kubectl. To switch back to this context, run this command: kubectl config use-context minikube.  Specifying the Kubernetes version : You can specify the version of Kubernetes for Minikube to use by adding the –kubernetes-version string to the minikube start command. For example, to run version v1. 18. 0, you would run the following: minikube start –kubernetes-version v1. 18. 0 #### Specifying the VM driver You can change the VM driver by adding the –driver= flag to minikube start. For example the command would be. minikube start –driver= Minikube supports the following drivers:  Note: See DRIVERS for details on supported drivers and how to install plugins. * docker (driver installation)  virtualbox (driver installation) podman (driver installation) (EXPERIMENTAL) vmwarefusion kvm2 (driver installation) hyperkit (driver installation) hyperv (driver installation) Note that the IP below is dynamic and can change. It can be retrieved with minikube ip.  vmware (driver installation) (VMware unified driver) parallels (driver installation) none (Runs the Kubernetes components on the host and not in a virtual machine. You need to be running Linux and to have DockerDocker is a software technology providing operating-system-level virtualization also known as containers. installed. ) Caution: If you use the none driver, some Kubernetes components run as privileged containers that have side effects outside of the Minikube environment. Those side effects mean that the none driver is not recommended for personal workstations. #### Starting a cluster on alternative container runtimes You can start Minikube on the following container runtimes.  containerd CRI-OTo use containerd as the container runtime, run:minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=containerd \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=unix:///run/containerd/containerd. sock \ –extra-config=kubelet. image-service-endpoint=unix:///run/containerd/containerd. sock \ –bootstrapper=kubeadm To use CRI-O as the container runtime, run: minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=cri-o \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=/var/run/crio. sock \ –extra-config=kubelet. image-service-endpoint=/var/run/crio. sock \ –bootstrapper=kubeadm $(function(){$(“#container-runtimes”). tabs();});#### Use local images by re-using the Docker daemon When using a single VM for Kubernetes, it’s useful to reuse Minikube’s built-in Docker daemon. Reusing the built-in daemon means you don’t have to build a Docker registry on your host machine and push the image into it. Instead, you can build inside the same Docker daemon as Minikube, which speeds up local experiments.  Note: Be sure to tag your Docker image with something other than latest and use that tag to pull the image. Because :latest is the default value, with a corresponding default image pull policy of Always, an image pull error (ErrImagePull) eventually results if you do not have the Docker image in the default Docker registry (usually DockerHub). To work with the Docker daemon on your Mac/Linux host, run the last line from minikube docker-env. You can now use Docker at the command line of your host Mac/Linux machine to communicate with the Docker daemon inside the Minikube VM: docker ps  Note:On Centos 7, Docker may report the following error:  Could not read CA certificate “/etc/docker/ca. pem”: open /etc/docker/ca. pem: no such file or directory You can fix this by updating /etc/sysconfig/docker to ensure that Minikube’s environment changes are respected:  &lt; DOCKER_CERT_PATH=/etc/docker — &gt; if [ -z “${DOCKER_CERT_PATH}” ]; then &gt; DOCKER_CERT_PATH=/etc/docker &gt; fi ### Configuring Kubernetes Minikube has a “configurator” feature that allows users to configure the Kubernetes components with arbitrary values. To use this feature, you can use the –extra-config flag on the minikube start command. This flag is repeated, so you can pass it several times with several different values to set multiple options. This flag takes a string of the form component. key=value, where component is one of the strings from the below list, key is a value on the configuration struct and value is the value to set. Valid keys can be found by examining the documentation for the Kubernetes componentconfigs for each component. Here is the documentation for each supported configuration:  kubelet apiserver proxy controller-manager etcd schedulerExamples : To change the MaxPods setting to 5 on the Kubelet, pass this flag: –extra-config=kubelet. MaxPods=5. This feature also supports nested structs. To change the LeaderElection. LeaderElect setting to true on the scheduler, pass this flag: –extra-config=scheduler. LeaderElection. LeaderElect=true. To set the AuthorizationMode on the apiserver to RBAC, you can use: –extra-config=apiserver. authorization-mode=RBAC. Stopping a Cluster : The minikube stop command can be used to stop your cluster. This command shuts down the Minikube Virtual Machine, but preserves all cluster state and data. Starting the cluster again will restore it to its previous state. Deleting a Cluster : The minikube delete command can be used to delete your cluster. This command shuts down and deletes the Minikube Virtual Machine. No data or state is preserved. Upgrading Minikube : If you are using macOS and Brew Package Manager is installed run: brew update brew upgrade minikube Interacting with Your Cluster : Kubectl : The minikube start command creates a kubectl context called “minikube”. This context contains the configuration to communicate with your Minikube cluster. Minikube sets this context to default automatically, but if you need to switch back to it in the future, run: kubectl config use-context minikube Or pass the context on each command like this: kubectl get pods –context=minikube Dashboard : To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address: minikube dashboard ### Services To access a Service exposed via a node port, run this command in a shell after starting Minikube to get the address: minikube service [-n NAMESPACE] [–url] NAME Networking : The Minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command. Any services of type NodePort can be accessed over that IP address, on the NodePort. To determine the NodePort for your service, you can use a kubectl command like this: kubectl get service $SERVICE –output=’jsonpath=”{. spec. ports[0]. nodePort}”’ Persistent Volumes : Minikube supports PersistentVolumes of type hostPath. These PersistentVolumes are mapped to a directory inside the Minikube VM. The Minikube VM boots into a tmpfs, so most directories will not be persisted across reboots (minikube stop). However, Minikube is configured to persist files stored under the following host directories:  /data /var/lib/minikube /var/lib/dockerHere is an example PersistentVolume config to persist data in the /data directory:apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: data/pv0001/ Mounted Host Folders : Some drivers will mount a host folder within the VM so that you can easily share files between the VM and host. These are not configurable at the moment and different for the driver and OS you are using.  Note: Host folder sharing is not implemented in the KVM driver yet. DriverOSHostFolderVMVirtualBoxLinux/home/hosthomeVirtualBoxmacOS/Users/UsersVirtualBoxWindowsC://Users/c/UsersVMware FusionmacOS/Users/mnt/hgfs/UsersXhyvemacOS/Users/UsersPrivate Container Registries To access a private container registry, follow the steps on this page. We recommend you use ImagePullSecrets, but if you would like to configure access on the Minikube VM you can place the . dockercfg in the /home/docker directory or the config. json in the /home/docker/. docker directory. Add-ons : In order to have Minikube properly start or restart custom addons, place the addons you wish to be launched with Minikube in the ~/. minikube/addons directory. Addons in this folder will be moved to the Minikube VM and launched each time Minikube is started or restarted. Using Minikube with an HTTP Proxy : Minikube creates a Virtual Machine that includes Kubernetes and a Docker daemon. When Kubernetes attempts to schedule containers using Docker, the Docker daemon may require external network access to pull containers. If you are behind an HTTP proxy, you may need to supply Docker with the proxy settings. To do this, pass the required environment variables as flags during minikube start. For example: minikube start –docker-env http_proxy=http://$YOURPROXY:PORT \ –docker-env https_proxy=https://$YOURPROXY:PORT If your Virtual Machine address is 192. 168. 99. 100, then chances are your proxy settings will prevent kubectl from directly reaching it. To by-pass proxy configuration for this IP address, you should modify your no_proxy settings. You can do so with: export no_proxy=$no_proxy,$(minikube ip) Known Issues : Features that require multiple nodes will not work in Minikube. Design : Minikube uses libmachine for provisioning VMs, and kubeadm to provision a Kubernetes cluster. For more information about Minikube, see the proposal. Additional Links :  Goals and Non-Goals: For the goals and non-goals of the Minikube project, please see our roadmap.  Development Guide: See Contributing for an overview of how to send pull requests.  Building Minikube: For instructions on how to build/test Minikube from source, see the build guide.  Adding a New Dependency: For instructions on how to add a new dependency to Minikube, see the adding dependencies guide.  Adding a New Addon: For instructions on how to add a new addon for Minikube, see the adding an addon guide.  MicroK8s: Linux users wishing to avoid running a virtual machine may consider MicroK8s as an alternative. Community Contributions, questions, and comments are all welcomed and encouraged! Minikube developers hang out on Slack in the #minikube channel (get an invitation here). We also have the kubernetes-dev Google Groups mailing list. If you are posting to the list please prefix your subject with “minikube: “. Feedback: Was this page helpful? Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub repo if you want to report a problem or suggest an improvement. "
    }, {
    "id": 35,
    "url": "https://bright-softwares.com/blog/en/kubernetes/play-with-kubernetes-with-minikube",
    "title": "Play with Kubernetes with Minikube",
    "body": "2020/08/12 - Installing Kubernetes with MinikubeMinikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Minikube Features : Minikube supports the following Kubernetes features:  DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, CRI-O, and containerd Enabling CNI (Container Network Interface) IngressInstallation See Installing Minikube. Quickstart : This brief demo guides you on how to start, use, and delete Minikube locally. Follow the steps given below to start and explore Minikube.  Start Minikube and create a cluster:minikube start The output is similar to this: Starting local Kubernetes cluster… Running pre-create checks… Creating machine… Starting local Kubernetes cluster… For more information on starting your cluster on a specific Kubernetes version, VM, or container runtime, see Starting a Cluster.  Now, you can interact with your cluster using kubectl. For more information, see Interacting with Your Cluster. Let’s create a Kubernetes Deployment using an existing image named echoserver, which is a simple HTTP server and expose it on port 8080 using –port. kubectl create deployment hello-minikube –image=k8s. gcr. io/echoserver:1. 10 The output is similar to this: deployment. apps/hello-minikube created 3. To access the hello-minikube Deployment, expose it as a Service: kubectl expose deployment hello-minikube –type=NodePort –port=8080 The option –type=NodePort specifies the type of the Service. The output is similar to this: service/hello-minikube exposed 4. The hello-minikube Pod is now launched but you have to wait until the Pod is up before accessing it via the exposed Service. Check if the Pod is up and running: kubectl get pod If the output shows the STATUS as ContainerCreating, the Pod is still being created: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 0/1 ContainerCreating 0 3s If the output shows the STATUS as Running, the Pod is now up and running: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 1/1 Running 0 13s 5. Get the URL of the exposed Service to view the Service details: minikube service hello-minikube –url 6. To view the details of your local cluster, copy and paste the URL you got as the output, on your browser. The output is similar to this: Hostname: hello-minikube-7c77b68cff-8wdzq Pod Information: -no pod information available- Server values: server_version=nginx: 1. 13. 3 - lua: 10008 Request Information: client_address=172. 17. 0. 1 method=GET real path=/ query= request_version=1. 1 request_scheme=http request_uri=http://192. 168. 99. 100:8080/ Request Headers: accept=/ host=192. 168. 99. 100:30674 user-agent=curl/7. 47. 0 Request Body: -no body in request- If you no longer want the Service and cluster to run, you can delete them.  Delete the hello-minikube Service:kubectl delete services hello-minikube The output is similar to this: service “hello-minikube” deleted 8. Delete the hello-minikube Deployment: kubectl delete deployment hello-minikube The output is similar to this: deployment. extensions “hello-minikube” deleted 9. Stop the local Minikube cluster: minikube stop The output is similar to this: Stopping “minikube”… “minikube” stopped. For more information, see Stopping a Cluster.  Delete the local Minikube cluster:minikube delete The output is similar to this: Deleting “minikube” … The “minikube” cluster has been deleted. For more information, see Deleting a cluster. Managing your Cluster : Starting a Cluster : The minikube start command can be used to start your cluster. This command creates and configures a Virtual Machine that runs a single-node Kubernetes cluster. This command also configures your kubectl installation to communicate with this cluster.  Note:If you are behind a web proxy, you need to pass this information to the minikube start command:  https_proxy= minikube start --docker-env http_proxy= --docker-env https_proxy= --docker-env no_proxy=192. 168. 99. 0/24 Unfortunately, setting the environment variables alone does not work.  Minikube also creates a “minikube” context, and sets it to default in kubectl. To switch back to this context, run this command: kubectl config use-context minikube.  Specifying the Kubernetes version : You can specify the version of Kubernetes for Minikube to use by adding the –kubernetes-version string to the minikube start command. For example, to run version v1. 18. 0, you would run the following: minikube start –kubernetes-version v1. 18. 0 #### Specifying the VM driver You can change the VM driver by adding the –driver= flag to minikube start. For example the command would be. minikube start –driver= Minikube supports the following drivers:  Note: See DRIVERS for details on supported drivers and how to install plugins. * docker (driver installation)  virtualbox (driver installation) podman (driver installation) (EXPERIMENTAL) vmwarefusion kvm2 (driver installation) hyperkit (driver installation) hyperv (driver installation) Note that the IP below is dynamic and can change. It can be retrieved with minikube ip.  vmware (driver installation) (VMware unified driver) parallels (driver installation) none (Runs the Kubernetes components on the host and not in a virtual machine. You need to be running Linux and to have DockerDocker is a software technology providing operating-system-level virtualization also known as containers. installed. ) Caution: If you use the none driver, some Kubernetes components run as privileged containers that have side effects outside of the Minikube environment. Those side effects mean that the none driver is not recommended for personal workstations. #### Starting a cluster on alternative container runtimes You can start Minikube on the following container runtimes.  containerd CRI-OTo use containerd as the container runtime, run:minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=containerd \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=unix:///run/containerd/containerd. sock \ –extra-config=kubelet. image-service-endpoint=unix:///run/containerd/containerd. sock \ –bootstrapper=kubeadm To use CRI-O as the container runtime, run: minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=cri-o \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=/var/run/crio. sock \ –extra-config=kubelet. image-service-endpoint=/var/run/crio. sock \ –bootstrapper=kubeadm $(function(){$(“#container-runtimes”). tabs();});#### Use local images by re-using the Docker daemon When using a single VM for Kubernetes, it’s useful to reuse Minikube’s built-in Docker daemon. Reusing the built-in daemon means you don’t have to build a Docker registry on your host machine and push the image into it. Instead, you can build inside the same Docker daemon as Minikube, which speeds up local experiments.  Note: Be sure to tag your Docker image with something other than latest and use that tag to pull the image. Because :latest is the default value, with a corresponding default image pull policy of Always, an image pull error (ErrImagePull) eventually results if you do not have the Docker image in the default Docker registry (usually DockerHub). To work with the Docker daemon on your Mac/Linux host, run the last line from minikube docker-env. You can now use Docker at the command line of your host Mac/Linux machine to communicate with the Docker daemon inside the Minikube VM: docker ps  Note:On Centos 7, Docker may report the following error:  Could not read CA certificate “/etc/docker/ca. pem”: open /etc/docker/ca. pem: no such file or directory You can fix this by updating /etc/sysconfig/docker to ensure that Minikube’s environment changes are respected:  &lt; DOCKER_CERT_PATH=/etc/docker — &gt; if [ -z “${DOCKER_CERT_PATH}” ]; then &gt; DOCKER_CERT_PATH=/etc/docker &gt; fi ### Configuring Kubernetes Minikube has a “configurator” feature that allows users to configure the Kubernetes components with arbitrary values. To use this feature, you can use the –extra-config flag on the minikube start command. This flag is repeated, so you can pass it several times with several different values to set multiple options. This flag takes a string of the form component. key=value, where component is one of the strings from the below list, key is a value on the configuration struct and value is the value to set. Valid keys can be found by examining the documentation for the Kubernetes componentconfigs for each component. Here is the documentation for each supported configuration:  kubelet apiserver proxy controller-manager etcd schedulerExamples : To change the MaxPods setting to 5 on the Kubelet, pass this flag: –extra-config=kubelet. MaxPods=5. This feature also supports nested structs. To change the LeaderElection. LeaderElect setting to true on the scheduler, pass this flag: –extra-config=scheduler. LeaderElection. LeaderElect=true. To set the AuthorizationMode on the apiserver to RBAC, you can use: –extra-config=apiserver. authorization-mode=RBAC. Stopping a Cluster : The minikube stop command can be used to stop your cluster. This command shuts down the Minikube Virtual Machine, but preserves all cluster state and data. Starting the cluster again will restore it to its previous state. Deleting a Cluster : The minikube delete command can be used to delete your cluster. This command shuts down and deletes the Minikube Virtual Machine. No data or state is preserved. Upgrading Minikube : If you are using macOS and Brew Package Manager is installed run: brew update brew upgrade minikube Interacting with Your Cluster : Kubectl : The minikube start command creates a kubectl context called “minikube”. This context contains the configuration to communicate with your Minikube cluster. Minikube sets this context to default automatically, but if you need to switch back to it in the future, run: kubectl config use-context minikube Or pass the context on each command like this: kubectl get pods –context=minikube Dashboard : To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address: minikube dashboard ### Services To access a Service exposed via a node port, run this command in a shell after starting Minikube to get the address: minikube service [-n NAMESPACE] [–url] NAME Networking : The Minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command. Any services of type NodePort can be accessed over that IP address, on the NodePort. To determine the NodePort for your service, you can use a kubectl command like this: kubectl get service $SERVICE –output=’jsonpath=”{. spec. ports[0]. nodePort}”’ Persistent Volumes : Minikube supports PersistentVolumes of type hostPath. These PersistentVolumes are mapped to a directory inside the Minikube VM. The Minikube VM boots into a tmpfs, so most directories will not be persisted across reboots (minikube stop). However, Minikube is configured to persist files stored under the following host directories:  /data /var/lib/minikube /var/lib/dockerHere is an example PersistentVolume config to persist data in the /data directory:apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: data/pv0001/ Mounted Host Folders : Some drivers will mount a host folder within the VM so that you can easily share files between the VM and host. These are not configurable at the moment and different for the driver and OS you are using.  Note: Host folder sharing is not implemented in the KVM driver yet. DriverOSHostFolderVMVirtualBoxLinux/home/hosthomeVirtualBoxmacOS/Users/UsersVirtualBoxWindowsC://Users/c/UsersVMware FusionmacOS/Users/mnt/hgfs/UsersXhyvemacOS/Users/UsersPrivate Container Registries To access a private container registry, follow the steps on this page. We recommend you use ImagePullSecrets, but if you would like to configure access on the Minikube VM you can place the . dockercfg in the /home/docker directory or the config. json in the /home/docker/. docker directory. Add-ons : In order to have Minikube properly start or restart custom addons, place the addons you wish to be launched with Minikube in the ~/. minikube/addons directory. Addons in this folder will be moved to the Minikube VM and launched each time Minikube is started or restarted. Using Minikube with an HTTP Proxy : Minikube creates a Virtual Machine that includes Kubernetes and a Docker daemon. When Kubernetes attempts to schedule containers using Docker, the Docker daemon may require external network access to pull containers. If you are behind an HTTP proxy, you may need to supply Docker with the proxy settings. To do this, pass the required environment variables as flags during minikube start. For example: minikube start –docker-env http_proxy=http://$YOURPROXY:PORT \ –docker-env https_proxy=https://$YOURPROXY:PORT If your Virtual Machine address is 192. 168. 99. 100, then chances are your proxy settings will prevent kubectl from directly reaching it. To by-pass proxy configuration for this IP address, you should modify your no_proxy settings. You can do so with: export no_proxy=$no_proxy,$(minikube ip) Known Issues : Features that require multiple nodes will not work in Minikube. Design : Minikube uses libmachine for provisioning VMs, and kubeadm to provision a Kubernetes cluster. For more information about Minikube, see the proposal. Additional Links :  Goals and Non-Goals: For the goals and non-goals of the Minikube project, please see our roadmap.  Development Guide: See Contributing for an overview of how to send pull requests.  Building Minikube: For instructions on how to build/test Minikube from source, see the build guide.  Adding a New Dependency: For instructions on how to add a new dependency to Minikube, see the adding dependencies guide.  Adding a New Addon: For instructions on how to add a new addon for Minikube, see the adding an addon guide.  MicroK8s: Linux users wishing to avoid running a virtual machine may consider MicroK8s as an alternative. Community Contributions, questions, and comments are all welcomed and encouraged! Minikube developers hang out on Slack in the #minikube channel (get an invitation here). We also have the kubernetes-dev Google Groups mailing list. If you are posting to the list please prefix your subject with “minikube: “. Feedback: Was this page helpful? Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub repo if you want to report a problem or suggest an improvement. "
    }, {
    "id": 36,
    "url": "https://bright-softwares.com/blog/en/kubernetes/installing-kubernetes-with-minikube",
    "title": "Installing Kubernetes with Minikube",
    "body": "2020/08/12 - Installing Kubernetes with MinikubeMinikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Minikube Features : Minikube supports the following Kubernetes features:  DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, CRI-O, and containerd Enabling CNI (Container Network Interface) IngressInstallation See Installing Minikube. Quickstart : This brief demo guides you on how to start, use, and delete Minikube locally. Follow the steps given below to start and explore Minikube.  Start Minikube and create a cluster:minikube start The output is similar to this: Starting local Kubernetes cluster… Running pre-create checks… Creating machine… Starting local Kubernetes cluster… For more information on starting your cluster on a specific Kubernetes version, VM, or container runtime, see Starting a Cluster.  Now, you can interact with your cluster using kubectl. For more information, see Interacting with Your Cluster. Let’s create a Kubernetes Deployment using an existing image named echoserver, which is a simple HTTP server and expose it on port 8080 using –port. kubectl create deployment hello-minikube –image=k8s. gcr. io/echoserver:1. 10 The output is similar to this: deployment. apps/hello-minikube created 3. To access the hello-minikube Deployment, expose it as a Service: kubectl expose deployment hello-minikube –type=NodePort –port=8080 The option –type=NodePort specifies the type of the Service. The output is similar to this: service/hello-minikube exposed 4. The hello-minikube Pod is now launched but you have to wait until the Pod is up before accessing it via the exposed Service. Check if the Pod is up and running: kubectl get pod If the output shows the STATUS as ContainerCreating, the Pod is still being created: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 0/1 ContainerCreating 0 3s If the output shows the STATUS as Running, the Pod is now up and running: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 1/1 Running 0 13s 5. Get the URL of the exposed Service to view the Service details: minikube service hello-minikube –url 6. To view the details of your local cluster, copy and paste the URL you got as the output, on your browser. The output is similar to this: Hostname: hello-minikube-7c77b68cff-8wdzq Pod Information: -no pod information available- Server values: server_version=nginx: 1. 13. 3 - lua: 10008 Request Information: client_address=172. 17. 0. 1 method=GET real path=/ query= request_version=1. 1 request_scheme=http request_uri=http://192. 168. 99. 100:8080/ Request Headers: accept=/ host=192. 168. 99. 100:30674 user-agent=curl/7. 47. 0 Request Body: -no body in request- If you no longer want the Service and cluster to run, you can delete them.  Delete the hello-minikube Service:kubectl delete services hello-minikube The output is similar to this: service “hello-minikube” deleted 8. Delete the hello-minikube Deployment: kubectl delete deployment hello-minikube The output is similar to this: deployment. extensions “hello-minikube” deleted 9. Stop the local Minikube cluster: minikube stop The output is similar to this: Stopping “minikube”… “minikube” stopped. For more information, see Stopping a Cluster.  Delete the local Minikube cluster:minikube delete The output is similar to this: Deleting “minikube” … The “minikube” cluster has been deleted. For more information, see Deleting a cluster. Managing your Cluster : Starting a Cluster : The minikube start command can be used to start your cluster. This command creates and configures a Virtual Machine that runs a single-node Kubernetes cluster. This command also configures your kubectl installation to communicate with this cluster.  Note:If you are behind a web proxy, you need to pass this information to the minikube start command:  https_proxy= minikube start --docker-env http_proxy= --docker-env https_proxy= --docker-env no_proxy=192. 168. 99. 0/24 Unfortunately, setting the environment variables alone does not work.  Minikube also creates a “minikube” context, and sets it to default in kubectl. To switch back to this context, run this command: kubectl config use-context minikube.  Specifying the Kubernetes version : You can specify the version of Kubernetes for Minikube to use by adding the –kubernetes-version string to the minikube start command. For example, to run version v1. 18. 0, you would run the following: minikube start –kubernetes-version v1. 18. 0 #### Specifying the VM driver You can change the VM driver by adding the –driver= flag to minikube start. For example the command would be. minikube start –driver= Minikube supports the following drivers:  Note: See DRIVERS for details on supported drivers and how to install plugins. * docker (driver installation)  virtualbox (driver installation) podman (driver installation) (EXPERIMENTAL) vmwarefusion kvm2 (driver installation) hyperkit (driver installation) hyperv (driver installation) Note that the IP below is dynamic and can change. It can be retrieved with minikube ip.  vmware (driver installation) (VMware unified driver) parallels (driver installation) none (Runs the Kubernetes components on the host and not in a virtual machine. You need to be running Linux and to have DockerDocker is a software technology providing operating-system-level virtualization also known as containers. installed. ) Caution: If you use the none driver, some Kubernetes components run as privileged containers that have side effects outside of the Minikube environment. Those side effects mean that the none driver is not recommended for personal workstations. #### Starting a cluster on alternative container runtimes You can start Minikube on the following container runtimes.  containerd CRI-OTo use containerd as the container runtime, run:minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=containerd \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=unix:///run/containerd/containerd. sock \ –extra-config=kubelet. image-service-endpoint=unix:///run/containerd/containerd. sock \ –bootstrapper=kubeadm To use CRI-O as the container runtime, run: minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=cri-o \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=/var/run/crio. sock \ –extra-config=kubelet. image-service-endpoint=/var/run/crio. sock \ –bootstrapper=kubeadm $(function(){$(“#container-runtimes”). tabs();});#### Use local images by re-using the Docker daemon When using a single VM for Kubernetes, it’s useful to reuse Minikube’s built-in Docker daemon. Reusing the built-in daemon means you don’t have to build a Docker registry on your host machine and push the image into it. Instead, you can build inside the same Docker daemon as Minikube, which speeds up local experiments.  Note: Be sure to tag your Docker image with something other than latest and use that tag to pull the image. Because :latest is the default value, with a corresponding default image pull policy of Always, an image pull error (ErrImagePull) eventually results if you do not have the Docker image in the default Docker registry (usually DockerHub). To work with the Docker daemon on your Mac/Linux host, run the last line from minikube docker-env. You can now use Docker at the command line of your host Mac/Linux machine to communicate with the Docker daemon inside the Minikube VM: docker ps  Note:On Centos 7, Docker may report the following error:  Could not read CA certificate “/etc/docker/ca. pem”: open /etc/docker/ca. pem: no such file or directory You can fix this by updating /etc/sysconfig/docker to ensure that Minikube’s environment changes are respected:  &lt; DOCKER_CERT_PATH=/etc/docker — &gt; if [ -z “${DOCKER_CERT_PATH}” ]; then &gt; DOCKER_CERT_PATH=/etc/docker &gt; fi ### Configuring Kubernetes Minikube has a “configurator” feature that allows users to configure the Kubernetes components with arbitrary values. To use this feature, you can use the –extra-config flag on the minikube start command. This flag is repeated, so you can pass it several times with several different values to set multiple options. This flag takes a string of the form component. key=value, where component is one of the strings from the below list, key is a value on the configuration struct and value is the value to set. Valid keys can be found by examining the documentation for the Kubernetes componentconfigs for each component. Here is the documentation for each supported configuration:  kubelet apiserver proxy controller-manager etcd schedulerExamples : To change the MaxPods setting to 5 on the Kubelet, pass this flag: –extra-config=kubelet. MaxPods=5. This feature also supports nested structs. To change the LeaderElection. LeaderElect setting to true on the scheduler, pass this flag: –extra-config=scheduler. LeaderElection. LeaderElect=true. To set the AuthorizationMode on the apiserver to RBAC, you can use: –extra-config=apiserver. authorization-mode=RBAC. Stopping a Cluster : The minikube stop command can be used to stop your cluster. This command shuts down the Minikube Virtual Machine, but preserves all cluster state and data. Starting the cluster again will restore it to its previous state. Deleting a Cluster : The minikube delete command can be used to delete your cluster. This command shuts down and deletes the Minikube Virtual Machine. No data or state is preserved. Upgrading Minikube : If you are using macOS and Brew Package Manager is installed run: brew update brew upgrade minikube Interacting with Your Cluster : Kubectl : The minikube start command creates a kubectl context called “minikube”. This context contains the configuration to communicate with your Minikube cluster. Minikube sets this context to default automatically, but if you need to switch back to it in the future, run: kubectl config use-context minikube Or pass the context on each command like this: kubectl get pods –context=minikube Dashboard : To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address: minikube dashboard ### Services To access a Service exposed via a node port, run this command in a shell after starting Minikube to get the address: minikube service [-n NAMESPACE] [–url] NAME Networking : The Minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command. Any services of type NodePort can be accessed over that IP address, on the NodePort. To determine the NodePort for your service, you can use a kubectl command like this: kubectl get service $SERVICE –output=’jsonpath=”{. spec. ports[0]. nodePort}”’ Persistent Volumes : Minikube supports PersistentVolumes of type hostPath. These PersistentVolumes are mapped to a directory inside the Minikube VM. The Minikube VM boots into a tmpfs, so most directories will not be persisted across reboots (minikube stop). However, Minikube is configured to persist files stored under the following host directories:  /data /var/lib/minikube /var/lib/dockerHere is an example PersistentVolume config to persist data in the /data directory:apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: data/pv0001/ Mounted Host Folders : Some drivers will mount a host folder within the VM so that you can easily share files between the VM and host. These are not configurable at the moment and different for the driver and OS you are using.  Note: Host folder sharing is not implemented in the KVM driver yet. DriverOSHostFolderVMVirtualBoxLinux/home/hosthomeVirtualBoxmacOS/Users/UsersVirtualBoxWindowsC://Users/c/UsersVMware FusionmacOS/Users/mnt/hgfs/UsersXhyvemacOS/Users/UsersPrivate Container Registries To access a private container registry, follow the steps on this page. We recommend you use ImagePullSecrets, but if you would like to configure access on the Minikube VM you can place the . dockercfg in the /home/docker directory or the config. json in the /home/docker/. docker directory. Add-ons : In order to have Minikube properly start or restart custom addons, place the addons you wish to be launched with Minikube in the ~/. minikube/addons directory. Addons in this folder will be moved to the Minikube VM and launched each time Minikube is started or restarted. Using Minikube with an HTTP Proxy : Minikube creates a Virtual Machine that includes Kubernetes and a Docker daemon. When Kubernetes attempts to schedule containers using Docker, the Docker daemon may require external network access to pull containers. If you are behind an HTTP proxy, you may need to supply Docker with the proxy settings. To do this, pass the required environment variables as flags during minikube start. For example: minikube start –docker-env http_proxy=http://$YOURPROXY:PORT \ –docker-env https_proxy=https://$YOURPROXY:PORT If your Virtual Machine address is 192. 168. 99. 100, then chances are your proxy settings will prevent kubectl from directly reaching it. To by-pass proxy configuration for this IP address, you should modify your no_proxy settings. You can do so with: export no_proxy=$no_proxy,$(minikube ip) Known Issues : Features that require multiple nodes will not work in Minikube. Design : Minikube uses libmachine for provisioning VMs, and kubeadm to provision a Kubernetes cluster. For more information about Minikube, see the proposal. Additional Links :  Goals and Non-Goals: For the goals and non-goals of the Minikube project, please see our roadmap.  Development Guide: See Contributing for an overview of how to send pull requests.  Building Minikube: For instructions on how to build/test Minikube from source, see the build guide.  Adding a New Dependency: For instructions on how to add a new dependency to Minikube, see the adding dependencies guide.  Adding a New Addon: For instructions on how to add a new addon for Minikube, see the adding an addon guide.  MicroK8s: Linux users wishing to avoid running a virtual machine may consider MicroK8s as an alternative. Community Contributions, questions, and comments are all welcomed and encouraged! Minikube developers hang out on Slack in the #minikube channel (get an invitation here). We also have the kubernetes-dev Google Groups mailing list. If you are posting to the list please prefix your subject with “minikube: “. Feedback: Was this page helpful? Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub repo if you want to report a problem or suggest an improvement. "
    }, {
    "id": 37,
    "url": "https://bright-softwares.com/blog/fr/backup-restore/how-do-i-get-my-data-off-an-old-computer",
    "title": "How Do I Get My Data Off an Old Computer?",
    "body": "2020/08/11 - I want to delete everything on my old computer, but…:  “Hello,  I have and older gateway tower but sold the monitor that came with it.  _I have a laptop and a TV and tried using an HDMI to VGA adapter thinking it would show on my TV or laptop but its not working. I would like to go through my old computer and wipe it clean before just recycling it. _  Any suggestions on how I can view the content on my old computer?” Why you can’t get a signal from your old computer: Good news: You have a few solutions to address this. To start, let’s talk about what isn’t going to work. Connecting your desktop to a laptop isn’t going to do anything, as your laptop’s connections are outputs. No matter what connection you use, hooking up your desktop PC to your laptop’s anything isn’t going to display your desktop’s picture on your laptop’s display. It’s a great idea in theory, but one that isn’t going to work, technologically. I’ve never tried using a VGA-to-HDMI adapter—in fact this is the first I’ve ever heard of someone actually using one, given how old that connection type is. My suspicion is that this technique won’t work, or has a high likelihood of failure, because VGA is an analog signal and HDMI is digital. It’s possible to go between the two, but whatever adapter you’re using is going to have to take that VGA signal and convert it. If you just have a simple cable that has a VGA connection on one end and an HDMI connection on the other, that’s probably not going to work. It’s also possible that whatever adapter you’ve picked up sucks, for lack of a better way to phrase it. If you went bargain-bin on your adapter, or bought one that hasn’t already been tested out by plenty of other people, it might not be able to do what you want it to do because that’s how these things sometimes go. An adapter can help, but there’s a better solution: This is a tricky situation, as what I’d typically recommend—a USB-to-DVI adapter, which you’d then connect to a DVI-to-HDMI cable, or a simpler USB-to-HDMI adapter—is probably going to cost more than what you’re willing to spend on a solution. At least, I’d probably ask around and see if any of my friends have an older monitor with a VGA input before I plunked down $30-50 on an adapter. One thing worth checking is whether your old desktop has an S-Video port. It’s unlikely, but if it does, and your TV has one, you can connect them together that way. It’s a longshot, but it’s worth exploring. Otherwise, I think your best option is to hit up Craigslist for your area and buy someone’s cheap older display. Make sure they can confirm it works with a VGA signal before you pay for it, but this will be the best and surest way to get an image from your system. And, in doing so, you can then start poring over your data and wiping your system. Looking at Craigslist right now for my area, people are selling their old, crappy, VGA monitors for around $15-30. You can probably find something as cheap as that where you live; remember, all that matters is that it works, not that it looks pretty, has a giant screen, or anything like that. Hook it up to your computer, save or delete whatever you want, and recycle that along with your old PC. To get at your old data, try removing your hard drive: Messing with displays and monitors is one approach. Since all you care about is the data on your drive, you could also open up your desktop PC, disconnect and remove its hard drive, and slap that hard drive in an external enclosure. I’d recommend one, but I’m not sure if your drive uses an ancient IDE connection or SATA. (You can find enclosures and docking stations that handle both, so it’s almost a moot point. ) Install your old desktop’s hard drive into one of those, connect the enclosure to a new desktop or laptop via USB, and you should be able to access all of your files. Save whatever you want, wipe the drive, and send it—and your old system—off for recycling when you’re done. While this sounds like a process, removing a hard drive from your desktop PC is as easy as disconnecting a few cables and unscrewing four screws. Take care not to jostle the drive too much as you’re taking it out. Don’t turn it upside-down or anything crazy like that, and make sure you’ve touched the site of your (metal) case to discharge of any static electricity before you go fiddling with your system’s insides. Were I you, I’d go that route because it’s easy and it saves you money. Instead of spending $20-30 on a display you’ll never use again, you can pick up a docking station that you* *could definitely use at some future point, whether you’re pulling files off another drive, copying files over to a bare backup drive, et cetera. It’s infinitely more practical than a crappy old display, but it requires slightly more legwork for your situation. You can do it. I have faith. "
    }, {
    "id": 38,
    "url": "https://bright-softwares.com/blog/fr/productivity/how-do-i-fix-my-cracked-iphone-screen-during-the-shutdown",
    "title": "How Do I Fix My Cracked iPhone Screen During the Shutdown?",
    "body": "2020/08/11 - There’s a lot you can’t do in a pandemic—at least, if you’re smart. You shouldn’t be going to the mall (even if it’s open), nor should you even consider just lollygagging around the Apple store and taking stupid pictures on their demo devices. However, the quarantine—self-imposed or otherwise—also comes with some frustrations. If your iPhone breaks, for example, you can’t just waltz into an Apple store to get it repaired at the ol’ bar of smart people. You’re kind of stuck. Or are you? If I’m quarantined, how can I get my phone repaired?: In this week’s, we’re taking a question from Lifehacker reader Stephanie. She writes:  _My iPhone screen was already very, very cracked before The Pandemic hit the U. S. , and it’s only gotten worse since. (As in there’s no way I would pull it out of my pocket when it’s even beginning to drizzle. ) _  _I know it’s absurd on the scale of problems right now, but how can I get my screen fixed before it’s too late? FYI I live in NYC, and the few services I’ve heard of are not currently accepting appointments. Also, I am currently on reduced pay, and resigned to the possibility that I could very well have no income any day now, so spending hundreds of dollars for a new phone etc. is absolutely out of the question. _  Thanks and please stay safe! You have options, but you might need extra patience: It’s not absurd to want to fix your device. It’s never absurd. Just because we’re in a quarantine, and you’re stuck in one of the biggest hotspots for COVID-19, doesn’t mean that you shouldn’t be able to have working technology. That’s especially true now, since your phone is even more of a lifeline to the world than before. Aside from the obvious calling, texting, and Slacking you’re probably doing, it’s what you’ll use to tether your other devices if, or when, your internet goes out. It’s how you’ll record all the silly videos that are keeping you sane during these trying times—or the images of the barren New York City streets that you’ll look back on years from now with fondness, once the sweaty hustle and bustle resumes in a (probably) post-vaccinated world. You’ll think, “The coronavirus is terrible, but it sure was nice not to have tourists standing in front of me wherever I go. ” I kid, but I do want to stress that wanting to lead a normal life—including normal and functional use of the gadgets you rely on—is a perfectly fine, unselfish thought right now. Unfortunately, you’re correct; the “new normal” we’re living in, especially in New York City, means that you aren’t going to have super-easy options for getting your phone fixed. That includes those shady-but-sometimes-reliable “fix your phone!” kiosks littered around the city, as well as the mighty Apple store itself. Try looking for an Apple-authorized repair provider that’s still open: I did a quick search for authorized service providers on Apple’s support site, and the results aren’t pretty for New York City. In fact, here’s all I could find: For those reading this not in NYC, your odds might be a lot better—since you likely don’t live in a coronavirus epicenter, and you might even live in one of those places where everyone’s partying on the beach and celebrating and such. So, who knows, maybe you’ll have 20 repair centers to pick from. Either way, I’d start here: Search Apple’s support site for authorized repair centers near you and see if any are available to repair your device. Otherwise, you’re probably going to have to send in your iPhone to have it repaired. That’ll cost you time, but at least it’ll get it fixed. I don’t know your specific warranty situation—and Apple will tell you, don’t worry—but you’re looking at potentially blowing up to $329 for an out-of-warranty repair. The number goes down the older and smaller your device is, so you might luck out with a mere $129 out the door if you’re repairing an ancient iPhone SE. You can check Apple’s prices here. You could go for a third-party repair, but…: Places like UBreakIFix will be more than happy to work out a repair arrangement for your device, which could even include curbside pickup. You’ll probably even be able to pay slightly less for your repair than you would at Apple, but you might also have a less-than-stellar experience, depending on *how *a third-party service fixes the device. Ah, the joys of Apple’s fight against right-to-repair. You didn’t mention which iPhone you have, but you might want to reconsider your “buy a new one” approach. I’d modify that sentence, actually, to simply read, “buy one. ” You don’t have to plunk down $1,000+ for a brand-new iPhone. If you’re still rocking, say, an iPhone 8, you can pick up a functional (but possibly not pretty) one for around $200 from a third-party seller. Heck, you could even get an I-just-need-something-to-survive iPhone SE for $50—the older iPhone, that is, not the newer one. You don’t have to buy an *expensive *iPhone replacement: Given the times we’re living in, and the fact that repairing your iPhone might be a pain in the ass (and expensive, as you noted), paying one-fourth the price of a repair for just something you can use to get by is an option. It’ll tide you over until your work hours go back to normal and you can repair your original device (or buy something better, potentially). Let’s recap: I don’t love any of these options, but there they are.  Send it in to Apple. Expensive, but authentic parts. You’ll have to wait, but you’ll get your iPhone fixed up the best possible way.  Find an authorized service provider: This will be tough in NYC during a quarantine, but if you can find an Apple provider that’s available, you might save a little time on your repair. Money? No. Time? Maybe.  Find a third-party repair shop. Depending on what’s open, you might save cash and time, but this can be a gamble. Replacing the screen at an unauthorized repair shop will void your Apple warranty, if applicable, and possibly lead to a worse-looking display depending on what parts the repair shop is using. You also might be fine. It’s a gamble—less so for older iPhones, and more so for newer ones.    Buying a used but functional replacement. Whether you’re replacing your broken phone with the exact same model or** **going back a generation (or two) to simply get something that’s cheap and functional, delaying the repair is certainly an option worth considering.   Phone a friend. Try asking around. The odds are decent that someone has a spare iPhone (likely an older one) sitting around in a drawer somewhere that they aren’t using. See if you can borrow it until you’re in a better spot, job-wise, then consider your repair options from there. Swapping out your SIM to another iPhone is easy. "
    }, {
    "id": 39,
    "url": "https://bright-softwares.com/blog/fr/productivity/help-windows-wont-boot-correctly-after-a-recent-os-update",
    "title": "Help! Windows Won't Boot Correctly After a Recent OS Update",
    "body": "2020/08/11 - What happens when Windows starts tossing up annoying error messages each and every time you try to launch the operating system? If you’re trying to log in for the day and actually do work for the critical deadlines you have, and you don’t really have an IT department to help out, this is probably the worst spot to be in. Since we’re all stuck—or will soon be stuck—in our homes and apartments for the foreseeable future, The question::  I have a Windows 10 desktop. Several weeks ago windows did an update and several days later when I powered on, I got the message “windows didn’t load properly” blue screen with several options. The ones I tried didn’t work well, until I found the “Revert to a previous date” option. That worked well. Computer went back on and functioned normally. Unfortunately I still receive the windows did not load properly screen every several days. Im running Windows 10 on a Mac. I was thinking of completely wiping out windows and then reloading it. Maybe I have a virus or bug ? Thanks !! The humble answer:: For what it’s worth, you’re not alone on this one. Microsoft has been having issues with Windows 10 updates lately, which can sometimes introduce more problems for users than they can fix. It’s possible that you’re in this camp, but the good news is that it’s very, very unlikely that you’ve been hit with a virus or some piece of sketchy malware. It’s just a Windows issue—not very soothing to hear when you’re experiencing it, I’m sure, but at least slight more comforting than, “Your system is infected” (I hope). Generally speaking, I like to abandon ship at the first sign of trouble that would likely take me longer to troubleshoot—with mixed results—than it would take me to reinstall Windows and all of my applications. I suspect that might be the case here. And since you’re Boot Camping into Windows on your Mac, I’m less bothered by putting you out of commission for a bit, since you’ll always have macOS to use if you absolutely need to do something on your computer. Before we go nuclear, though, let’s try a few things. First off, if you can boot into Windows—and it sounds like you can—I’m not sure there’s anything you can uninstall that will help you. The last major Windows 10 update that would have probably affected you was the big 1909 update from November. There have been a number of piecemeal updates since then, but I can’t think of one that’s been especially problematic. Oh, except for KB4535996, which even Microsoft suggested users uninstall. So, let’s start there. Pull up Windows Update, click on View Update History, click on Uninstall Updates, and look to see if you can uninstall KB4535996. If you can, great! If not, there goes that troubleshooting technique. While you’re here, maybe check to see if there are any additional Windows updates you can install. It’s a long shot, but perhaps something has arrived that could fix whatever issues your Windows installation is struggling to deal with. And since you’re using Boot Camp to run Windows on your Mac, pull up Apple Software Update and make sure there aren’t any new drivers or updates to install. How Can I Keep an Old All-In-One PC Running Quickly and Efficiently?I’m always thrilled to get “help me out with a tech problem” letters in my inbox, and I’ve had… Finally, try reinstalling Boot Camp’s Windows Support Drivers, which might magically cure whatever is causing your system to blue screen upon launch. There’s no guarantee this will fix things, but it’s worth exploring before you take more drastic measures. While you’re in Windows, you can also open up an elevated command prompt (search for “Command Prompt” in the Start Menu, right click on it, and select “Run as administrator”). From there, try running a simple “chkdsk /f” to conform there aren’t any issues with your file system. You can also try “chkdsk /r /f” for a much more thorough analysis and fixing process, but it’ll take a lot longer. If your hard drive is failing and that’s the reason behind your Windows issues, it’s also possible you might not get any additional information from chkdsk. You’ll want to use some other techniques to confirm you’re ok (or headed toward disaster). You can also run “sfc /verifyonly” followed by “sfc /scannow” in the same elevated command prompt. If the first command found any corruption in your Windows system files, the second command should fix them. Once you’ve finished this, consider pulling up the Windows Troubleshooter. Pull up the old-school Control Panel (via the Start Menu) and select Troubleshooting. Then, click on “Fix problems with Windows Update,” and see what the utility finds (if anything!) Finally, click on your Start Menu, click on the Power icon, hold down the Shift key on your keyboard, and click on Restart. This should boot you into Windows 10’s Advanced Startup options menu. Click on Troubleshoot, click on Advanced options, and try using the Startup Repair option to see if that can solve your Windows problem. How Can You Keep Your Old Desktop PC Running Well?When your desktop PC is aging, but you don’t have the heart (or the budget) to replace it, there’s… If all else fails, then a wipe and restore might be your best option. Save all your critical Windows 10 files to a flash drive or cloud storage, then launch macOS and use Boot Camp Assistant to remove your Windows OS. Use Microsoft’s Media Creation Tool to download a new, fresh . ISO of Windows 10, and then use Boot Camp to reinstall that on your Mac. Once Windows is up and running, make sure you’ve installed any updates from Apple (the aforementioned Windows Support Drivers and Apple Software Update) first, then install all the WIndows Updates Microsoft offers, then start putting your files and apps back on your system once you’ve verified that everything feels right. Don’t worry; it takes a lot less time than it sounds like. "
    }, {
    "id": 40,
    "url": "https://bright-softwares.com/blog/en/productivity/how-do-i-fix-my-cracked-iphone-screen-during-the-shutdown",
    "title": "How Do I Fix My Cracked iPhone Screen During the Shutdown?",
    "body": "2020/08/11 - There’s a lot you can’t do in a pandemic—at least, if you’re smart. You shouldn’t be going to the mall (even if it’s open), nor should you even consider just lollygagging around the Apple store and taking stupid pictures on their demo devices. However, the quarantine—self-imposed or otherwise—also comes with some frustrations. If your iPhone breaks, for example, you can’t just waltz into an Apple store to get it repaired at the ol’ bar of smart people. You’re kind of stuck. Or are you? If I’m quarantined, how can I get my phone repaired?: In this week’s, we’re taking a question from Lifehacker reader Stephanie. She writes:  _My iPhone screen was already very, very cracked before The Pandemic hit the U. S. , and it’s only gotten worse since. (As in there’s no way I would pull it out of my pocket when it’s even beginning to drizzle. ) _  _I know it’s absurd on the scale of problems right now, but how can I get my screen fixed before it’s too late? FYI I live in NYC, and the few services I’ve heard of are not currently accepting appointments. Also, I am currently on reduced pay, and resigned to the possibility that I could very well have no income any day now, so spending hundreds of dollars for a new phone etc. is absolutely out of the question. _  Thanks and please stay safe! You have options, but you might need extra patience: It’s not absurd to want to fix your device. It’s never absurd. Just because we’re in a quarantine, and you’re stuck in one of the biggest hotspots for COVID-19, doesn’t mean that you shouldn’t be able to have working technology. That’s especially true now, since your phone is even more of a lifeline to the world than before. Aside from the obvious calling, texting, and Slacking you’re probably doing, it’s what you’ll use to tether your other devices if, or when, your internet goes out. It’s how you’ll record all the silly videos that are keeping you sane during these trying times—or the images of the barren New York City streets that you’ll look back on years from now with fondness, once the sweaty hustle and bustle resumes in a (probably) post-vaccinated world. You’ll think, “The coronavirus is terrible, but it sure was nice not to have tourists standing in front of me wherever I go. ” I kid, but I do want to stress that wanting to lead a normal life—including normal and functional use of the gadgets you rely on—is a perfectly fine, unselfish thought right now. Unfortunately, you’re correct; the “new normal” we’re living in, especially in New York City, means that you aren’t going to have super-easy options for getting your phone fixed. That includes those shady-but-sometimes-reliable “fix your phone!” kiosks littered around the city, as well as the mighty Apple store itself. Try looking for an Apple-authorized repair provider that’s still open: I did a quick search for authorized service providers on Apple’s support site, and the results aren’t pretty for New York City. In fact, here’s all I could find: For those reading this not in NYC, your odds might be a lot better—since you likely don’t live in a coronavirus epicenter, and you might even live in one of those places where everyone’s partying on the beach and celebrating and such. So, who knows, maybe you’ll have 20 repair centers to pick from. Either way, I’d start here: Search Apple’s support site for authorized repair centers near you and see if any are available to repair your device. Otherwise, you’re probably going to have to send in your iPhone to have it repaired. That’ll cost you time, but at least it’ll get it fixed. I don’t know your specific warranty situation—and Apple will tell you, don’t worry—but you’re looking at potentially blowing up to $329 for an out-of-warranty repair. The number goes down the older and smaller your device is, so you might luck out with a mere $129 out the door if you’re repairing an ancient iPhone SE. You can check Apple’s prices here. You could go for a third-party repair, but…: Places like UBreakIFix will be more than happy to work out a repair arrangement for your device, which could even include curbside pickup. You’ll probably even be able to pay slightly less for your repair than you would at Apple, but you might also have a less-than-stellar experience, depending on *how *a third-party service fixes the device. Ah, the joys of Apple’s fight against right-to-repair. You didn’t mention which iPhone you have, but you might want to reconsider your “buy a new one” approach. I’d modify that sentence, actually, to simply read, “buy one. ” You don’t have to plunk down $1,000+ for a brand-new iPhone. If you’re still rocking, say, an iPhone 8, you can pick up a functional (but possibly not pretty) one for around $200 from a third-party seller. Heck, you could even get an I-just-need-something-to-survive iPhone SE for $50—the older iPhone, that is, not the newer one. You don’t have to buy an *expensive *iPhone replacement: Given the times we’re living in, and the fact that repairing your iPhone might be a pain in the ass (and expensive, as you noted), paying one-fourth the price of a repair for just something you can use to get by is an option. It’ll tide you over until your work hours go back to normal and you can repair your original device (or buy something better, potentially). Let’s recap: I don’t love any of these options, but there they are.  Send it in to Apple. Expensive, but authentic parts. You’ll have to wait, but you’ll get your iPhone fixed up the best possible way.  Find an authorized service provider: This will be tough in NYC during a quarantine, but if you can find an Apple provider that’s available, you might save a little time on your repair. Money? No. Time? Maybe.  Find a third-party repair shop. Depending on what’s open, you might save cash and time, but this can be a gamble. Replacing the screen at an unauthorized repair shop will void your Apple warranty, if applicable, and possibly lead to a worse-looking display depending on what parts the repair shop is using. You also might be fine. It’s a gamble—less so for older iPhones, and more so for newer ones.    Buying a used but functional replacement. Whether you’re replacing your broken phone with the exact same model or** **going back a generation (or two) to simply get something that’s cheap and functional, delaying the repair is certainly an option worth considering.   Phone a friend. Try asking around. The odds are decent that someone has a spare iPhone (likely an older one) sitting around in a drawer somewhere that they aren’t using. See if you can borrow it until you’re in a better spot, job-wise, then consider your repair options from there. Swapping out your SIM to another iPhone is easy. "
    }, {
    "id": 41,
    "url": "https://bright-softwares.com/blog/en/productivity/help-windows-wont-boot-correctly-after-a-recent-os-update",
    "title": "Help! Windows Won't Boot Correctly After a Recent OS Update",
    "body": "2020/08/11 - What happens when Windows starts tossing up annoying error messages each and every time you try to launch the operating system? If you’re trying to log in for the day and actually do work for the critical deadlines you have, and you don’t really have an IT department to help out, this is probably the worst spot to be in. Since we’re all stuck—or will soon be stuck—in our homes and apartments for the foreseeable future, The question::  I have a Windows 10 desktop. Several weeks ago windows did an update and several days later when I powered on, I got the message “windows didn’t load properly” blue screen with several options. The ones I tried didn’t work well, until I found the “Revert to a previous date” option. That worked well. Computer went back on and functioned normally. Unfortunately I still receive the windows did not load properly screen every several days. Im running Windows 10 on a Mac. I was thinking of completely wiping out windows and then reloading it. Maybe I have a virus or bug ? Thanks !! The humble answer:: For what it’s worth, you’re not alone on this one. Microsoft has been having issues with Windows 10 updates lately, which can sometimes introduce more problems for users than they can fix. It’s possible that you’re in this camp, but the good news is that it’s very, very unlikely that you’ve been hit with a virus or some piece of sketchy malware. It’s just a Windows issue—not very soothing to hear when you’re experiencing it, I’m sure, but at least slight more comforting than, “Your system is infected” (I hope). Generally speaking, I like to abandon ship at the first sign of trouble that would likely take me longer to troubleshoot—with mixed results—than it would take me to reinstall Windows and all of my applications. I suspect that might be the case here. And since you’re Boot Camping into Windows on your Mac, I’m less bothered by putting you out of commission for a bit, since you’ll always have macOS to use if you absolutely need to do something on your computer. Before we go nuclear, though, let’s try a few things. First off, if you can boot into Windows—and it sounds like you can—I’m not sure there’s anything you can uninstall that will help you. The last major Windows 10 update that would have probably affected you was the big 1909 update from November. There have been a number of piecemeal updates since then, but I can’t think of one that’s been especially problematic. Oh, except for KB4535996, which even Microsoft suggested users uninstall. So, let’s start there. Pull up Windows Update, click on View Update History, click on Uninstall Updates, and look to see if you can uninstall KB4535996. If you can, great! If not, there goes that troubleshooting technique. While you’re here, maybe check to see if there are any additional Windows updates you can install. It’s a long shot, but perhaps something has arrived that could fix whatever issues your Windows installation is struggling to deal with. And since you’re using Boot Camp to run Windows on your Mac, pull up Apple Software Update and make sure there aren’t any new drivers or updates to install. How Can I Keep an Old All-In-One PC Running Quickly and Efficiently?I’m always thrilled to get “help me out with a tech problem” letters in my inbox, and I’ve had… Finally, try reinstalling Boot Camp’s Windows Support Drivers, which might magically cure whatever is causing your system to blue screen upon launch. There’s no guarantee this will fix things, but it’s worth exploring before you take more drastic measures. While you’re in Windows, you can also open up an elevated command prompt (search for “Command Prompt” in the Start Menu, right click on it, and select “Run as administrator”). From there, try running a simple “chkdsk /f” to conform there aren’t any issues with your file system. You can also try “chkdsk /r /f” for a much more thorough analysis and fixing process, but it’ll take a lot longer. If your hard drive is failing and that’s the reason behind your Windows issues, it’s also possible you might not get any additional information from chkdsk. You’ll want to use some other techniques to confirm you’re ok (or headed toward disaster). You can also run “sfc /verifyonly” followed by “sfc /scannow” in the same elevated command prompt. If the first command found any corruption in your Windows system files, the second command should fix them. Once you’ve finished this, consider pulling up the Windows Troubleshooter. Pull up the old-school Control Panel (via the Start Menu) and select Troubleshooting. Then, click on “Fix problems with Windows Update,” and see what the utility finds (if anything!) Finally, click on your Start Menu, click on the Power icon, hold down the Shift key on your keyboard, and click on Restart. This should boot you into Windows 10’s Advanced Startup options menu. Click on Troubleshoot, click on Advanced options, and try using the Startup Repair option to see if that can solve your Windows problem. How Can You Keep Your Old Desktop PC Running Well?When your desktop PC is aging, but you don’t have the heart (or the budget) to replace it, there’s… If all else fails, then a wipe and restore might be your best option. Save all your critical Windows 10 files to a flash drive or cloud storage, then launch macOS and use Boot Camp Assistant to remove your Windows OS. Use Microsoft’s Media Creation Tool to download a new, fresh . ISO of Windows 10, and then use Boot Camp to reinstall that on your Mac. Once Windows is up and running, make sure you’ve installed any updates from Apple (the aforementioned Windows Support Drivers and Apple Software Update) first, then install all the WIndows Updates Microsoft offers, then start putting your files and apps back on your system once you’ve verified that everything feels right. Don’t worry; it takes a lot less time than it sounds like. "
    }, {
    "id": 42,
    "url": "https://bright-softwares.com/blog/fr/kubernetes/playing-kubernetes-with-minikube",
    "title": "Playing Kubernetes with Minikube",
    "body": "2020/08/04 - Installing Kubernetes with MinikubeMinikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Minikube Features : Minikube supports the following Kubernetes features:  DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, CRI-O, and containerd Enabling CNI (Container Network Interface) IngressInstallation See Installing Minikube. Quickstart : This brief demo guides you on how to start, use, and delete Minikube locally. Follow the steps given below to start and explore Minikube.  Start Minikube and create a cluster:minikube start The output is similar to this: Starting local Kubernetes cluster… Running pre-create checks… Creating machine… Starting local Kubernetes cluster… For more information on starting your cluster on a specific Kubernetes version, VM, or container runtime, see Starting a Cluster.  Now, you can interact with your cluster using kubectl. For more information, see Interacting with Your Cluster. Let’s create a Kubernetes Deployment using an existing image named echoserver, which is a simple HTTP server and expose it on port 8080 using –port. kubectl create deployment hello-minikube –image=k8s. gcr. io/echoserver:1. 10 The output is similar to this: deployment. apps/hello-minikube created 3. To access the hello-minikube Deployment, expose it as a Service: kubectl expose deployment hello-minikube –type=NodePort –port=8080 The option –type=NodePort specifies the type of the Service. The output is similar to this: service/hello-minikube exposed 4. The hello-minikube Pod is now launched but you have to wait until the Pod is up before accessing it via the exposed Service. Check if the Pod is up and running: kubectl get pod If the output shows the STATUS as ContainerCreating, the Pod is still being created: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 0/1 ContainerCreating 0 3s If the output shows the STATUS as Running, the Pod is now up and running: NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 1/1 Running 0 13s 5. Get the URL of the exposed Service to view the Service details: minikube service hello-minikube –url 6. To view the details of your local cluster, copy and paste the URL you got as the output, on your browser. The output is similar to this: Hostname: hello-minikube-7c77b68cff-8wdzq Pod Information: -no pod information available- Server values: server_version=nginx: 1. 13. 3 - lua: 10008 Request Information: client_address=172. 17. 0. 1 method=GET real path=/ query= request_version=1. 1 request_scheme=http request_uri=http://192. 168. 99. 100:8080/ Request Headers: accept=/ host=192. 168. 99. 100:30674 user-agent=curl/7. 47. 0 Request Body: -no body in request- If you no longer want the Service and cluster to run, you can delete them.  Delete the hello-minikube Service:kubectl delete services hello-minikube The output is similar to this: service “hello-minikube” deleted 8. Delete the hello-minikube Deployment: kubectl delete deployment hello-minikube The output is similar to this: deployment. extensions “hello-minikube” deleted 9. Stop the local Minikube cluster: minikube stop The output is similar to this: Stopping “minikube”… “minikube” stopped. For more information, see Stopping a Cluster.  Delete the local Minikube cluster:minikube delete The output is similar to this: Deleting “minikube” … The “minikube” cluster has been deleted. For more information, see Deleting a cluster. Managing your Cluster : Starting a Cluster : The minikube start command can be used to start your cluster. This command creates and configures a Virtual Machine that runs a single-node Kubernetes cluster. This command also configures your kubectl installation to communicate with this cluster.  Note:If you are behind a web proxy, you need to pass this information to the minikube start command:  https_proxy= minikube start --docker-env http_proxy= --docker-env https_proxy= --docker-env no_proxy=192. 168. 99. 0/24 Unfortunately, setting the environment variables alone does not work.  Minikube also creates a “minikube” context, and sets it to default in kubectl. To switch back to this context, run this command: kubectl config use-context minikube.  Specifying the Kubernetes version : You can specify the version of Kubernetes for Minikube to use by adding the –kubernetes-version string to the minikube start command. For example, to run version v1. 18. 0, you would run the following: minikube start –kubernetes-version v1. 18. 0 #### Specifying the VM driver You can change the VM driver by adding the –driver= flag to minikube start. For example the command would be. minikube start –driver= Minikube supports the following drivers:  Note: See DRIVERS for details on supported drivers and how to install plugins. * docker (driver installation)  virtualbox (driver installation) podman (driver installation) (EXPERIMENTAL) vmwarefusion kvm2 (driver installation) hyperkit (driver installation) hyperv (driver installation) Note that the IP below is dynamic and can change. It can be retrieved with minikube ip.  vmware (driver installation) (VMware unified driver) parallels (driver installation) none (Runs the Kubernetes components on the host and not in a virtual machine. You need to be running Linux and to have DockerDocker is a software technology providing operating-system-level virtualization also known as containers. installed. ) Caution: If you use the none driver, some Kubernetes components run as privileged containers that have side effects outside of the Minikube environment. Those side effects mean that the none driver is not recommended for personal workstations. #### Starting a cluster on alternative container runtimes You can start Minikube on the following container runtimes.  containerd CRI-OTo use containerd as the container runtime, run:minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=containerd \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=unix:///run/containerd/containerd. sock \ –extra-config=kubelet. image-service-endpoint=unix:///run/containerd/containerd. sock \ –bootstrapper=kubeadm To use CRI-O as the container runtime, run: minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=cri-o \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=/var/run/crio. sock \ –extra-config=kubelet. image-service-endpoint=/var/run/crio. sock \ –bootstrapper=kubeadm $(function(){$(“#container-runtimes”). tabs();});#### Use local images by re-using the Docker daemon When using a single VM for Kubernetes, it’s useful to reuse Minikube’s built-in Docker daemon. Reusing the built-in daemon means you don’t have to build a Docker registry on your host machine and push the image into it. Instead, you can build inside the same Docker daemon as Minikube, which speeds up local experiments.  Note: Be sure to tag your Docker image with something other than latest and use that tag to pull the image. Because :latest is the default value, with a corresponding default image pull policy of Always, an image pull error (ErrImagePull) eventually results if you do not have the Docker image in the default Docker registry (usually DockerHub). To work with the Docker daemon on your Mac/Linux host, run the last line from minikube docker-env. You can now use Docker at the command line of your host Mac/Linux machine to communicate with the Docker daemon inside the Minikube VM: docker ps  Note:On Centos 7, Docker may report the following error:  Could not read CA certificate “/etc/docker/ca. pem”: open /etc/docker/ca. pem: no such file or directory You can fix this by updating /etc/sysconfig/docker to ensure that Minikube’s environment changes are respected:  &lt; DOCKER_CERT_PATH=/etc/docker — &gt; if [ -z “${DOCKER_CERT_PATH}” ]; then &gt; DOCKER_CERT_PATH=/etc/docker &gt; fi ### Configuring Kubernetes Minikube has a “configurator” feature that allows users to configure the Kubernetes components with arbitrary values. To use this feature, you can use the –extra-config flag on the minikube start command. This flag is repeated, so you can pass it several times with several different values to set multiple options. This flag takes a string of the form component. key=value, where component is one of the strings from the below list, key is a value on the configuration struct and value is the value to set. Valid keys can be found by examining the documentation for the Kubernetes componentconfigs for each component. Here is the documentation for each supported configuration:  kubelet apiserver proxy controller-manager etcd schedulerExamples : To change the MaxPods setting to 5 on the Kubelet, pass this flag: –extra-config=kubelet. MaxPods=5. This feature also supports nested structs. To change the LeaderElection. LeaderElect setting to true on the scheduler, pass this flag: –extra-config=scheduler. LeaderElection. LeaderElect=true. To set the AuthorizationMode on the apiserver to RBAC, you can use: –extra-config=apiserver. authorization-mode=RBAC. Stopping a Cluster : The minikube stop command can be used to stop your cluster. This command shuts down the Minikube Virtual Machine, but preserves all cluster state and data. Starting the cluster again will restore it to its previous state. Deleting a Cluster : The minikube delete command can be used to delete your cluster. This command shuts down and deletes the Minikube Virtual Machine. No data or state is preserved. Upgrading Minikube : If you are using macOS and Brew Package Manager is installed run: brew update brew upgrade minikube Interacting with Your Cluster : Kubectl : The minikube start command creates a kubectl context called “minikube”. This context contains the configuration to communicate with your Minikube cluster. Minikube sets this context to default automatically, but if you need to switch back to it in the future, run: kubectl config use-context minikube Or pass the context on each command like this: kubectl get pods –context=minikube Dashboard : To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address: minikube dashboard ### Services To access a Service exposed via a node port, run this command in a shell after starting Minikube to get the address: minikube service [-n NAMESPACE] [–url] NAME Networking : The Minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command. Any services of type NodePort can be accessed over that IP address, on the NodePort. To determine the NodePort for your service, you can use a kubectl command like this: kubectl get service $SERVICE –output=’jsonpath=”{. spec. ports[0]. nodePort}”’ Persistent Volumes : Minikube supports PersistentVolumes of type hostPath. These PersistentVolumes are mapped to a directory inside the Minikube VM. The Minikube VM boots into a tmpfs, so most directories will not be persisted across reboots (minikube stop). However, Minikube is configured to persist files stored under the following host directories:  /data /var/lib/minikube /var/lib/dockerHere is an example PersistentVolume config to persist data in the /data directory:apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: data/pv0001/ Mounted Host Folders : Some drivers will mount a host folder within the VM so that you can easily share files between the VM and host. These are not configurable at the moment and different for the driver and OS you are using.  Note: Host folder sharing is not implemented in the KVM driver yet. DriverOSHostFolderVMVirtualBoxLinux/home/hosthomeVirtualBoxmacOS/Users/UsersVirtualBoxWindowsC://Users/c/UsersVMware FusionmacOS/Users/mnt/hgfs/UsersXhyvemacOS/Users/UsersPrivate Container Registries To access a private container registry, follow the steps on this page. We recommend you use ImagePullSecrets, but if you would like to configure access on the Minikube VM you can place the . dockercfg in the /home/docker directory or the config. json in the /home/docker/. docker directory. Add-ons : In order to have Minikube properly start or restart custom addons, place the addons you wish to be launched with Minikube in the ~/. minikube/addons directory. Addons in this folder will be moved to the Minikube VM and launched each time Minikube is started or restarted. Using Minikube with an HTTP Proxy : Minikube creates a Virtual Machine that includes Kubernetes and a Docker daemon. When Kubernetes attempts to schedule containers using Docker, the Docker daemon may require external network access to pull containers. If you are behind an HTTP proxy, you may need to supply Docker with the proxy settings. To do this, pass the required environment variables as flags during minikube start. For example: minikube start –docker-env http_proxy=http://$YOURPROXY:PORT \ –docker-env https_proxy=https://$YOURPROXY:PORT If your Virtual Machine address is 192. 168. 99. 100, then chances are your proxy settings will prevent kubectl from directly reaching it. To by-pass proxy configuration for this IP address, you should modify your no_proxy settings. You can do so with: export no_proxy=$no_proxy,$(minikube ip) Known Issues : Features that require multiple nodes will not work in Minikube. Design : Minikube uses libmachine for provisioning VMs, and kubeadm to provision a Kubernetes cluster. For more information about Minikube, see the proposal. Additional Links :  Goals and Non-Goals: For the goals and non-goals of the Minikube project, please see our roadmap.  Development Guide: See Contributing for an overview of how to send pull requests.  Building Minikube: For instructions on how to build/test Minikube from source, see the build guide.  Adding a New Dependency: For instructions on how to add a new dependency to Minikube, see the adding dependencies guide.  Adding a New Addon: For instructions on how to add a new addon for Minikube, see the adding an addon guide.  MicroK8s: Linux users wishing to avoid running a virtual machine may consider MicroK8s as an alternative. Community Contributions, questions, and comments are all welcomed and encouraged! Minikube developers hang out on Slack in the #minikube channel (get an invitation here). We also have the kubernetes-dev Google Groups mailing list. If you are posting to the list please prefix your subject with “minikube: “. Feedback: Was this page helpful? Yes NoThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow. Open an issue in the GitHub repo if you want to report a problem or suggest an improvement. "
    }, {
    "id": 43,
    "url": "https://bright-softwares.com/blog/fr/docker/docker-tip-inspect-and-less",
    "title": "Docker tip : inspect and less",
    "body": "2020/08/04 - Docker inspect and less: This isn’t so much a docker tip, as it is a jq tip. If you haven’t heard of jq, it is a great tool for parsing JSON from the command line. This also makes it a great tool to see what is happening in a container instead of having to use the –format specifier which I can never remember how to use exactly: # Get network information:$ docker inspect 4c45aea49180 | jq '. []. NetworkSettings. Networks'{  bridge : {   EndpointID :  ba1b6efba16de99f260e0fa8892fd4685dbe2f79cba37ac0114195e9fad66075 ,   Gateway :  172. 17. 0. 1 ,   IPAddress :  172. 17. 0. 2 ,   IPPrefixLen : 16,   IPv6Gateway :   ,   GlobalIPv6Address :   ,   GlobalIPv6PrefixLen : 0,   MacAddress :  02:42:ac:11:00:02  }}# Get the arguments with which the container was started$ docker inspect 4c45aea49180 | jq '. []. Args'[ -server , -advertise , 192. 168. 99. 100 , -bootstrap-expect , 1 ]# Get all the mounted volumes11:22 $ docker inspect 4c45aea49180 | jq '. []. Mounts'[{ Name :  a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f , Source :  /mnt/sda1/var/lib/docker/volumes/a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f/_data , Destination :  /data , Driver :  local , Mode :   , RW : true}]And of course also works great for querying other kinds of (docker-esque) APIs that produce JSON (e. g Marathon, Mesos, Consul etc. ). JQ provides a very extensive API for accessing and processing JSON. More information can be found here: https://stedolan. github. io/jq/ "
    }, {
    "id": 44,
    "url": "https://bright-softwares.com/blog/fr/docker/docker-tip-inspect-and-jq",
    "title": "Docker tip : inspect and jq",
    "body": "2020/08/04 - Docker inspect and jq: This isn’t so much a docker tip, as it is a jq tip. If you haven’t heard of jq, it is a great tool for parsing JSON from the command line. This also makes it a great tool to see what is happening in a container instead of having to use the –format specifier which I can never remember how to use exactly: # Get network information:$ docker inspect 4c45aea49180 | jq '. []. NetworkSettings. Networks'{  bridge : {   EndpointID :  ba1b6efba16de99f260e0fa8892fd4685dbe2f79cba37ac0114195e9fad66075 ,   Gateway :  172. 17. 0. 1 ,   IPAddress :  172. 17. 0. 2 ,   IPPrefixLen : 16,   IPv6Gateway :   ,   GlobalIPv6Address :   ,   GlobalIPv6PrefixLen : 0,   MacAddress :  02:42:ac:11:00:02  }}# Get the arguments with which the container was started$ docker inspect 4c45aea49180 | jq '. []. Args'[ -server , -advertise , 192. 168. 99. 100 , -bootstrap-expect , 1 ]# Get all the mounted volumes11:22 $ docker inspect 4c45aea49180 | jq '. []. Mounts'[{ Name :  a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f , Source :  /mnt/sda1/var/lib/docker/volumes/a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f/_data , Destination :  /data , Driver :  local , Mode :   , RW : true}]And of course also works great for querying other kinds of (docker-esque) APIs that produce JSON (e. g Marathon, Mesos, Consul etc. ). JQ provides a very extensive API for accessing and processing JSON. More information can be found here: https://stedolan. github. io/jq/ "
    }, {
    "id": 45,
    "url": "https://bright-softwares.com/blog/fr/docker/docker-tip-inspect-and-grep",
    "title": "Docker tip : inspect and grep",
    "body": "2020/08/04 - Docker inspect and grep: This isn’t so much a docker tip, as it is a jq tip. If you haven’t heard of jq, it is a great tool for parsing JSON from the command line. This also makes it a great tool to see what is happening in a container instead of having to use the –format specifier which I can never remember how to use exactly: # Get network information:$ docker inspect 4c45aea49180 | jq '. []. NetworkSettings. Networks'{  bridge : {   EndpointID :  ba1b6efba16de99f260e0fa8892fd4685dbe2f79cba37ac0114195e9fad66075 ,   Gateway :  172. 17. 0. 1 ,   IPAddress :  172. 17. 0. 2 ,   IPPrefixLen : 16,   IPv6Gateway :   ,   GlobalIPv6Address :   ,   GlobalIPv6PrefixLen : 0,   MacAddress :  02:42:ac:11:00:02  }}# Get the arguments with which the container was started$ docker inspect 4c45aea49180 | jq '. []. Args'[ -server , -advertise , 192. 168. 99. 100 , -bootstrap-expect , 1 ]# Get all the mounted volumes11:22 $ docker inspect 4c45aea49180 | jq '. []. Mounts'[{ Name :  a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f , Source :  /mnt/sda1/var/lib/docker/volumes/a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f/_data , Destination :  /data , Driver :  local , Mode :   , RW : true}]And of course also works great for querying other kinds of (docker-esque) APIs that produce JSON (e. g Marathon, Mesos, Consul etc. ). JQ provides a very extensive API for accessing and processing JSON. More information can be found here: https://stedolan. github. io/jq/ "
    }, {
    "id": 46,
    "url": "https://bright-softwares.com/blog/fr/productivity/do-i-need-a-new-cable-modem-if-im-stuck-working-from-home",
    "title": "Do I Need a New Cable Modem If I'm Stuck Working From Home?",
    "body": "2020/08/04 - The question:: I have to work from home for a fancy tech company and it takes too long to download and upload my work. I upgraded my internet plan with Comcast, but does that mean I also need to get a new cable modem? The correct answer:: I\xe2\x80\x99m going to go with the good ol\xe2\x80\x99 \xe2\x80\x9cIt depends. \xe2\x80\x9d However, I first want to commend you for actually owning your own cable modem instead of coughing up $14 or so each month for Comcast to rent you one of their meh modem/router combinations. There\xe2\x80\x99s no reason you should be paying a fee to rent that which you can purchase yourself. Of course, this also means that you\xe2\x80\x99ll have to buy a new cable modem if, or when, your internet plan exceeds your modem\xe2\x80\x99s capabilities. In this case, I asked my friend for more information, and he mentioned that he had bumped up to a 300Mbps plan from Comcast. He didn\xe2\x80\x99t mention what his service plan was before that, so there is the possibility that he would need to buy a new cable modem to be able to max out on those speeds\xe2\x80\x94download speeds, that is, since \xe2\x80\x9c300Mbps,\xe2\x80\x9d as we know, doesn\xe2\x80\x99t mean 300Mbps both ways. As it turns out, his Netgear CM500 cable modem was more than up for the task. A quick Google search confirms that it supports speeds of up to 680Mbps\xe2\x80\x94more than enough for a 300Mbps plan. And while I could get into the technical details of what \xe2\x80\x9c16x4” means as part of its configuration, that\xe2\x80\x99s irrelevant for most people. It\xe2\x80\x99s compatible with Comcast and, most importantly, Comcast itself certifies that it\xe2\x80\x99ll support 300Mbps speeds. That bit is important, because you should never assume that a manufacturer is correct when it comes to cable modems. Double-check with your ISP\xe2\x80\x99s list of compatible cable modems to confirm that whatever you own, or are looking to purchase, works with the company\xe2\x80\x99s service and can handle whatever speeds you\xe2\x80\x99re expecting. If you don\xe2\x80\x99t see your modem on the list, or don\xe2\x80\x99t see a list at all, give them a ring. To note: If you also get phone service through your ISP, the kind of thing where you\xe2\x80\x99re actually connecting a VOIP phone to your modem, then you\xe2\x80\x99ll probably need a more souped-up (and expensive) cable modem. Them\xe2\x80\x99s the breaks; don\xe2\x80\x99t buy a new, cheap, typical modem and assume that it\xe2\x80\x99ll work with your VOIP setup. But maybe it\xe2\x80\x99s not the cable modem…: I also think it\xe2\x80\x99s important to know that upgrading your service plan through your ISP, and buying a new cable modem, might not actually give you the speed boosts you\xe2\x80\x99re expecting. If your wifi setup at home is crappy or underpowered, or if you\xe2\x80\x99re trying to access your files from a location that\xe2\x80\x99s fairly far away from your primary router\xe2\x80\x94or there\xe2\x80\x99s a lot of stuff (including walls) between it and you\xe2\x80\x94then your wireless setup might be more to blame than your service plan. Honestly, I\xe2\x80\x99d investigate that first. Look up whatever service tier you\xe2\x80\x99re paying for on your ISP\xe2\x80\x99s monthly bill, and walk around your house\xe2\x80\x94running either fast. com or speedtest. net at various points\xe2\x80\x94to see whether you\xe2\x80\x99re getting that maximum speed in the places you usually use your various devices. If not, fixing your wireless setup should be priority one, then worry about faster internet services from your ISP. And how do you do that, you ask? To start, make sure your router is in a central location within your house. That might involve you stringing ethernet cable along floors and ceilings to get it there, rather than the tiny closet in the extreme corner of your house that\xe2\x80\x99s farthest away from your work-at-home desk. This will pay dividends, trust me. (And if you\xe2\x80\x99re quarantined, you have plenty of time for projects like these!) Beyond that, if you\xe2\x80\x99re extending your router\xe2\x80\x99s signal with a wifi extender or mesh setup, know that each \xe2\x80\x9chop\xe2\x80\x9d you make in the chain\xe2\x80\x94from a router, to an extender, to another extender, to your laptop\xe2\x80\x94can cut your throughput in half each time. Unless your mesh setup has a dedicated backhaul connection, and your access points are located close enough to one another to benefit from a strong connection, simply extending your network to where you need wireless coverage can get you online, but it might be painfully slow. As always, use wired Ethernet wherever possible for the best possible speeds. Grab a dongle and connect your laptop to your wall\xe2\x80\x99s Ethernet port (using a solid Cat6 cable, since I like to future-proof and it\xe2\x80\x99s hardly more expensive than a Cat5 or Cat5e). Use Ethernet cables to connect your access points and router to one another, annoying as it might be, rather than extending your wireless network using wifi. It\xe2\x80\x99s a pain in the butt, I know. But I just converted a house from an all-wireless setup to wired, and now the family can finally enjoy the gigabit ethernet speeds they\xe2\x80\x99ve been paying for\xe2\x80\x94speeds they *never *had, since they were relying on a crappy cable modem/router and extender \xe2\x80\x9cpods\xe2\x80\x9d to try and fill their house with wireless connectivity. They could have saved a ton of money by dialing back their internet plan to match the capabilities of their network; that, or they could have upgraded their network, as I did, with access points connected to a primary switch (and router) via Ethernet cable and actually enjoyed the super-fast speeds they were paying for. That\xe2\x80\x99s just my long-winded way to say that your internet plan might not be what you should first blame for slow internet speeds at home. It\xe2\x80\x99s a part of the equation, sure, but you\xe2\x80\x99ll want to make sure your wireless network is up to snuff, too. If, or when, you upgrade your plan and get a new cable modem that supports it (if applicable), make sure you\xe2\x80\x99re doing the same process\xe2\x80\x94walking around your house, testing your speeds, and fixing up your wifi network\xe2\x80\x94to make sure you\xe2\x80\x99re not just paying your ISP for service you\xe2\x80\x99ll never see. Do you have a tech question keeping you up at night? Tired of troubleshooting your Windows or Mac? Looking for advice on apps, browser extensions, or utilities to accomplish a particular task? Let us know! Tell us in the comments below or email __david. murphy@lifehacker. com_. _ \n "
    }, {
    "id": 47,
    "url": "https://bright-softwares.com/blog/en/playing-kubernetes-with-minikube",
    "title": "Playing Kubernetes with Minikube",
    "body": "2020/08/04 - Installing Kubernetes with MinikubeMinikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Minikube Features : Minikube supports the following Kubernetes features:  DNS NodePorts ConfigMaps and Secrets Dashboards Container Runtime: Docker, CRI-O, and containerd Enabling CNI (Container Network Interface) IngressInstallation See Installing Minikube. Quickstart : This brief demo guides you on how to start, use, and delete Minikube locally. Follow the steps given below to start and explore Minikube.  Start Minikube and create a cluster:minikube start The output is similar to this: Starting local Kubernetes cluster. . . Running pre-create checks. . . Creating machine. . . Starting local Kubernetes cluster. . . For more information on starting your cluster on a specific Kubernetes version, VM, or container runtime, see Starting a Cluster.  Now, you can interact with your cluster using kubectl. For more information, see Interacting with Your Cluster. Let’s create a Kubernetes Deployment using an existing image named echoserver, which is a simple HTTP server and expose it on port 8080 using –port.   kubectl create deployment hello-minikube --image=k8s. gcr. io/echoserver:1. 10The output is similar to this:   deployment. apps/hello-minikube created  To access the hello-minikube Deployment, expose it as a Service:  kubectl expose deployment hello-minikube --type=NodePort --port=8080The option --type=NodePort specifies the type of the Service. The output is similar to this:   service/hello-minikube exposed The hello-minikube Pod is now launched but you have to wait until the Pod is up before accessing it via the exposed Service. Check if the Pod is up and running:   kubectl get pod If the output shows the STATUS as ContainerCreating, the Pod is still being created:   NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 0/1 ContainerCreating 0 3sIf the output shows the STATUS as Running, the Pod is now up and running:   NAME READY STATUS RESTARTS AGE hello-minikube-3383150820-vctvh 1/1 Running 0 13s 5. Get the URL of the exposed Service to view the Service details:   minikube service hello-minikube --url 6. To view the details of your local cluster, copy and paste the URL you got as the output, on your browser. The output is similar to this:   Hostname: hello-minikube-7c77b68cff-8wdzq Pod Information: -no pod information available- Server values: server_version=nginx: 1. 13. 3 - lua: 10008 Request Information: client_address=172. 17. 0. 1 method=GET real path=/ query= request_version=1. 1 request_scheme=http request_uri=http://192. 168. 99. 100:8080/ Request Headers: accept=_/_ host=192. 168. 99. 100:30674 user-agent=curl/7. 47. 0 Request Body: -no body in request- If you no longer want the Service and cluster to run, you can delete them.  Delete the hello-minikube Service:  kubectl delete services hello-minikube The output is similar to this:   service  hello-minikube  deleted  Delete the hello-minikube Deployment:  kubectl delete deployment hello-minikube The output is similar to this:   deployment. extensions  hello-minikube  deleted  Stop the local Minikube cluster:  minikube stop The output is similar to this:   Stopping  minikube . . .  minikube  stopped. For more information, see Stopping a Cluster.  Delete the local Minikube cluster:  minikube delete The output is similar to this:   Deleting  minikube  . . . The  minikube  cluster has been deleted. For more information, see Deleting a cluster. Managing your Cluster : Starting a Cluster : The minikube start command can be used to start your cluster. This command creates and configures a Virtual Machine that runs a single-node Kubernetes cluster. This command also configures your kubectl installation to communicate with this cluster.  Note:If you are behind a web proxy, you need to pass this information to the minikube start command:  https_proxy=&lt;my proxy&gt; minikube start --docker-env http_proxy=&lt;my proxy&gt; --docker-env https_proxy=&lt;my proxy&gt; --docker-env no_proxy=192. 168. 99. 0/24   Unfortunately, setting the environment variables alone does not work.  Minikube also creates a “minikube” context, and sets it to default in kubectl. To switch back to this context, run this command: kubectl config use-context minikube.  Specifying the Kubernetes version : You can specify the version of Kubernetes for Minikube to use by adding the --kubernetes-version string to the minikube start command. For example, to run version v1. 18. 0, you would run the following:   minikube start --kubernetes-version v1. 18. 0Specifying the VM driver : You can change the VM driver by adding the --driver=&lt;enter_driver_name&gt; flag to minikube start. For example the command would be.   minikube start --driver=&lt;driver_name&gt; Minikube supports the following drivers:  Note: See DRIVERS for details on supported drivers and how to install plugins. * docker (driver installation)  virtualbox (driver installation) podman (driver installation) (EXPERIMENTAL) vmwarefusion kvm2 (driver installation) hyperkit (driver installation) hyperv (driver installation) Note that the IP below is dynamic and can change. It can be retrieved with minikube ip.  vmware (driver installation) (VMware unified driver) parallels (driver installation) none (Runs the Kubernetes components on the host and not in a virtual machine. You need to be running Linux and to have DockerDocker is a software technology providing operating-system-level virtualization also known as containers. installed. ) Caution: If you use the none driver, some Kubernetes components run as privileged containers that have side effects outside of the Minikube environment. Those side effects mean that the none driver is not recommended for personal workstations. #### Starting a cluster on alternative container runtimes You can start Minikube on the following container runtimes.  containerd CRI-OTo use containerd as the container runtime, run:minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=containerd \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=unix:///run/containerd/containerd. sock \ –extra-config=kubelet. image-service-endpoint=unix:///run/containerd/containerd. sock \ –bootstrapper=kubeadm To use CRI-O as the container runtime, run: minikube start \ –network-plugin=cni \ –enable-default-cni \ –container-runtime=cri-o \ –bootstrapper=kubeadm Or you can use the extended version: minikube start \ –network-plugin=cni \ –enable-default-cni \ –extra-config=kubelet. container-runtime=remote \ –extra-config=kubelet. container-runtime-endpoint=/var/run/crio. sock \ –extra-config=kubelet. image-service-endpoint=/var/run/crio. sock \ –bootstrapper=kubeadm $(function(){$(“#container-runtimes”). tabs();}); Use local images by re-using the Docker daemon : When using a single VM for Kubernetes, it’s useful to reuse Minikube’s built-in Docker daemon. Reusing the built-in daemon means you don’t have to build a Docker registry on your host machine and push the image into it. Instead, you can build inside the same Docker daemon as Minikube, which speeds up local experiments.  Note: Be sure to tag your Docker image with something other than latest and use that tag to pull the image. Because :latest is the default value, with a corresponding default image pull policy of Always, an image pull error (ErrImagePull) eventually results if you do not have the Docker image in the default Docker registry (usually DockerHub). To work with the Docker daemon on your Mac/Linux host, run the last line from minikube docker-env. You can now use Docker at the command line of your host Mac/Linux machine to communicate with the Docker daemon inside the Minikube VM: docker ps  Note:On Centos 7, Docker may report the following error:  Could not read CA certificate “/etc/docker/ca. pem”: open /etc/docker/ca. pem: no such file or directory You can fix this by updating /etc/sysconfig/docker to ensure that Minikube’s environment changes are respected:  &lt; DOCKER_CERT_PATH=/etc/docker — &gt; if [ -z “${DOCKER_CERT_PATH}” ]; then &gt; DOCKER_CERT_PATH=/etc/docker &gt; fi ### Configuring Kubernetes Minikube has a “configurator” feature that allows users to configure the Kubernetes components with arbitrary values. To use this feature, you can use the –extra-config flag on the minikube start command. This flag is repeated, so you can pass it several times with several different values to set multiple options. This flag takes a string of the form component. key=value, where component is one of the strings from the below list, key is a value on the configuration struct and value is the value to set. Valid keys can be found by examining the documentation for the Kubernetes componentconfigs for each component. Here is the documentation for each supported configuration:  kubelet apiserver proxy controller-manager etcd schedulerExamples : To change the MaxPods setting to 5 on the Kubelet, pass this flag: –extra-config=kubelet. MaxPods=5. This feature also supports nested structs. To change the LeaderElection. LeaderElect setting to true on the scheduler, pass this flag: –extra-config=scheduler. LeaderElection. LeaderElect=true. To set the AuthorizationMode on the apiserver to RBAC, you can use: –extra-config=apiserver. authorization-mode=RBAC. Stopping a Cluster : The minikube stop command can be used to stop your cluster. This command shuts down the Minikube Virtual Machine, but preserves all cluster state and data. Starting the cluster again will restore it to its previous state. Deleting a Cluster : The minikube delete command can be used to delete your cluster. This command shuts down and deletes the Minikube Virtual Machine. No data or state is preserved. Upgrading Minikube : If you are using macOS and Brew Package Manager is installed run: brew update brew upgrade minikube Interacting with Your Cluster : Kubectl : The minikube start command creates a kubectl context called “minikube”. This context contains the configuration to communicate with your Minikube cluster. Minikube sets this context to default automatically, but if you need to switch back to it in the future, run:   kubectl config use-context minikubeOr pass the context on each command like this:   kubectl get pods --context=minikubeDashboard : To access the Kubernetes Dashboard, run this command in a shell after starting Minikube to get the address:   minikube dashboardServices : To access a Service exposed via a node port, run this command in a shell after starting Minikube to get the address:   minikube service [-n NAMESPACE] [--url] NAME Networking The Minikube VM is exposed to the host system via a host-only IP address, that can be obtained with the minikube ip command. Any services of type NodePort can be accessed over that IP address, on the NodePort. To determine the NodePort for your service, you can use a kubectl command like this:   kubectl get service $SERVICE --output='jsonpath= {. spec. ports[0]. nodePort} 'Persistent Volumes : Minikube supports PersistentVolumes of type hostPath. These PersistentVolumes are mapped to a directory inside the Minikube VM. The Minikube VM boots into a tmpfs, so most directories will not be persisted across reboots (minikube stop). However, Minikube is configured to persist files stored under the following host directories:  /data /var/lib/minikube /var/lib/dockerHere is an example PersistentVolume config to persist data in the /data directory:   ## apiVersion: v1 kind: PersistentVolume metadata: name: pv0001 spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi hostPath: path: data/pv0001/ Mounted Host Folders[ ](#mounted-host-folders)Some drivers will mount a host folder within the VM so that you can easily share files between the VM and host. These are not configurable at the moment and different for the driver and OS you are using.  Note: Host folder sharing is not implemented in the KVM driver yet. DriverOSHostFolderVMVirtualBoxLinux/home/hosthomeVirtualBoxmacOS/Users/UsersVirtualBoxWindowsC://Users/c/UsersVMware FusionmacOS/Users/mnt/hgfs/UsersXhyvemacOS/Users/UsersPrivate Container Registries To access a private container registry, follow the steps on this page. We recommend you use ImagePullSecrets, but if you would like to configure access on the Minikube VM you can place the . dockercfg in the /home/docker directory or the config. json in the /home/docker/. docker directory. Add-ons : In order to have Minikube properly start or restart custom addons, place the addons you wish to be launched with Minikube in the ~/. minikube/addons directory. Addons in this folder will be moved to the Minikube VM and launched each time Minikube is started or restarted. Using Minikube with an HTTP Proxy : Minikube creates a Virtual Machine that includes Kubernetes and a Docker daemon. When Kubernetes attempts to schedule containers using Docker, the Docker daemon may require external network access to pull containers. If you are behind an HTTP proxy, you may need to supply Docker with the proxy settings. To do this, pass the required environment variables as flags during minikube start. For example:   minikube start --docker-env http_proxy=http://$YOURPROXY:PORT \ --docker-env https\_proxy=https://$YOURPROXY:PORTIf your Virtual Machine address is 192. 168. 99. 100, then chances are your proxy settings will prevent kubectl from directly reaching it. To by-pass proxy configuration for this IP address, you should modify your no_proxy settings. You can do so with:   ## export no_proxy=$no\_proxy,$(minikube ip) Known Issues[ ](#known-issues)Features that require multiple nodes will not work in Minikube. Design : Minikube uses libmachine for provisioning VMs, and kubeadm to provision a Kubernetes cluster. For more information about Minikube, see the proposal. "
    }, {
    "id": 48,
    "url": "https://bright-softwares.com/blog/en/docker/docker-tip-inspect-and-less",
    "title": "Docker tip : inspect and less",
    "body": "2020/08/04 - Docker inspect and less: This isn’t so much a docker tip, as it is a jq tip. If you haven’t heard of jq, it is a great tool for parsing JSON from the command line. This also makes it a great tool to see what is happening in a container instead of having to use the –format specifier which I can never remember how to use exactly: # Get network information:$ docker inspect 4c45aea49180 | jq '. []. NetworkSettings. Networks'{  bridge : {   EndpointID :  ba1b6efba16de99f260e0fa8892fd4685dbe2f79cba37ac0114195e9fad66075 ,   Gateway :  172. 17. 0. 1 ,   IPAddress :  172. 17. 0. 2 ,   IPPrefixLen : 16,   IPv6Gateway :   ,   GlobalIPv6Address :   ,   GlobalIPv6PrefixLen : 0,   MacAddress :  02:42:ac:11:00:02  }}# Get the arguments with which the container was started$ docker inspect 4c45aea49180 | jq '. []. Args'[ -server , -advertise , 192. 168. 99. 100 , -bootstrap-expect , 1 ]# Get all the mounted volumes11:22 $ docker inspect 4c45aea49180 | jq '. []. Mounts'[{ Name :  a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f , Source :  /mnt/sda1/var/lib/docker/volumes/a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f/_data , Destination :  /data , Driver :  local , Mode :   , RW : true}]And of course also works great for querying other kinds of (docker-esque) APIs that produce JSON (e. g Marathon, Mesos, Consul etc. ). JQ provides a very extensive API for accessing and processing JSON. More information can be found here: https://stedolan. github. io/jq/ "
    }, {
    "id": 49,
    "url": "https://bright-softwares.com/blog/en/docker/docker-tip-inspect-and-grep",
    "title": "Docker tip : inspect and grep",
    "body": "2020/08/04 - Docker inspect and grep: This isn’t so much a docker tip, as it is a jq tip. If you haven’t heard of jq, it is a great tool for parsing JSON from the command line. This also makes it a great tool to see what is happening in a container instead of having to use the –format specifier which I can never remember how to use exactly: # Get network information:$ docker inspect 4c45aea49180 | jq '. []. NetworkSettings. Networks'{  bridge : {   EndpointID :  ba1b6efba16de99f260e0fa8892fd4685dbe2f79cba37ac0114195e9fad66075 ,   Gateway :  172. 17. 0. 1 ,   IPAddress :  172. 17. 0. 2 ,   IPPrefixLen : 16,   IPv6Gateway :   ,   GlobalIPv6Address :   ,   GlobalIPv6PrefixLen : 0,   MacAddress :  02:42:ac:11:00:02  }}# Get the arguments with which the container was started$ docker inspect 4c45aea49180 | jq '. []. Args'[ -server , -advertise , 192. 168. 99. 100 , -bootstrap-expect , 1 ]# Get all the mounted volumes11:22 $ docker inspect 4c45aea49180 | jq '. []. Mounts'[{ Name :  a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f , Source :  /mnt/sda1/var/lib/docker/volumes/a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f/_data , Destination :  /data , Driver :  local , Mode :   , RW : true}]And of course also works great for querying other kinds of (docker-esque) APIs that produce JSON (e. g Marathon, Mesos, Consul etc. ). JQ provides a very extensive API for accessing and processing JSON. More information can be found here: https://stedolan. github. io/jq/ "
    }, {
    "id": 50,
    "url": "https://bright-softwares.com/blog/en/productivity/do-i-need-a-new-cable-modem-if-im-stuck-working-from-home",
    "title": "Do I Need a New Cable Modem If I'm Stuck Working From Home?",
    "body": "2020/08/04 - The question:: I have to work from home for a fancy tech company and it takes too long to download and upload my work. I upgraded my internet plan with Comcast, but does that mean I also need to get a new cable modem? The correct answer:: I\xe2\x80\x99m going to go with the good ol\xe2\x80\x99 \xe2\x80\x9cIt depends. \xe2\x80\x9d However, I first want to commend you for actually owning your own cable modem instead of coughing up $14 or so each month for Comcast to rent you one of their meh modem/router combinations. There\xe2\x80\x99s no reason you should be paying a fee to rent that which you can purchase yourself. Of course, this also means that you\xe2\x80\x99ll have to buy a new cable modem if, or when, your internet plan exceeds your modem\xe2\x80\x99s capabilities. In this case, I asked my friend for more information, and he mentioned that he had bumped up to a 300Mbps plan from Comcast. He didn\xe2\x80\x99t mention what his service plan was before that, so there is the possibility that he would need to buy a new cable modem to be able to max out on those speeds\xe2\x80\x94download speeds, that is, since \xe2\x80\x9c300Mbps,\xe2\x80\x9d as we know, doesn\xe2\x80\x99t mean 300Mbps both ways. As it turns out, his Netgear CM500 cable modem was more than up for the task. A quick Google search confirms that it supports speeds of up to 680Mbps\xe2\x80\x94more than enough for a 300Mbps plan. And while I could get into the technical details of what \xe2\x80\x9c16x4” means as part of its configuration, that\xe2\x80\x99s irrelevant for most people. It\xe2\x80\x99s compatible with Comcast and, most importantly, Comcast itself certifies that it\xe2\x80\x99ll support 300Mbps speeds. That bit is important, because you should never assume that a manufacturer is correct when it comes to cable modems. Double-check with your ISP\xe2\x80\x99s list of compatible cable modems to confirm that whatever you own, or are looking to purchase, works with the company\xe2\x80\x99s service and can handle whatever speeds you\xe2\x80\x99re expecting. If you don\xe2\x80\x99t see your modem on the list, or don\xe2\x80\x99t see a list at all, give them a ring. To note: If you also get phone service through your ISP, the kind of thing where you\xe2\x80\x99re actually connecting a VOIP phone to your modem, then you\xe2\x80\x99ll probably need a more souped-up (and expensive) cable modem. Them\xe2\x80\x99s the breaks; don\xe2\x80\x99t buy a new, cheap, typical modem and assume that it\xe2\x80\x99ll work with your VOIP setup. But maybe it\xe2\x80\x99s not the cable modem…: I also think it\xe2\x80\x99s important to know that upgrading your service plan through your ISP, and buying a new cable modem, might not actually give you the speed boosts you\xe2\x80\x99re expecting. If your wifi setup at home is crappy or underpowered, or if you\xe2\x80\x99re trying to access your files from a location that\xe2\x80\x99s fairly far away from your primary router\xe2\x80\x94or there\xe2\x80\x99s a lot of stuff (including walls) between it and you\xe2\x80\x94then your wireless setup might be more to blame than your service plan. Honestly, I\xe2\x80\x99d investigate that first. Look up whatever service tier you\xe2\x80\x99re paying for on your ISP\xe2\x80\x99s monthly bill, and walk around your house\xe2\x80\x94running either fast. com or speedtest. net at various points\xe2\x80\x94to see whether you\xe2\x80\x99re getting that maximum speed in the places you usually use your various devices. If not, fixing your wireless setup should be priority one, then worry about faster internet services from your ISP. And how do you do that, you ask? To start, make sure your router is in a central location within your house. That might involve you stringing ethernet cable along floors and ceilings to get it there, rather than the tiny closet in the extreme corner of your house that\xe2\x80\x99s farthest away from your work-at-home desk. This will pay dividends, trust me. (And if you\xe2\x80\x99re quarantined, you have plenty of time for projects like these!) Beyond that, if you\xe2\x80\x99re extending your router\xe2\x80\x99s signal with a wifi extender or mesh setup, know that each \xe2\x80\x9chop\xe2\x80\x9d you make in the chain\xe2\x80\x94from a router, to an extender, to another extender, to your laptop\xe2\x80\x94can cut your throughput in half each time. Unless your mesh setup has a dedicated backhaul connection, and your access points are located close enough to one another to benefit from a strong connection, simply extending your network to where you need wireless coverage can get you online, but it might be painfully slow. As always, use wired Ethernet wherever possible for the best possible speeds. Grab a dongle and connect your laptop to your wall\xe2\x80\x99s Ethernet port (using a solid Cat6 cable, since I like to future-proof and it\xe2\x80\x99s hardly more expensive than a Cat5 or Cat5e). Use Ethernet cables to connect your access points and router to one another, annoying as it might be, rather than extending your wireless network using wifi. It\xe2\x80\x99s a pain in the butt, I know. But I just converted a house from an all-wireless setup to wired, and now the family can finally enjoy the gigabit ethernet speeds they\xe2\x80\x99ve been paying for\xe2\x80\x94speeds they *never *had, since they were relying on a crappy cable modem/router and extender \xe2\x80\x9cpods\xe2\x80\x9d to try and fill their house with wireless connectivity. They could have saved a ton of money by dialing back their internet plan to match the capabilities of their network; that, or they could have upgraded their network, as I did, with access points connected to a primary switch (and router) via Ethernet cable and actually enjoyed the super-fast speeds they were paying for. That\xe2\x80\x99s just my long-winded way to say that your internet plan might not be what you should first blame for slow internet speeds at home. It\xe2\x80\x99s a part of the equation, sure, but you\xe2\x80\x99ll want to make sure your wireless network is up to snuff, too. If, or when, you upgrade your plan and get a new cable modem that supports it (if applicable), make sure you\xe2\x80\x99re doing the same process\xe2\x80\x94walking around your house, testing your speeds, and fixing up your wifi network\xe2\x80\x94to make sure you\xe2\x80\x99re not just paying your ISP for service you\xe2\x80\x99ll never see. Do you have a tech question keeping you up at night? Tired of troubleshooting your Windows or Mac? Looking for advice on apps, browser extensions, or utilities to accomplish a particular task? Let us know! Tell us in the comments below or email __david. murphy@lifehacker. com_. _ \n "
    }, {
    "id": 51,
    "url": "https://bright-softwares.com/blog/fr/productivity/why-do-my-download-speeds-differ-so-drastically-between-computers",
    "title": "Why Do My Download Speeds Differ So Drastically Between Computers?",
    "body": "2020/07/28 - The good news? Measuring that ROI is arguably easier than ever for small businesses. By stressing efficiency and using the right tools, uncovering and tracking new leads, traffic and revenue don’t have to feel like a guessing game. Making it all happen starts with a process of defining what positive results look like and identifying the most efficient paths to achieving those results. For business leaders, this means making each and every marketing touchpoint, message and published asset count, so you know you’re getting the most bang for your buck. Instead of blindly throwing time and money at what “should” work, teams need to look inward to figure out how they can deliver more value. Below I’ve outlined what a smooth, streamlined marketing strategy looks like in 2018 and beyond. No tricks, no gimmicks or “hacks” here. Just five simple steps businesses of any size can follow to ensure their marketing efforts are as ROI-positive as possible. 1. Look before you leap: Prior to spending a dime or diving into any sort of marketing program, you need to have a plan. And remember, there’s a huge difference between brainstorming a vague or unattainably lofty marketing plan that you may never actually execute and mapping out your SMART goals. Unclear goals like “growing your audience” or “driving engagement” might sound nice, but what do they actually mean? For example, there’s a huge difference between “sending cold emails” and “sending 25 cold emails per day to a list of targeted prospects, to grow our customer base by 10% by the end of the quarter. ” Whether you’re talking about growing revenue, driving traffic or anything between, your team needs to have specific objectives in mind. In short, something you can measure. When you have a metric to track (think: dollar signs, funnel volume, or percentages), it’s much easier to assess how you’re doing and correct the course. Similarly, roles need to be assigned to hold your team accountable when it’s time to roll out a campaign. Who’s overseeing the campaign? Who’s going to track metrics? How long is the project going to last? What external stakeholders and platforms need to be involved? When these points are defined, you are in position to work toward an end-game instead of spinning your wheels in pursuit of “results. ” 2. Automate those repetitive tasks: There’s a reason why businesses both big and small are investing in automation like never before. For starters, businesses that automate marketing tasks grow 1. 6% faster than those that rely on human resources alone. Marketing in any shape or form can be cripplingly time-consuming. The ability to put even some of your marketing activity on autopilot is huge in a day and age where lack of time is the number one burden on today’s teams. Also, these tasks can grow tedious for your colleagues. Crafting social post after social post, email after email or report after report is a surefire road to burning out even the brightest of marketers over time. Not only does automation keep your team engaged, it also helps people emphasize their respective strengths. Let your star blogger focus on writing rather than metrics. Let your best salesperson worry about closing leads rather than finding them on social media. With the right platforms on deck, you can remove non-essential tasks from people’s plates and allow them to get down to business. But what exactly does effective marketing automation look like? Let’s take a quick look at some tools today’s top teams are using to streamline the tasks that eat up so much of their precious time. Metrics: Your marketing campaigns absolutely must be data-driven, but making sense of your data to others in your company can be a headache. That’s why tools like SuperMetrics are such lifesavers for marketers looking to clearly define the ROI of their marketing campaigns for their teams and management alike.  Image Source: SuperMetrics With the ability to run scheduled imports of your marketing data from Google Analytics, social media and ad platforms to one pre-formatted spreadsheet, you can spend less time bouncing between reports and more time, well, marketing. Content Distribution: Brands today are not only tasked with creating and boosting their own content, but also promoting the content of fellow industry players and influencers. The process of finding and promoting that content is no small feat, though. This is especially true considering that brands are expected to promote content multiple times per day across multiple platforms. That’s why tools such as Quuu are invaluable. This tool finds hand-picked content on your behalf, based on your niche topic selections, and packages it for you to promote.  Image Source: Quuu Social Media: Speaking of social media, this is possibly the number one marketing channel when it comes to professionals’ difficulty to assess ROI. After all, marketers can spend hours going back and forth with customers or leads without resulting in a single sale, or sales can come extremely quickly, but via “dark” click throughs. Tools such as Socedo, on the other hand, help transform your social presence into something measurable and actionable (think: SMART). Want to find relevant audience members who match your criteria for qualified leads? Want to funnel more Twitter traffic to your site? Want to engage your social products from all angles? No problem.  Image Source: G2Crowd2crowd Simply put, Socedo takes the question marks out of your social media ROI without sinking all of your time. 3. Make the most of your meetings: Meetings are a crucial component of tracking your progress, but they can quickly eat away at your schedule if you aren’t careful. This is true when coordinating internally and with customers alike. Think about it. How many times in the past month has a 20-minute call or check-in devolved into an hour-long back-and-forth session that didn’t yield any actionable takeaways? Much like your SMART goals, meetings demand defined purposes. Saying that you’re simply going to talk about a campaign isn’t going to cut it. Instead, you’re going to need to be specific, to-the-point and prepared prior to a meeting. Some quick tips for making your meetings more effective. … Define your talking points and schedule in a written agenda. This helps cut down on tangents and moves the discussion from Point A to Point B, lickety-split. Conclude with action items. Everyone involved needs a takeaway task from any given meeting, something to hold them accountable. Follow up on action items. Much like you review performance metrics, revisiting the results of a previous meeting is what’s going to make your discussions “stick. ”Meetings are absolutely necessary, but businesses today shouldn’t use them as an excuse to waste time. It’s all about finding a balance between saying what needs to be said and taking action. 4. Eliminate wasteful spending ASAP: Marketing is becoming more and more of a pay-to-play game. Facebook’s algorithm changes are just the tip of the iceberg. As a result, businesses need to be especially careful with their budgets. This means choosing the right platforms, outsourcing roles where appropriate and making the most of the impactful free platforms that you still have available. That said, how do you ensure that the money that you do decide to spend doesn’t go to waste? If your goal is to become a more frugal marketer in the future, you’re on the right track. Fortunately, there are plenty of proven strategies to boost your business that don’t require a massive financial investment. A shining example is thought leader Andy Crestodina’s zero-waste principle of collaborating on content with influencers who will be happy to share your content when it goes live. Don’t neglect the power of relationships when it comes to moving the needle with marketing. Of course, there are tools out there to help you manage your marketing spend, too. Services like Siftery can effectively break down how much you’re spending on any given subscription marketing platform and suggest cost-friendly alternatives to keep your budget in check.  Image Source: Siftery Also, consider strategies such as using a Facebook Pixel to effectively track each ad campaign’s visitors and leads as they interact with your brand. Rather than constantly chase new business, this type of measurement allows you to make sure your campaigns are optimized for sales – and to make the most of those who are already familiar with you by using retargeting and lookalikes. These sorts of solutions enable a marketing strategy that’s both cost-effective and focused on accountability. 5. Learn to avoid distractions and find your flow: Finally, don’t fall into the trap of being controlled by tech as you ramp up your business’s marketing campaigns. Think about. Emails. Phone calls. Texts. Notifications. They pour in day after day, don’t they? While you may see them as business as usual, these sorts of pulses can be incredibly distracting. Rather than live “in the moment” of distractions, marketers must focus on long-term goals. Cutting down on distractions means establishing dedicated timetables to take care of tasks, versus spending all day “putting out fires,” by checking emails, project management tool notifications or social updates.  Image Source: Business Insider Whether it’s a Chrome extension to block distracting sites or a built-in Pomodoro timer for your desktop, consider investing in some time management apps to maximize your productivity. You can even create your own version of the Eisenhower Decision Matrix to break down what you want done versus what needs to be done. The takeaway here? In order to avoid time-wasting, you must prioritize your tasks and find your flow ASAP. Conclusion: Truly efficient marketing comes from a thought-out combination of tactics and tools. Make every minute and dollar count. Focus on objectives and priorities instead of “what-if”s. Scale what works and drop what doesn’t. This five-step strategy is possible for businesses of all shapes and sizes. Don’t assume that a well-oiled marketing team is beyond your reach – these principles can be your pillars, even as the marketing landscape continues to change. Guest author:&nbsp;Vikas Agrawal&nbsp;is a start-up Investor &amp; co-founder of the&nbsp;Infographic design agency&nbsp;Infobrandz that offers creative and premium visual content solutions to medium to large companies. Content created by Infobrandz are loved, shared &amp; can be found all over the internet on high authority platforms like HuffingtonPost, Businessinsider, Forbes , Tech. co &amp; EliteDaily. &nbsp; &lt;/div&gt; "
    }, {
    "id": 52,
    "url": "https://bright-softwares.com/blog/en/productivity/why-do-my-download-speeds-differ-so-drastically-between-computers",
    "title": "Why Do My Download Speeds Differ So Drastically Between Computers?",
    "body": "2020/07/28 - The good news? Measuring that ROI is arguably easier than ever for small businesses. By stressing efficiency and using the right tools, uncovering and tracking new leads, traffic and revenue don’t have to feel like a guessing game. Making it all happen starts with a process of defining what positive results look like and identifying the most efficient paths to achieving those results. For business leaders, this means making each and every marketing touchpoint, message and published asset count, so you know you’re getting the most bang for your buck. Instead of blindly throwing time and money at what “should” work, teams need to look inward to figure out how they can deliver more value. Below I’ve outlined what a smooth, streamlined marketing strategy looks like in 2018 and beyond. No tricks, no gimmicks or “hacks” here. Just five simple steps businesses of any size can follow to ensure their marketing efforts are as ROI-positive as possible. 1. Look before you leap: Prior to spending a dime or diving into any sort of marketing program, you need to have a plan. And remember, there’s a huge difference between brainstorming a vague or unattainably lofty marketing plan that you may never actually execute and mapping out your SMART goals. Unclear goals like “growing your audience” or “driving engagement” might sound nice, but what do they actually mean? For example, there’s a huge difference between “sending cold emails” and “sending 25 cold emails per day to a list of targeted prospects, to grow our customer base by 10% by the end of the quarter. ” Whether you’re talking about growing revenue, driving traffic or anything between, your team needs to have specific objectives in mind. In short, something you can measure. When you have a metric to track (think: dollar signs, funnel volume, or percentages), it’s much easier to assess how you’re doing and correct the course. Similarly, roles need to be assigned to hold your team accountable when it’s time to roll out a campaign. Who’s overseeing the campaign? Who’s going to track metrics? How long is the project going to last? What external stakeholders and platforms need to be involved? When these points are defined, you are in position to work toward an end-game instead of spinning your wheels in pursuit of “results. ” 2. Automate those repetitive tasks: There’s a reason why businesses both big and small are investing in automation like never before. For starters, businesses that automate marketing tasks grow 1. 6% faster than those that rely on human resources alone. Marketing in any shape or form can be cripplingly time-consuming. The ability to put even some of your marketing activity on autopilot is huge in a day and age where lack of time is the number one burden on today’s teams. Also, these tasks can grow tedious for your colleagues. Crafting social post after social post, email after email or report after report is a surefire road to burning out even the brightest of marketers over time. Not only does automation keep your team engaged, it also helps people emphasize their respective strengths. Let your star blogger focus on writing rather than metrics. Let your best salesperson worry about closing leads rather than finding them on social media. With the right platforms on deck, you can remove non-essential tasks from people’s plates and allow them to get down to business. But what exactly does effective marketing automation look like? Let’s take a quick look at some tools today’s top teams are using to streamline the tasks that eat up so much of their precious time. Metrics: Your marketing campaigns absolutely must be data-driven, but making sense of your data to others in your company can be a headache. That’s why tools like SuperMetrics are such lifesavers for marketers looking to clearly define the ROI of their marketing campaigns for their teams and management alike.  Image Source: SuperMetrics With the ability to run scheduled imports of your marketing data from Google Analytics, social media and ad platforms to one pre-formatted spreadsheet, you can spend less time bouncing between reports and more time, well, marketing. Content Distribution: Brands today are not only tasked with creating and boosting their own content, but also promoting the content of fellow industry players and influencers. The process of finding and promoting that content is no small feat, though. This is especially true considering that brands are expected to promote content multiple times per day across multiple platforms. That’s why tools such as Quuu are invaluable. This tool finds hand-picked content on your behalf, based on your niche topic selections, and packages it for you to promote.  Image Source: Quuu Social Media: Speaking of social media, this is possibly the number one marketing channel when it comes to professionals’ difficulty to assess ROI. After all, marketers can spend hours going back and forth with customers or leads without resulting in a single sale, or sales can come extremely quickly, but via “dark” click throughs. Tools such as Socedo, on the other hand, help transform your social presence into something measurable and actionable (think: SMART). Want to find relevant audience members who match your criteria for qualified leads? Want to funnel more Twitter traffic to your site? Want to engage your social products from all angles? No problem.  Image Source: G2Crowd2crowd Simply put, Socedo takes the question marks out of your social media ROI without sinking all of your time. 3. Make the most of your meetings: Meetings are a crucial component of tracking your progress, but they can quickly eat away at your schedule if you aren’t careful. This is true when coordinating internally and with customers alike. Think about it. How many times in the past month has a 20-minute call or check-in devolved into an hour-long back-and-forth session that didn’t yield any actionable takeaways? Much like your SMART goals, meetings demand defined purposes. Saying that you’re simply going to talk about a campaign isn’t going to cut it. Instead, you’re going to need to be specific, to-the-point and prepared prior to a meeting. Some quick tips for making your meetings more effective. … Define your talking points and schedule in a written agenda. This helps cut down on tangents and moves the discussion from Point A to Point B, lickety-split. Conclude with action items. Everyone involved needs a takeaway task from any given meeting, something to hold them accountable. Follow up on action items. Much like you review performance metrics, revisiting the results of a previous meeting is what’s going to make your discussions “stick. ”Meetings are absolutely necessary, but businesses today shouldn’t use them as an excuse to waste time. It’s all about finding a balance between saying what needs to be said and taking action. 4. Eliminate wasteful spending ASAP: Marketing is becoming more and more of a pay-to-play game. Facebook’s algorithm changes are just the tip of the iceberg. As a result, businesses need to be especially careful with their budgets. This means choosing the right platforms, outsourcing roles where appropriate and making the most of the impactful free platforms that you still have available. That said, how do you ensure that the money that you do decide to spend doesn’t go to waste? If your goal is to become a more frugal marketer in the future, you’re on the right track. Fortunately, there are plenty of proven strategies to boost your business that don’t require a massive financial investment. A shining example is thought leader Andy Crestodina’s zero-waste principle of collaborating on content with influencers who will be happy to share your content when it goes live. Don’t neglect the power of relationships when it comes to moving the needle with marketing. Of course, there are tools out there to help you manage your marketing spend, too. Services like Siftery can effectively break down how much you’re spending on any given subscription marketing platform and suggest cost-friendly alternatives to keep your budget in check.  Image Source: Siftery Also, consider strategies such as using a Facebook Pixel to effectively track each ad campaign’s visitors and leads as they interact with your brand. Rather than constantly chase new business, this type of measurement allows you to make sure your campaigns are optimized for sales – and to make the most of those who are already familiar with you by using retargeting and lookalikes. These sorts of solutions enable a marketing strategy that’s both cost-effective and focused on accountability. 5. Learn to avoid distractions and find your flow: Finally, don’t fall into the trap of being controlled by tech as you ramp up your business’s marketing campaigns. Think about. Emails. Phone calls. Texts. Notifications. They pour in day after day, don’t they? While you may see them as business as usual, these sorts of pulses can be incredibly distracting. Rather than live “in the moment” of distractions, marketers must focus on long-term goals. Cutting down on distractions means establishing dedicated timetables to take care of tasks, versus spending all day “putting out fires,” by checking emails, project management tool notifications or social updates.  Image Source: Business Insider Whether it’s a Chrome extension to block distracting sites or a built-in Pomodoro timer for your desktop, consider investing in some time management apps to maximize your productivity. You can even create your own version of the Eisenhower Decision Matrix to break down what you want done versus what needs to be done. The takeaway here? In order to avoid time-wasting, you must prioritize your tasks and find your flow ASAP. Conclusion: Truly efficient marketing comes from a thought-out combination of tactics and tools. Make every minute and dollar count. Focus on objectives and priorities instead of “what-if”s. Scale what works and drop what doesn’t. This five-step strategy is possible for businesses of all shapes and sizes. Don’t assume that a well-oiled marketing team is beyond your reach – these principles can be your pillars, even as the marketing landscape continues to change. Guest author:&nbsp;Vikas Agrawal&nbsp;is a start-up Investor &amp; co-founder of the&nbsp;Infographic design agency&nbsp;Infobrandz that offers creative and premium visual content solutions to medium to large companies. Content created by Infobrandz are loved, shared &amp; can be found all over the internet on high authority platforms like HuffingtonPost, Businessinsider, Forbes , Tech. co &amp; EliteDaily. &nbsp; &lt;/div&gt; "
    }, {
    "id": 53,
    "url": "https://bright-softwares.com/blog/fr/javascript/understanding-template-literals-in-javascript",
    "title": "Understanding Template Literals in JavaScript",
    "body": "2020/07/05 - The author selected the COVID-19 Relief Fund to receive a donation as part of the Write for DOnations program. Introduction: The 2015 edition of the ECMAScript specification (ES6) added template literals to the JavaScript language. Template literals are a new form of making strings in JavaScript that add a lot of powerful new capabilities, such as creating multi-line strings more easily and using placeholders to embed expressions in a string. In addition, an advanced feature called tagged template literals allows you to perform operations on the expressions within a string. All of these capabilities increase your options for string manipulation as a developer, letting you generate dynamic strings that could be used for URLs or functions that customize HTML elements. In this article, you will go over the differences between single/double-quoted strings and template literals, running through the various ways to declare strings of different shape, including multi-line strings and dynamic strings that change depending on the value of a variable or expression. You will then learn about tagged templates and see some real-world examples of projects using them. Declaring Strings: This section will review how to declare strings with single quotes and double quotes, and will then show you how to do the same with template literals. In JavaScript, a string can be written with single quotes (' '): const single = 'Every day is a good day when you paint. 'A string can also be written with double quotes (   ): const double =  Be so very light. Be a gentle whisper.  There is no major difference in JavaScript between single- or double-quoted strings, unlike other languages that might allow interpolation in one type of string but not the other. In this context, interpolation refers to the evaluation of a placeholder as a dynamic part of a string. The use of single- or double-quoted strings mostly comes down to personal preference and convention, but used in conjunction, each type of string only needs to escape its own type of quote: // Escaping a single quote in a single-quoted stringconst single = ' We don\'t make mistakes. We just have happy accidents.   - Bob Ross'// Escaping a double quote in a double-quoted stringconst double =  \ We don't make mistakes. We just have happy accidents. \  - Bob Ross console. log(single);console. log(double);The result of the log() method here will print the same two strings to the console: Output We don't make mistakes. We just have happy accidents.   - Bob Ross We don't make mistakes. We just have happy accidents.   - Bob RossTemplate literals, on the other hand, are written by surrounding the string with the backtick character, or grave accent (`): const template = `Find freedom on this canvas. `They do not need to escape single or double quotes: const template = ` We don't make mistakes. We just have happy accidents.   - Bob Ross`However, they do still need to escape backticks: const template = `Template literals use the \` character. `Template literals can do everything that regular strings can, so you could possibly replace all strings in your project with them and have the same functionality. However, the most common convention in codebases is to only use template literals when using the additional capabilities of template literals, and consistently using the single or double quotes for all other simple strings. Following this standard will make your code easier to read if examined by another developer. Now that you&rsquo;ve seen how to declare strings with single quotes, double quotes, and backticks, you can move on to the first advantage of template literals: writing multi-line strings. Multi-line Strings: In this section, you will first run through the way strings with multiple lines were declared before ES6, then see how template literals make this easier. Originally, if you wanted to write a string that spans multiple lines in your text editor, you would use the concatenation operator. However, this was not always a straight-forward process. The following string concatenation seemed to run over multiple lines: const address =  'Homer J. Simpson' +  '742 Evergreen Terrace' +  'Springfield'This might allow you to break up the string into smaller lines and include it over multiple lines in the text editor, but it has no effect on the output of the string. In this case, the strings will all be on one line and not separated by newlines or spaces. If you logged address to the console, you would get the following: OutputHomer J. Simpson742 Evergreen TerraceSpringfieldYou can use the backslash character (\) to continue the string onto multiple lines: const address = 'Homer J. Simpson\ 742 Evergreen Terrace\ Springfield'This will retain any indentation as whitespace, but the string will still be on one line in the output: OutputHomer J. Simpson 742 Evergreen Terrace SpringfieldUsing the newline character (\n), you can create a true multi-line string: const address =  'Homer J. Simpson\n' +  '742 Evergreen Terrace\n' +  'Springfield'When logged to the console, this will display the following: OutputHomer J. Simpson742 Evergreen TerraceSpringfieldUsing newline characters to designate multi-line strings can be counterintuitive, however. In contrast, creating a multi-line string with template literals can be much more straight-forward. There is no need to concatenate, use newline characters, or use backslashes. Just pressing ENTER and writing the string across multiple lines works by default: const address = `Homer J. Simpson742 Evergreen TerraceSpringfield`The output of logging this to the console is the same as the input: OutputHomer J. Simpson742 Evergreen TerraceSpringfieldAny indentation will be preserved, so it&rsquo;s important not to indent any additional lines in the string if that is not desired. For example, consider the following: const address = `Homer J. Simpson         742 Evergreen Terrace         Springfield`Although this style of writing the line might make the code more human readable, the output will not be: OutputHomer J. Simpson         742 Evergreen Terrace         SpringfieldWith multi-line strings now covered, the next section will deal with how expressions are interpolated into their values with the different string declarations. Expression Interpolation: In strings before ES6, concatenation was used to create a dynamic string with variables or expressions: const method = 'concatenation'const dynamicString = 'This string is using ' + method + '. 'When logged to the console, this will yield the following: OutputThis string is using concatenation. With template literals, an expression can be embedded in a placeholder. A placeholder is represented by ${}, with anything within the curly brackets treated as JavaScript and anything outside the brackets treated as a string: const method = 'interpolation'const dynamicString = `This string is using ${method}. `When dynamicString is logged to the console, the console will show the following: OutputThis string is using interpolation. One common example of embedding values in a string might be for creating dynamic URLs. With concatenation, this can be cumbersome. For example, the following declares a function to generate an OAuth access string: function createOAuthString(host, clientId, scope) { return host + '/login/oauth/authorize?client_id=' + clientId + '&amp;scope=' + scope}createOAuthString('https://github. com', 'abc123', 'repo,user')Logging this function will yield the following URL to the console: Outputhttps://github. com/login/oauth/authorize?client_id=abc123&amp;scope=repo,userUsing string interpolation, you no longer have to keep track of opening and closing strings and concatenation operator placement. Here is the same example with template literals: function createOAuthString(host, clientId, scope) { return `${host}/login/oauth/authorize?client_id=${clientId}&amp;scope=${scope}`}createOAuthString('https://github. com', 'abc123', 'repo,user')This will have the same output as the concatenation example: Outputhttps://github. com/login/oauth/authorize?client_id=abc123&amp;scope=repo,userYou can also use the trim() method on a template literal to remove any whitespace at the beginning or end of the string. For example, the following uses an arrow function to create an HTML &lt;li&gt; element with a customized link: const menuItem = (url, link) =&gt; `&lt;li&gt; &lt;a href= ${url} &gt;${link}&lt;/a&gt;&lt;/li&gt;`. trim()menuItem('https://google. com', 'Google')The result will be trimmed of all the whitespace, ensuring that the element will be rendered correctly: Output&lt;li&gt; &lt;a href= https://google. com &gt;Google&lt;/a&gt;&lt;/li&gt;Entire expressions can be interpolated, not just variables, such as in this example of the sum of two numbers: const sum = (x, y) =&gt; x + yconst x = 5const y = 100const string = `The sum of ${x} and ${y} is ${sum(x, y)}. `console. log(string)This code defines the sum function and the variables x and y, then uses both the function and the variables in a string. The logged result will show the following: OutputThe sum of 5 and 100 is 105. This can be particularly useful with ternary operators, which allow conditionals within a string: const age = 19const message = `You can ${age &lt; 21 ? 'not' : ''} view this page`console. log(message)The logged message here will change depnding on whether the value of age is over or under 21. Since it is 19 in this example, the following output will be logged: OutputYou can not view this pageNow you have an idea of how template literals can be useful when used to interpolate expressions. The next section will take this a step further by examining tagged template literals to work with the expressions passed into placeholders. Tagged Template Literals: An advanced feature of template literals is the use of tagged template literals, sometimes referred to as template tags. A tagged template starts with a tag function that parses a template literal, allowing you more control over manipulating and returning a dynamic string. In this example, you&rsquo;ll create a tag function to use as the function operating on a tagged template. The string literals are the first parameter of the function, named strings here, and any expressions interpolated into the string are packed into the second parameter using rest parameters. You can console out the parameter to see what they will contain: function tag(strings, . . . expressions) { console. log(strings) console. log(expressions)}Use the tag function as the tagged template function and parse the string as follows: const string = tag`This is a string with ${true} and ${false} and ${100} interpolated inside. `Since you&rsquo;re console logging strings and expressions, this will be the output: Output(4) [ This is a string with  ,   and  ,   and  ,   interpolated inside.  (3) [true, false, 100]The first parameter, strings, is an array containing all the string literals:  This is a string with    and    and    interpolated inside.  There is also a raw property available on this argument at strings. raw, which contains the strings without any escape sequences being processed. For example, /n would just be the character /n and not be escaped into a newline. The second parameter, . . . expressions, is a rest parameter array consisting of all the expressions: truefalse100The string literals and expressions are passed as parameters to the tagged template function tag. Note that the tagged template does not need to return a string; it can operate on those values and return any type of value. For example, we can have the function ignore everything and return null, as in this returnsNull function: function returnsNull(strings, . . . expressions) { return null}const string = returnsNull`Does this work?`console. log(string)Logging the string variable will return: OutputnullAn example of an action that can be performed in tagged templates is applying some change to both sides of each expression, such as if you wanted to wrap each expression in an HTML tag. Create a bold function that will add &lt;strong&gt; and &lt;/strong&gt; to each expression: function bold(strings, . . . expressions) { let finalString = ''// Loop through all expressionsexpressions. forEach((value, i) =&gt; {finalString += `${strings[i]}&lt;strong&gt;${value}&lt;/strong&gt;`})// Add the last string literalfinalString += strings[strings. length - 1]return finalString}const string = bold`This is a string with ${true} and ${false} and ${100} interpolated inside. `console. log(string)This code uses the forEach method to loop over the expressions array and add the bolding element: OutputThis is a string with &lt;strong&gt;true&lt;/strong&gt; and &lt;strong&gt;false&lt;/strong&gt; and &lt;strong&gt;100&lt;/strong&gt; interpolated inside. There are a few examples of tagged template literals in popular JavaScript libraries. The graphql-tag library uses the gql tagged template to parse GraphQL query strings into the abstract syntax tree (AST) that GraphQL understands: import gql from 'graphql-tag'// A query to retrieve the first and last name from user 5const query = gql` { user(id: 5) { firstName lastName } }`Another library that uses tagged template functions is styled-components, which allows you to create new React components from regular DOM elements and apply additional CSS styles to them: import styled from 'styled-components'const Button = styled. button` color: magenta;`// &lt;Button&gt; can now be used as a custom componentYou can also use the built-in String. raw method on tagged template literals to prevent any escape sequences from being processed: const rawString = String. raw`I want to write /n without it being escaped. `console. log(rawString)This will log the following: OutputI want to write /n without it being escaped. Conclusion: In this article, you reviewed single- and double-quoted string literals and you learned about template literals and tagged template literals. Template literals make a lot of common string tasks simpler by interpolating expressions in strings and creating multi-line strings without any concatenation or escaping. Template tags are also a useful advanced feature of template literals that many popular libraries have used, such as GraphQL and styled-components. To learn more about strings in JavaScript, read How To Work with Strings in JavaScript and How To Index, Split, and Manipulate Strings in JavaScript. "
    }, {
    "id": 54,
    "url": "https://bright-softwares.com/blog/fr/database/how-to-use-onetomany-database-relationships-with-flask-and-sqlite",
    "title": "How To Use One-to-Many Database Relationships with Flask and SQLite",
    "body": "2020/07/05 - The author selected the COVID-19 Relief Fund to receive a donation as part of the Write for DOnations program. Introduction: Flask is a framework for building web applications using the Python language, and SQLite is a database engine that can be used with Python to store application data. In this tutorial, you will use Flask with SQLite to create a to-do application where users can create lists of to-do items. You will learn how to use SQLite with Flask and how one-to-many database relationships work. A one-to-many database relationship is a relationship between two database tables where a record in one table can reference several records in another table. For example, in a blogging application, a table for storing posts can have a one-to-many relationship with a table for storing comments. Each post can reference many comments, and each comment references a single post; therefore, one post has a relationship with many comments. The post table is a parent table, while the comments table is a child table—a record in the parent table can reference many records in the child table. This is important to be able to have access to related data in each table. We&rsquo;ll use SQLite because it is portable and does not need any additional set up to work with Python. It is also great for prototyping an application before moving to a larger database such as MySQL or Postgres. For more on how to choose the right database system read our SQLite vs MySQL vs PostgreSQL: A Comparison Of Relational Database Management Systems article. Prerequisites: Before you start following this guide, you will need: A local Python 3 programming environment, follow the tutorial for your distribution in How To Install and Set Up a Local Programming Environment for Python 3 series for your local machine. In this tutorial we’ll call our project directory flask_todo. An understanding of basic Flask concepts such as creating routes, rendering HTML templates, and connecting to an SQLite database. You can follow How To Make a Web Application Using Flask in Python 3, if you are not familiar with these concepts, but it&rsquo;s not necessary. Step 1 — Creating the Database: In this step, you will activate your programming environment, install Flask, create the SQLite database, and populate it with sample data. You&rsquo;ll learn how to use foreign keys to create a one-to-many relationship between lists and items. A foreign key is a key used to associate a database table with another table, it is the link between the child table and its parent table. If you haven’t already activated your programming environment, make sure you’re in your project directory (flask_todo) and use this command to activate it: source env/bin/activateOnce your programming environment is activated, install Flask using the following command: pip install flaskOnce the installation is complete, you can now create the database schema file that contains SQL commands to create the tables you need to store your to-do data. You will need two tables: a table called lists to store to-do lists, and an items table to store the items of each list. Open a file called schema. sql inside your flask_todo directory: nano schema. sqlType the following SQL commands inside this file: flask_todo/schema. sqlDROP TABLE IF EXISTS lists;DROP TABLE IF EXISTS items;CREATE TABLE lists (id INTEGER PRIMARY KEY AUTOINCREMENT,created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,title TEXT NOT NULL);CREATE TABLE items (id INTEGER PRIMARY KEY AUTOINCREMENT,list_id INTEGER NOT NULL,created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,content TEXT NOT NULL,FOREIGN KEY (list_id) REFERENCES lists (id));Save and close the file. The first two SQL command are DROP TABLE IF EXISTS lists; and DROP TABLE IF EXISTS items;, these delete any already existing tables named lists and items so you don&rsquo;t see confusing behavior. Note that this will delete all of the content you have in the database whenever you use these SQL commands, so ensure you don&rsquo;t write any important content in the web application until you finish this tutorial and experiment with the final result. Next, you use CREATE TABLE lists to create the lists table that will store the to-do lists (such as a study list, work list, home list, and so on) with the following columns: id: An integer that represents a primary key, this will get assigned a unique value by the database for each entry (i. e. to-do list). created: The time the to-do list was created at. NOT NULL signifies that this column should not be empty and the DEFAULT value is the CURRENT_TIMESTAMP value, which is the time at which the list was added to the database. Just like id, you don&rsquo;t need to specify a value for this column, as it will be automatically filled in. title: The list title. Then, you create a table called items to store to-do items. This table has an ID, a list_id integer column to identify which list an item belongs to, a creation date, and the item&rsquo;s content. To link an item to a list in the database you use a foreign key constraint with the line FOREIGN KEY (list_id) REFERENCES lists (id). Here the lists table is a parent table, which is the table that is being referenced by the foreign key constraint, this indicates a list can have multiple items. The items table is a child table, which is the table the constraint applies to. This means items belong to a single list. The list_id column references the id column of the lists parent table. Since a list can have many items, and an item belongs to only one list, the relationship between the lists and items tables is a one-to-many relationship. Next, you will use the schema. sql file to create the database. Open a file named init_db. py inside the flask_todo directory: nano init_db. pyThen add the following code: flask_todo/init_db. pyimport sqlite3connection = sqlite3. connect('database. db')with open('schema. sql') as f:connection. executescript(f. read())cur = connection. cursor()cur. execute( INSERT INTO lists (title) VALUES (?) , ('Work',))cur. execute( INSERT INTO lists (title) VALUES (?) , ('Home',))cur. execute( INSERT INTO lists (title) VALUES (?) , ('Study',))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(1, 'Morning meeting'))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(2, 'Buy fruit'))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(2, 'Cook dinner'))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(3, 'Learn Flask'))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(3, 'Learn SQLite'))connection. commit()connection. close()Save and close the file. Here you connect to a file called database. db that will be created once you execute this program. You then open the schema. sql file and run it using the executescript() method that executes multiple SQL statements at once. Running schema. sql will create the lists and items tables. Next, using a Cursor object, you execute a few INSERT SQL statements to create three lists and five to-do items. You use the list_id column to link each item to a list via the list&rsquo;s id value. For example, the Work list was the first insertion into the database, so it will have the ID 1. This is how you can link the Morning meeting to-do item to Work—the same rule applies to the other lists and items. Finally, you commit the changes and close the connection. Run the program: python init_db. pyAfter execution, a new file called database. db will appear in your flask_todo directory. You&rsquo;ve activated your environment, installed Flask, and created the SQLite database. Next, you&rsquo;ll retrieve the lists and items from the database and display them in the application&rsquo;s homepage. Step 2 — Displaying To-do Items: In this step, you will connect the database you created in the previous step to a Flask application that displays the to-do lists and the items of each list. You will learn how to use SQLite joins to query data from two tables and how to group to-do items by their lists. First, you will create the application file. Open a file named app. py inside the flask_todo directory: nano app. pyAnd then add the following code to the file: flask_todo/app. pyfrom itertools import groupbyimport sqlite3from flask import Flask, render_template, request, flash, redirect, url_fordef get_db_connection():conn = sqlite3. connect('database. db')conn. row_factory = sqlite3. Rowreturn connapp = Flask(**name**)app. config['SECRET_KEY'] = 'this should be a secret random string'@app. route('/')def index():conn = get_db_connection()todos = conn. execute('SELECT i. content, l. title FROM items i JOIN lists l \ ON i. list_id = l. id ORDER BY l. title;'). fetchall()  lists = {}  for k, g in groupby(todos, key=lambda t: t['title']):    lists[k] = list(g)  conn. close()  return render_template('index. html', lists=lists)Save and close the file. The get_db_connection() function opens a connection to the database. db database file and then sets the row_factory attribute to sqlite3. Row. In this way you can have name-based access to columns; this means that the database connection will return rows that behave like regular Python dictionaries. Lastly, the function returns the conn connection object you&rsquo;ll be using to access the database. In the index() view function, you open a database connection and execute the following SQL query: SELECT i. content, l. title FROM items i JOIN lists l ON i. list_id = l. id ORDER BY l. title;You then retrieve its results by using the fetchall() method and save the data in a variable called todos. In this query, you use SELECT to get the content of the item and the title of the list it belongs to by joining both the items and lists tables (with the table aliases i for items and l for lists). With the join condition i. list_id = l. id after the ON keyword, you will get each row from the items table with every row from the lists table where the list_id column of the items table matches the id of the lists table. You then use ORDER BY to order the results by list titles. To understand this query better, open the Python REPL in your flask_todo directory: pythonTo understand the SQL query, examine the contents of the todos variable by running this small program: from app import get_db_connectionconn = get_db_connection()todos = conn. execute('SELECT i. content, l. title FROM items i JOIN lists l \ON i. list_id = l. id ORDER BY l. title;'). fetchall()for todo in todos:  print(todo['title'], ':', todo['content'])You first import the get_db_connection from the app. py file then open a connection and execute the query (note that this is the same SQL query you have in your app. py file). In the for loop you print the title of the list and the content of each to-do item. The output will be as follows: OutputHome : Buy fruitHome : Cook dinnerStudy : Learn FlaskStudy : Learn SQLiteWork : Morning meetingClose the REPL using CTRL + D. Now that you understand how SQL joins work and what the query achieves, let&rsquo;s return back to the index() view function in your app. py file. After declaring the todos variable, you group the results using the following code: lists = {}for k, g in groupby(todos, key=lambda t: t['title']):lists[k] = list(g)You first declare an empty dictionary called lists, then use a for loop to go through a grouping of the results in the todos variable by the list&rsquo;s title. You use the groupby() function you imported from the itertools standard library. This function will go through each item in the todos variable and generate a group of results for each key in the for loop. k represents list titles (that is, Home, Study, Work), which are extracted using the function you pass to the key parameter of the groupby() function. In this case the function is lambda t: t['title'] that takes a to-do item and returns the title of the list (as you have done before with todo['title'] in the previous for loop). g represents the group that contains the to-do items of each list title. For example, in the first iteration, k will be 'Home', while g is an iterable that will contain the items 'Buy fruit' and 'Cook dinner'. This gives us a representation of the one-to-many relationship between lists and items, where each list title has several to-do items. When running the app. py file, and after the for loop finishes execution, lists will be as follows: Output{'Home': [&lt;sqlite3. Row object at 0x7f9f58460950&gt;,     &lt;sqlite3. Row object at 0x7f9f58460c30&gt;], 'Study': [&lt;sqlite3. Row object at 0x7f9f58460b70&gt;,      &lt;sqlite3. Row object at 0x7f9f58460b50&gt;], 'Work': [&lt;sqlite3. Row object at 0x7f9f58460890&gt;]}Each sqlite3. Row object will contain the data you retrieved from the items table using the SQL query in the index() function. To represent this data better, let&rsquo;s make a program that goes through the lists dictionary and displays each list and its items. Open a file called list_example. py in your flask_todo directory: nano list_example. pyThen add the following code: flask_todo/list_example. pyfrom itertools import groupbyfrom app import get_db_connectionconn = get_db_connection()todos = conn. execute('SELECT i. content, l. title FROM items i JOIN lists l \ ON i. list_id = l. id ORDER BY l. title;'). fetchall()lists = {}for k, g in groupby(todos, key=lambda t: t['title']):lists[k] = list(g)for list*, items in lists. items():print(list*)for item in items:print(' ', item['content'])Save and close the file. This is very similar to the content in your index() view function. The last for loop here illustrates how the lists dictionary is structured. You first go through the dictionary&rsquo;s items, print the list title (which is in the list_ variable), then go through each group of to-do items that belong to the list and print the content value of the item. Run the list_example. py program: python list_example. pyHere is the output of list_example. py: OutputHome   Buy fruit   Cook dinnerStudy   Learn Flask   Learn SQLiteWork   Morning meetingNow that you understand each part of the index() function, let&rsquo;s create a base template and create the index. html file you rendered using the line return render_template('index. html', lists=lists). In your flask_todo directory, create a templates directory and open a file called base. html inside it: mkdir templatesnano templates/base. htmlAdd the following code inside base. html, note that you&rsquo;re using Bootstrap here. If you are not familiar with HTML templates in Flask, see Step 3 of How To Make a Web Application Using Flask in Python 3: flask_todo/templates/base. html&lt;!doctype html&gt;&lt;html lang= en &gt; &lt;head&gt;  &lt;!-- Required meta tags --&gt;  &lt;meta charset= utf-8 &gt;  &lt;meta name= viewport  content= width=device-width, initial-scale=1, shrink-to-fit=no &gt;  &lt;!-- Bootstrap CSS --&gt;  &lt;link rel= stylesheet  href= https://stackpath. bootstrapcdn. com/bootstrap/4. 3. 1/css/bootstrap. min. css  integrity= sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T  crossorigin= anonymous &gt;  &lt;title&gt;{ block title } { endblock }&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;nav class= navbar navbar-expand-md navbar-light bg-light &gt;&lt;a class= navbar-brand  href=  &gt;FlaskTodo&lt;/a&gt;&lt;button class= navbar-toggler  type= button  data-toggle= collapse  data-target= #navbarNav  aria-controls= navbarNav  aria-expanded= false  aria-label= Toggle navigation &gt;&lt;span class= navbar-toggler-icon &gt;&lt;/span&gt;&lt;/button&gt;&lt;div class= collapse navbar-collapse  id= navbarNav &gt;&lt;ul class= navbar-nav &gt;&lt;li class= nav-item active &gt;&lt;a class= nav-link  href= # &gt;About&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/nav&gt;&lt;div class= container &gt;{ block content } { endblock }&lt;/div&gt;  &lt;!-- Optional JavaScript --&gt;  &lt;!-- jQuery first, then Popper. js, then Bootstrap JS --&gt;  &lt;script src= https://code. jquery. com/jquery-3. 3. 1. slim. min. js  integrity= sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo  crossorigin= anonymous &gt;&lt;/script&gt;  &lt;script src= https://cdnjs. cloudflare. com/ajax/libs/popper. js/1. 14. 7/umd/popper. min. js  integrity= sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1  crossorigin= anonymous &gt;&lt;/script&gt;  &lt;script src= https://stackpath. bootstrapcdn. com/bootstrap/4. 3. 1/js/bootstrap. min. js  integrity= sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM  crossorigin= anonymous &gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;Save and close the file. Most of the code in the preceding block is standard HTML and code required for Bootstrap. The &lt;meta&gt; tags provide information for the web browser, the &lt;link&gt; tag links the Bootstrap CSS files, and the &lt;script&gt; tags are links to JavaScript code that allows some additional Bootstrap features. Check out the Bootstrap documentation for more information. Next, create the index. html file that will extend this base. html file: nano templates/index. htmlAdd the following code to index. html: flask_todo/templates/index. html{ extends 'base. html' }{ block content }&lt;h1&gt;{ block title } Welcome to FlaskTodo { endblock }&lt;/h1&gt;{ for list, items in lists. items() }&lt;div class= card  style= width: 18rem; margin-bottom: 50px; &gt;&lt;div class= card-header &gt;&lt;h3&gt;&lt;/h3&gt;&lt;/div&gt;&lt;ul class= list-group list-group-flush &gt;{ for item in items }&lt;li class= list-group-item &gt;&lt;/li&gt;{ endfor }&lt;/ul&gt;&lt;/div&gt;{ endfor }{ endblock }Here you use a for loop to go through each item of the lists dictionary, you display the list title as a card header inside an &lt;h3&gt; tag, and then use a list group to display each to-do item that belongs to the list in an &lt;li&gt; tag. This follows the same rules explained in the list_example. py program. You will now set the environment variables Flask needs and run the application using the following commands: export FLASK_APP=appexport FLASK_ENV=developmentflask runOnce the development server is running, you can visit the URL http://127. 0. 0. 1:5000/ in your browser. You will see a web page with the &ldquo;Welcome to FlaskTodo&rdquo; and your list items.  You can now type CTRL + C to stop your development server. You&rsquo;ve created a Flask application that displays the to-do lists and the items of each list. In the next step, you will add a new page for creating new to-do items. Step 3 — Adding New To-do Items: In this step, you will make a new route for creating to-do items, you will insert data into database tables, and associate items with the lists they belong to. First, open the app. py file: nano app. pyThen, add a new /create route with a view function called create() at the end of the file: flask_todo/app. py. . . @app. route('/create/', methods=('GET', 'POST'))def create():  conn = get_db_connection()  lists = conn. execute('SELECT title FROM lists;'). fetchall()  conn. close()  return render_template('create. html', lists=lists)Save and close the file. Because you will use this route to insert new data to the database via a web form, you allow both GET and POST requests using methods=('GET', 'POST') in the app. route() decorator. In the create() view function, you open a database connection then get all the list titles available in the database, close the connection, and render a create. html template passing it the list titles. Next, open a new template file called create. html: nano templates/create. htmlAdd the following HTML code to create. html: flask_todo/templates/create. html{ extends 'base. html' }{ block content }&lt;h1&gt;{ block title } Create a New Item { endblock }&lt;/h1&gt;&lt;form method= post &gt;&lt;div class= form-group &gt;&lt;label for= content &gt;Content&lt;/label&gt;&lt;input type= text  name= content placeholder= Todo content  class= form-control value=  &gt;&lt;/input&gt;&lt;/div&gt;  &lt;div class= form-group &gt;    &lt;label for= list &gt;List&lt;/label&gt;    &lt;select class= form-control  name= list &gt;      { for list in lists }        { if list['title'] == request. form['list'] }          &lt;option value=   selected&gt;                      &lt;/option&gt;        { else }          &lt;option value=  &gt;                      &lt;/option&gt;        { endif }      { endfor }    &lt;/select&gt;  &lt;/div&gt;  &lt;div class= form-group &gt;    &lt;button type= submit  class= btn btn-primary &gt;Submit&lt;/button&gt;  &lt;/div&gt;&lt;/form&gt;{ endblock }Save and close the file. You use request. form to access the form data that is stored in case something goes wrong with your form submission (for example, if no to-do content was provided). In the &lt;select&gt; element, you loop through the lists you retrieved from the database in the create() function. If the list title is equal to what is stored in request. form then the selected option is that list title, otherwise, you display the list title in a normal non-selected &lt;option&gt; tag. Now, in the terminal, run your Flask application: flask runThen visit http://127. 0. 0. 1:5000/create in your browser, you will see a form for creating a new to-do item, note that the form doesn&rsquo;t work yet because you have no code to handle POST requests that get sent by the browser when submitting the form. Type CTRL + C to stop your development server. Next, let&rsquo;s add the code for handling POST requests to the create() function and make the form function properly, open app. py: nano app. pyThen edit the create() function to look like so: flask_todo/app. py. . . @app. route('/create/', methods=('GET', 'POST'))def create():  conn = get_db_connection()  if request. method == 'POST':    content = request. form['content']    list_title = request. form['list']    if not content:      flash('Content is required!')      return redirect()    list_id = conn. execute('SELECT id FROM lists WHERE title = (?);',                 (list_title,)). fetchone()['id']    conn. execute('INSERT INTO items (content, list_id) VALUES (?, ?)',           (content, list_id))    conn. commit()    conn. close()    return redirect()  lists = conn. execute('SELECT title FROM lists;'). fetchall()  conn. close()  return render_template('create. html', lists=lists)Save and close the file. Inside the request. method == 'POST' condition you get the to-do item&rsquo;s content and the list&rsquo;s title from the form data. If no content was submitted, you send the user a message using the flash() function and redirect to the index page. If this condition was not triggered, then you execute a SELECT statement to get the list ID from the provided list title and save it in a variable called list_id. You then execute an INSERT INTO statement to insert the new to-do item into the items table. You use the list_id variable to link the item to the list it belongs to. Finally, you commit the transaction, close the connection, and redirect to the index page. As a last step, you will add a link to /create in the navigation bar and display flashed messages below it, to do this, open base. html: nano templates/base. htmlEdit the file by adding a new &lt;li&gt; navigation item that links to the create() view function. Then display the flashed messages using a for loop above the content block. These are available in the get_flashed_messages() Flask function: flask_todo/templates/base. html&lt;nav class= navbar navbar-expand-md navbar-light bg-light &gt;  &lt;a class= navbar-brand  href=  &gt;FlaskTodo&lt;/a&gt;  &lt;button class= navbar-toggler  type= button  data-toggle= collapse  data-target= #navbarNav  aria-controls= navbarNav  aria-expanded= false  aria-label= Toggle navigation &gt;    &lt;span class= navbar-toggler-icon &gt;&lt;/span&gt;  &lt;/button&gt;  &lt;div class= collapse navbar-collapse  id= navbarNav &gt;    &lt;ul class= navbar-nav &gt;    &lt;li class= nav-item active &gt;      &lt;a class= nav-link  href=  &gt;New&lt;/a&gt;    &lt;/li&gt;    &lt;li class= nav-item active &gt;      &lt;a class= nav-link  href= # &gt;About&lt;/a&gt;    &lt;/li&gt;    &lt;/ul&gt;  &lt;/div&gt;&lt;/nav&gt;&lt;div class= container &gt;{ for message in get_flashed_messages() } &lt;div class= alert alert-danger &gt;&lt;/div&gt;{ endfor }{block content } { endblock }&lt;/div&gt;Save and close the file. Now, in the terminal, run your Flask application: flask runA new link to /create will appear in the navigation bar. If you navigate to this page and try to add a new to-do item with no content, you&rsquo;ll receive a flashed message saying Content is required!. If you fill in the content form, a new to-do item will appear on the index page. In this step, you have added the ability to create new to-do items and save them to the database. You can find the source code for this project in this repository. Conclusion: You now have an application to manage to-do lists and items. Each list has several to-do items and each to-do item belongs to a single list in a one-to-many relationship. You learned how to use Flask and SQLite to manage multiple related database tables, how to use foreign keys and how to retrieve and display related data from two tables in a web application using SQLite joins. Furthermore, you grouped results using the groupby() function, inserted new data to the database, and associated database table rows with the tables they are related to. You can learn more about foreign keys and database relationships from the SQLite documentation. You can also read more of our Python Framework content. If you want to check out the sqlite3 Python module, read our tutorial on How To Use the sqlite3 Module in Python 3. "
    }, {
    "id": 55,
    "url": "https://bright-softwares.com/blog/fr/javascript/how-to-manage-state-on-react-class-components",
    "title": "How To Manage State on React Class Components",
    "body": "2020/07/05 - The author selected Creative Commons to receive a donation as part of the Write for DOnations program. Introduction: In React, state refers to a structure that keeps track of how data changes over time in your application. Managing state is a crucial skill in React because it allows you to make interactive components and dynamic web applications. State is used for everything from tracking form inputs to capturing dynamic data from an API. In this tutorial, you&rsquo;ll run through an example of managing state on class-based components. As of the writing of this tutorial, the official React documentation encourages developers to adopt React Hooks to manage state with functional components when writing new code, rather than using class-based components. Although the use of React Hooks is considered a more modern practice, it&rsquo;s important to understand how to manage state on class-based components as well. Learning the concepts behind state management will help you navigate and troubleshoot class-based state management in existing code bases and help you decide when class-based state management is more appropriate. There&rsquo;s also a class-based method called componentDidCatch that is not available in Hooks and will require setting state using class methods. This tutorial will first show you how to set state using a static value, which is useful for cases where the next state does not depend on the first state, such as setting data from an API that overrides old values. Then it will run through how to set a state as the current state, which is useful when the next state depends on the current state, such as toggling a value. To explore these different ways of setting state, you&rsquo;ll create a product page component that you&rsquo;ll update by adding purchases from a list of options. Prerequisites: You will need a development environment running Node. js; this tutorial was tested on Node. js version 10. 20. 1 and npm version 6. 14. 4. To install this on macOS or Ubuntu 18. 04, follow the steps in How to Install Node. js and Create a Local Development Environment on macOS or the Installing Using a PPA section of How To Install Node. js on Ubuntu 18. 04. In this tutorial, you will create apps with Create React App. You can find instructions for installing an application with Create React App at How To Set Up a React Project with Create React App. You will also need a basic knowledge of JavaScript, which you can find in How To Code in JavaScript, along with a basic knowledge of HTML and CSS. A good resource for HTML and CSS is the Mozilla Developer Network. Step 1 — Creating an Empty Project: In this step, you&rsquo;ll create a new project using Create React App. Then you will delete the sample project and related files that are installed when you bootstrap the project. Finally, you will create a simple file structure to organize your components. This will give you a solid basis on which to build this tutorial&rsquo;s sample application for managing state on class-based components. To start, make a new project. In your terminal, run the following script to install a fresh project using create-react-app: npx create-react-app state-class-tutorialAfter the project is finished, change into the directory: cd state-class-tutorialIn a new terminal tab or window, start the project using the Create React App start script. The browser will auto-refresh on changes, so leave this script running while you work: npm startYou will get a running local server. If the project did not open in a browser window, you can open it with http://localhost:3000/. If you are running this from a remote server, the address will be http://your_domain:3000. Your browser will load with a simple React application included as part of Create React App: You will be building a completely new set of custom components, so you&rsquo;ll need to start by clearing out some boilerplate code so that you can have an empty project. To start, open src/App. js in a text editor. This is the root component that is injected into the page. All components will start from here. You can find more information about App. js at How To Set Up a React Project with Create React App. Open src/App. js with the following command: nano src/App. jsYou will see a file like this: state-class-tutorial/src/App. jsimport React from 'react';import logo from '. /logo. svg';import '. /App. css';function App() {return (&lt;div className= App &gt;&lt;header className= App-header &gt;&lt;img src={logo} className= App-logo  alt= logo  /&gt;&lt;p&gt;Edit &lt;code&gt;src/App. js&lt;/code&gt; and save to reload. &lt;/p&gt;&lt;aclassName= App-link href= https://reactjs. org target= \_blank rel= noopener noreferrer &gt;Learn React&lt;/a&gt;&lt;/header&gt;&lt;/div&gt;);}export default App;Delete the line import logo from '. /logo. svg';. Then replace everything in the return statement to return a set of empty tags: &lt;&gt;&lt;/&gt;. This will give you a valid page that returns nothing. The final code will look like this: state-class-tutorial/src/App. jsimport React from 'react';import '. /App. css';function App() {return &lt;&gt;&lt;/&gt;;}export default App;Save and exit the text editor. Finally, delete the logo. You won&rsquo;t be using it in your application and you should remove unused files as you work. It will save you from confusion in the long run. In the terminal window type the following command: rm src/logo. svgIf you look at your browser, you will see a blank screen.  Now that you have cleared out the sample Create React App project, create a simple file structure. This will help you keep your components isolated and independent. Create a directory called components in the src directory. This will hold all of your custom components. mkdir src/componentsEach component will have its own directory to store the component file along with the styles, images, and tests. Create a directory for App: mkdir src/components/AppMove all of the App files into that directory. Use the wildcard, *, to select any files that start with App. regardless of file extension. Then use the mv command to put them into the new directory: mv src/App. * src/components/AppNext, update the relative import path in index. js, which is the root component that bootstraps the whole process: nano src/index. jsThe import statement needs to point to the App. js file in the App directory, so make the following highlighted change: state-class-tutorial/src/index. jsimport React from 'react';import ReactDOM from 'react-dom';import '. /index. css';import App from '. /components/App/App';import * as serviceWorker from '. /serviceWorker';ReactDOM. render(&lt;React. StrictMode&gt;&lt;App /&gt;&lt;/React. StrictMode&gt;,document. getElementById('root'));// If you want your app to work offline and load faster, you can change// unregister() to register() below. Note this comes with some pitfalls. // Learn more about service workers: https://bit. ly/CRA-PWAserviceWorker. unregister();Save and exit the file. Now that the project is set up, you can create your first component. Step 2 — Using State in a Component: In this step, you&rsquo;ll set the initial state of a component on its class and reference the state to display a value. You&rsquo;ll then make a product page with a shopping cart that displays the total items in the cart using the state value. By the end of the step, you&rsquo;ll know the different ways to hold a value and when you should use state rather than a prop or a static value. Building the Components: Start by creating a directory for Product: mkdir src/components/ProductNext, open up Product. js in that directory: nano src/components/Product/Product. jsStart by creating a component with no state. The component will have two parts: The cart, which has the number of items and the total price, and the product, which has a button to add and remove an item. For now, the buttons will have no actions. Add the following code to Product. js: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: 0 total items. &lt;/div&gt;&lt;div&gt;Total: 0&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button&gt;Add&lt;/button&gt; &lt;button&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}You have also included a couple of div elements that have JSX class names so you can add some basic styling. Save and close the file, then open Product. css: nano src/components/Product/Product. cssGive some light styling to increase the font-size for the text and the emoji: state-class-tutorial/src/components/Product/Product. css. product span {  font-size: 100px;}. wrapper {padding: 20px;font-size: 20px;}. wrapper button {font-size: 20px;background: none;}The emoji will need a much larger font size than the text, since it&rsquo;s acting as the product image in this example. In addition, you are removing the default gradient background on buttons by setting the background to none.  Save and close the file. Now, render the Product component in the App component so you can see the results in the browser. Open App. js: nano src/components/App/App. jsImport the component and render it. You can also delete the CSS import since you won&rsquo;t be using it in this tutorial: state-class-tutorial/src/components/App/App. jsimport React from 'react';import Product from '. . /Product/Product';function App() {return &lt;Product /&gt;}export default App;Save and close the file. When you do, the browser will refresh and you&rsquo;ll see the Product component.  Setting the Initial State on a Class Component: There are two values in your component values that are going to change in your display: total number of items and total cost. Instead of hard coding them, in this step you&rsquo;ll move them into an object called state. The state of a React class is a special property that controls the rendering of a page. When you change the state, React knows that the component is out-of-date and will automatically re-render. When a component re-renders, it modifies the rendered output to include the most up-to-date information in state. In this example, the component will re-render whenever you add a product to the cart or remove it from the cart. You can add other properties to a React class, but they won&rsquo;t have the same ability to trigger re-rendering. Open Product. js: nano src/components/Product/Product. jsAdd a property called state to the Product class. Then add two values to the state object: cart and total. The cart will be an array, since it may eventually hold many items. The total will be a number. After assigning these, replace references to the values with this. state. property: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {state = {cart: [],total: 0}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. state. total}&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button&gt;Add&lt;/button&gt; &lt;button&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}Notice that in both cases, since you are referencing JavaScript inside of your JSX, you need to wrap the code in curly braces. You are also displaying the length of the cart array to get a count of the number of items in the array. Save the file. When you do, the browser will refresh and you&rsquo;ll see the same page as before.  The state property is a standard class property, which means that it is accessible in other methods, not just the render method. Next, instead of displaying the price as a static value, convert it to a string using the toLocaleString method, which will convert the number to a string that matches the way numbers are displayed in the browser&rsquo;s region. Create a method called getTotal() that takes the state and converts it to a localized string using an array of currencyOptions. Then, replace the reference to state in the JSX with a method call: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {state = {cart: [],total: 0}currencyOptions = {minimumFractionDigits: 2,maximumFractionDigits: 2,}getTotal = () =&gt; {return this. state. total. toLocaleString(undefined, this. currencyOptions)}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button&gt;Add&lt;/button&gt; &lt;button&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}Since total is a price for goods, you are passing currencyOptions that set the maximum and minimum decimal places for your total to two. Note that this is set as a separate property. Often, beginner React developers will put information like this in the state object, but it is best to only add information to state that you expect to change. This way, the information in state will be easier to keep strack of as your application scales. Another important change you made was to create the getTotal() method by assigning an arrow function to a class property. Without using the arrow function, this method would create a new this binding, which would interfere with the current this binding and introduce a bug into our code. You&rsquo;ll see more on this in the next step. Save the file. When you do, the page will refresh and you&rsquo;ll see the value converted to a decimal.  You&rsquo;ve now added state to a component and referenced it in your class. You also accessed values in the render method and in other class methods. Next, you&rsquo;ll create methods to update the state and show dynamic values.  Step 3 — Setting State from a Static Value: So far you&rsquo;ve created a base state for the component and you&rsquo;ve referenced that state in your functions and your JSX code. In this step, you&rsquo;ll update your product page to modify the state on button clicks. You&rsquo;ll learn how to pass a new object containing updated values to a special method called setState, which will then set the state with the updated data. To update state, React developers use a special method called setState that is inherited from the base Component class. The setState method can take either an object or a function as the first argument. If you have a static value that doesn&rsquo;t need to reference the state, it&rsquo;s best to pass an object containing the new value, since it&rsquo;s easier to read. If you need to reference the current state, you pass a function to avoid any references to out-of-date state. Start by adding an event to the buttons. If your user clicks Add, then the program will add the item to the cart and update the total. If they click Remove, it will reset the cart to an empty array and the total to 0. For example purposes, the program will not allow a user to add an item more then once. Open Product. js: nano src/components/Product/Product. jsInside the component, create a new method called add, then pass the method to the onClick prop for the Add button: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {state = {cart: [],total: 0}add = () =&gt; {this. setState({cart: ['ice cream'],total: 5})}currencyOptions = {minimumFractionDigits: 2,maximumFractionDigits: 2,}getTotal = () =&gt; {return this. state. total. toLocaleString(undefined, this. currencyOptions)}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button onClick={this. add}&gt;Add&lt;/button&gt;    &lt;button&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}Inside the add method, you call the setState method and pass an object containing the updated cart with a single item ice cream and the updated price of 5. Notice that you again used an arrow function to create the add method. As mentioned before, this will ensure the function has the proper this context when running the update. If you add the function as a method without using the arrow function, the setState would not exist without binding the function to the current context. For example, if you created the add function this way: export default class Product extends Component {. . .  add() {  this. setState({   cart: ['ice cream'],   total: 5  }) }. . . }The user would get an error when they click on the Add button.  Using an arrow function ensures that you&rsquo;ll have the proper context to avoid this error. Save the file. When you do, the browser will reload, and when you click on the Add button the cart will update with the current amount.  With the add method, you passed both properties of the state object: cart and total. However, you do not always need to pass a complete object. You only need to pass an object containing the properties that you want to update, and everything else will stay the same. To see how React can handle a smaller object, create a new function called remove. Pass a new object containing just the cart with an empty array, then add the method to the onClick property of the Remove button: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {. . . remove = () =&gt; {this. setState({cart: []})}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button onClick={this. add}&gt;Add&lt;/button&gt;    &lt;button onClick={this. remove}&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}Save the file. When the browser refreshes, click on the Add and Remove buttons. You&rsquo;ll see the cart update, but not the price. The total state value is preserved during the update. This value is only preserved for example purposes; with this application, you would want to update both properties of the state object. But you will often have components with stateful properties that have different responsibilities, and you can make them persist by leaving them out of the updated object. The change in this step was static. You knew exactly what the values would be ahead of time, and they didn&rsquo;t need to be recalculated from state. But if the product page had many products and you wanted to be able to add them multiple times, passing a static object would provide no guarantee of referencing the most up-to-date state, even if your object used a this. state value. In this case, you could instead use a function. In the next step, you&rsquo;ll update state using functions that reference the current state. Step 4 — Setting State Using Current State: There are many times when you&rsquo;ll need to reference a previous state to update a current state, such as updating an array, adding a number, or modifying an object. To be as accurate as possible, you need to reference the most up-to-date state object. Unlike updating state with a predefined value, in this step you&rsquo;ll pass a function to the setState method, which will take the current state as an argument. Using this method, you will update a component&rsquo;s state using the current state. Another benefit of setting state with a function is increased reliability. To improve performance, React may batch setState calls, which means that this. state. value may not be fully reliable. For example, if you update state quickly in several places, it is possible that a value could be out of date. This can happen during data fetches, form validations, or any situation where several actions are occurring in parallel. But using a function with the most up-to-date state as the argument ensures that this bug will not enter your code. To demonstrate this form of state management, add some more items to the product page. First, open the Product. js file: nano src/components/Product/Product. jsNext, create an array of objects for different products. The array will contain the product emoji, name, and price. Then loop over the array to display each product with an Add and Remove button: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';const products = [{emoji: '🍦',name: 'ice cream',price: 5},{emoji: '🍩',name: 'donuts',price: 2. 5,},{emoji: '🍉',name: 'watermelon',price: 4}];export default class Product extends Component {. . . render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;&lt;div&gt;{products. map(product =&gt; (&lt;div key={product. name}&gt;&lt;div className= product &gt;&lt;span role= img  aria-label={product. name}&gt;{product. emoji}&lt;/span&gt;&lt;/div&gt;&lt;button onClick={this. add}&gt;Add&lt;/button&gt;&lt;button onClick={this. remove}&gt;Remove&lt;/button&gt;&lt;/div&gt;))}&lt;/div&gt;&lt;/div&gt;)}}In this code, you are using the map() array method to loop over the products array and return the JSX that will display each element in your browser. Save the file. When the browser reloads, you&rsquo;ll see an updated product list: Now you need to update your methods. First, change the add() method to take the product as an argument. Then instead of passing an object to setState(), pass a function that takes the state as an argument and returns an object that has the cart updated with the new product and the total updated with the new price: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';. . . export default class Product extends Component {state = {cart: [],total: 0}add = (product) =&gt; {this. setState(state =&gt; ({cart: [. . . state. cart, product. name],total: state. total + product. price}))}currencyOptions = {minimumFractionDigits: 2,maximumFractionDigits: 2,}getTotal = () =&gt; {return this. state. total. toLocaleString(undefined, this. currencyOptions)}remove = () =&gt; {this. setState({cart: []})}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;    &lt;div&gt;     {products. map(product =&gt; (      &lt;div key={product. name}&gt;       &lt;div className= product &gt;        &lt;span role= img  aria-label={product. name}&gt;{product. emoji}&lt;/span&gt;       &lt;/div&gt;       &lt;button onClick={() =&gt; this. add(product)}&gt;Add&lt;/button&gt;       &lt;button onClick={this. remove}&gt;Remove&lt;/button&gt;      &lt;/div&gt;     ))}    &lt;/div&gt;   &lt;/div&gt;  )}}Inside the anonymous function that you pass to setState(), make sure you reference the argument—state—and not the component&rsquo;s state—this. state. Otherwise, you still run a risk of getting an out-of-date state object. The state in your function will be otherwise identical. Take care not to directly mutate state. Instead, when adding a new value to the cart, you can add the new product to the state by using the spread syntax on the current value and adding the new value onto the end. Finally, update the call to this. add by changing the onClick() prop to take an anonymous function that calls this. add() with the relevant product. Save the file. When you do, the browser will reload and you&rsquo;ll be able to add multiple products.  Next, update the remove() method. Follow the same steps: convert setState to take a function, update the values without mutating, and update the onChange() prop: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';. . . export default class Product extends Component {. . . remove = (product) =&gt; {this. setState(state =&gt; {const cart = [. . . state. cart];cart. splice(cart. indexOf(product. name))return ({cart,total: state. total - product. price})})}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;&lt;div&gt;{products. map(product =&gt; (&lt;div key={product. name}&gt;&lt;div className= product &gt;&lt;span role= img  aria-label={product. name}&gt;{product. emoji}&lt;/span&gt;&lt;/div&gt;&lt;button onClick={() =&gt; this. add(product)}&gt;Add&lt;/button&gt;&lt;button onClick={() =&gt; this. remove(product)}&gt;Remove&lt;/button&gt;&lt;/div&gt;))}&lt;/div&gt;&lt;/div&gt;)}}To avoid mutating the state object, you must first make a copy of it using the spread operator. Then you can splice out the item you want from the copy and return the copy in the new object. By copying state as the first step, you can be sure that you will not mutate the state object. Save the file. When you do, the browser will refresh and you&rsquo;ll be able to add and remove items: There is still a bug in this application: In the remove method, a user can subtract from the total even if the item is not in the cart. If you click Remove on the ice cream without adding it to your cart, your total will be -5. 00. You can fix the bug by checking for an item&rsquo;s existence before subtracting, but an easier way is to keep your state object small by only keeping references to the products and not separating references to products and total cost. Try to avoid double references to the same data. Instead, store the raw data in state— in this case the whole product object—then perform the calculations outside of the state. Refactor the component so that the add() method adds the whole object, the remove() method removes the whole object, and the getTotal method uses the cart: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';. . . export default class Product extends Component {state = {cart: [],}add = (product) =&gt; {this. setState(state =&gt; ({cart: [. . . state. cart, product],}))}currencyOptions = {minimumFractionDigits: 2,maximumFractionDigits: 2,}getTotal = () =&gt; {const total = this. state. cart. reduce((totalCost, item) =&gt; totalCost + item. price, 0);return total. toLocaleString(undefined, this. currencyOptions)}remove = (product) =&gt; {this. setState(state =&gt; {const cart = [. . . state. cart];const productIndex = cart. findIndex(p =&gt; p. name === product. name);if(productIndex &lt; 0) {return;}cart. splice(productIndex, 1)return ({cart})})}render() {. . . }}The add() method is similar to what it was before, except that reference to the total property has been removed. In the remove() method, you find the index of the product with findByIndex. If the index doesn&rsquo;t exist, you&rsquo;ll get a -1. In that case, you use a conditional statement toreturn nothing. By returning nothing, React will know the state didn&rsquo;t change and won&rsquo;t trigger a re-render. If you return state or an empty object, it will still trigger a re-render. When using the splice() method, you are now passing 1 as the second argument, which will remove one value and keep the rest. Finally, you calculate the total using the reduce() array method. Save the file. When you do, the browser will refresh and you&rsquo;ll have your final cart: The setState function you pass can have an additional argument of the current props, which can be helpful if you have state that needs to reference the current props. You can also pass a callback function to setState as the second argument, regardless of if you pass an object or function for the first argument. This is particularly useful when you are setting state after fetching data from an API and you need to perform a new action after the state update is complete. In this step, you learned how to update a new state based on the current state. You passed a function to the setState function and calculated new values without mutating the current state. You also learned how to exit a setState function if there is no update in a manner that will prevent a re-render, adding a slight performance enhancement. Conclusion: In this tutorial, you have developed a class-based component with a dynamic state that you&rsquo;ve updated statically and using the current state. You now have the tools to make complex projects that respond to users and dynamic information.  React does have a way to manage state with Hooks, but it is helpful to understand how to use state on components if you need to work with components that must be class-based, such as those that use the componentDidCatch method. Managing state is key to nearly all components and is necessary for creating interactive applications. With this knowledge you can recreate many common web components, such as sliders, accordions, forms, and more. You will then use the same concepts as you build applications using hooks or develop components that pull data dynamically from APIs. If you would like to look at more React tutorials, check out our React Topic page, or return to the How To Code in React. js series page. "
    }, {
    "id": 56,
    "url": "https://bright-softwares.com/blog/fr/ubuntu/how-to-create-a-selfsigned-ssl-certificate-for-apache-on-centos-8",
    "title": "How To Create a Self-Signed SSL Certificate for Apache on CentOS 8",
    "body": "2020/07/05 - Introduction: TLS, or &ldquo;transport layer security&rdquo; — and its predecessor SSL — are protocols used to wrap normal traffic in a protected, encrypted wrapper. Using this technology, servers can safely send information to their clients without their messages being intercepted or read by an outside party. In this guide, we will show you how to create and use a self-signed SSL certificate with the Apache web server on a CentOS 8 machine. &lt;p&gt;Note: A self-signed certificate will encrypt communication between your server and its clients. However, because it is not signed by any of the trusted certificate authorities included with web browsers and operating systems, users cannot use the certificate to automatically validate the identity of your server. As a result, your users will see a security error when visiting your site. &lt;/p&gt; Because of this limitation, self-signed certificates are not appropriate for a production environment serving the public. They are typically used for testing, or for securing non-critical services used by a single user or a small group of users that can establish trust in the certificate&rsquo;s validity through alternate communication channels. For a more production-ready certificate solution, check out Let&rsquo;s Encrypt, a free certificate authority. You can learn how to download and configure a Let&rsquo;s Encrypt certificate in our How To Secure Apache with Let&rsquo;s Encrypt on CentOS 8 tutorial. &lt;/span&gt; Prerequisites: Before starting this tutorial, you&rsquo;ll need the following: Access to a CentOS 8 server with a non-root, sudo-enabled user. Our Initial Server Setup with CentOS 8 guide can show you how to create this account. You will also need to have Apache installed. You can install Apache using dnf: sudo dnf install httpdEnable Apache and start it using systemctl: sudo systemctl enable httpdsudo systemctl start httpdAnd finally, if you have a firewalld firewall set up, open up the http and https ports: sudo firewall-cmd --permanent --add-service=httpsudo firewall-cmd --permanent --add-service=httpssudo firewall-cmd --reloadAfter these steps are complete, be sure you are logged in as your non-root user and continue with the tutorial. Step 1 — Installing mod_ssl: We first need to install mod_ssl, an Apache module that provides support for SSL encryption. Install mod_ssl with the dnf command: sudo dnf install mod_sslBecause of a packaging bug, we need to restart Apache once to properly generate the default SSL certificate and key, otherwise we&rsquo;ll get an error reading '/etc/pki/tls/certs/localhost. crt' does not exist or is empty. sudo systemctl restart httpdThe mod_ssl module is now enabled and ready for use. Step 2 — Creating the SSL Certificate: Now that Apache is ready to use encryption, we can move on to generating a new SSL certificate. The certificate will store some basic information about your site, and will be accompanied by a key file that allows the server to securely handle encrypted data. We can create the SSL key and certificate files with the openssl command: sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/pki/tls/private/apache-selfsigned. key -out /etc/pki/tls/certs/apache-selfsigned. crtAfter you enter the command, you will be taken to a prompt where you can enter information about your website. Before we go over that, let&rsquo;s take a look at what is happening in the command we are issuing: openssl: This is the command line tool for creating and managing OpenSSL certificates, keys, and other files. req -x509: This specifies that we want to use X. 509 certificate signing request (CSR) management. X. 509 is a public key infrastructure standard that SSL and TLS adhere to for key and certificate management. -nodes: This tells OpenSSL to skip the option to secure our certificate with a passphrase. We need Apache to be able to read the file, without user intervention, when the server starts up. A passphrase would prevent this from happening, since we would have to enter it after every restart. -days 365: This option sets the length of time that the certificate will be considered valid. We set it for one year here. Many modern browsers will reject any certificates that are valid for longer than one year. -newkey rsa:2048: This specifies that we want to generate a new certificate and a new key at the same time. We did not create the key that is required to sign the certificate in a previous step, so we need to create it along with the certificate. The rsa:2048 portion tells it to make an RSA key that is 2048 bits long. -keyout: This line tells OpenSSL where to place the generated private key file that we are creating. -out: This tells OpenSSL where to place the certificate that we are creating. Fill out the prompts appropriately. The most important line is the one that requests the Common Name. You need to enter either the hostname you&rsquo;ll use to access the server by, or the public IP of the server. It&rsquo;s important that this field matches whatever you&rsquo;ll put into your browser&rsquo;s address bar to access the site, as a mismatch will cause more security errors. The full list of prompts will look something like this: Country Name (2 letter code) [XX]:USState or Province Name (full name) []:ExampleLocality Name (eg, city) [Default City]:Example Organization Name (eg, company) [Default Company Ltd]:Example IncOrganizational Unit Name (eg, section) []:Example DeptCommon Name (eg, your name or your server's hostname) []:your_domain_or_ipEmail Address []:webmaster@example. comBoth of the files you created will be placed in the appropriate subdirectories of the /etc/pki/tls directory. This is a standard directory provided by CentOS for this purpose. Next we will update our Apache configuration to use the new certificate and key. Step 3 — Configuring Apache to Use SSL: Now that we have our self-signed certificate and key available, we need to update our Apache configuration to use them. On CentOS, you can place new Apache configuration files (they must end in . conf) into /etc/httpd/conf. d and they will be loaded the next time the Apache process is reloaded or restarted. For this tutorial we will create a new minimal configuration file. If you already have an Apache &lt;Virtualhost&gt; set up and just need to add SSL to it, you will likely need to copy over the configuration lines that start with SSL, and switch the VirtualHost port from 80 to 443. We will take care of port 80 in the next step. Open a new file in the /etc/httpd/conf. d directory: sudo vi /etc/httpd/conf. d/your_domain_or_ip. confPaste in the following minimal VirtualHost configuration: /etc/httpd/conf. d/your_domain_or_ip. conf&lt;VirtualHost *:443&gt;  ServerName your_domain_or_ip  DocumentRoot /var/www/ssl-test  SSLEngine on  SSLCertificateFile /etc/pki/tls/certs/apache-selfsigned. crt  SSLCertificateKeyFile /etc/pki/tls/private/apache-selfsigned. key&lt;/VirtualHost&gt;Be sure to update the ServerName line to however you intend to address your server. This can be a hostname, full domain name, or an IP address. Make sure whatever you choose matches the Common Name you chose when making the certificate. The remaining lines specify a DocumentRoot directory to serve files from, and the SSL options needed to point Apache to our newly-created certificate and key. Now let&rsquo;s create our DocumentRoot and put an HTML file in it just for testing purposes: sudo mkdir /var/www/ssl-testOpen a new index. html file with your text editor: sudo vi /var/www/ssl-test/index. htmlPaste the following into the blank file: /var/www/ssl-test/index. html&lt;h1&gt;it worked!&lt;/h1&gt;This is not a full HTML file, of course, but browsers are lenient and it will be enough to verify our configuration. Save and close the file, then check your Apache configuration for syntax errors by typing: sudo apachectl configtestYou may see some warnings, but as long as the output ends with Syntax OK, you are safe to continue. If this is not part of your output, check the syntax of your files and try again. When all is well, reload Apache to pick up the configuration changes: sudo systemctl reload httpdNow load your site in a browser, being sure to use https:// at the beginning.  You should see an error. This is normal for a self-signed certificate! The browser is warning you that it can&rsquo;t verify the identity of the server, because our certificate is not signed by any of the browser&rsquo;s known certificate authorities. For testing purposes and personal use this can be fine. You should be able to click through to advanced or more information and choose to proceed. After you do so, your browser will load the it worked! message. &lt;p&gt;Note: if your browser doesn’t connect at all to the server, make sure your connection isn’t being blocked by a firewall. If you are using firewalld, the following commands will open ports 80 and 443:&lt;/p&gt; sudo firewall-cmd --permanent --add-service=httpsudo firewall-cmd --permanent --add-service=httpssudo firewall-cmd --reload &lt;/span&gt; Next we will add another VirtualHost section to our configuration to serve plain HTTP requests and redirect them to HTTPS. Step 4 — Redirecting HTTP to HTTPS: Currently, our configuration will only respond to HTTPS requests on port 443. It is good practice to also respond on port 80, even if you want to force all traffic to be encrypted. Let&rsquo;s set up a VirtualHost to respond to these unencrypted requests and redirect them to HTTPS. Open the same Apache configuration file we started in previous steps: sudo vi /etc/httpd/conf. d/your_domain_or_ip. confAt the bottom, create another VirtualHost block to match requests on port 80. Use the ServerName directive to again match your domain name or IP address. Then, use Redirect to match any requests and send them to the SSL VirtualHost. Make sure to include the trailing slash: /etc/httpd/conf. d/your_domain_or_ip. conf&lt;VirtualHost *:80&gt;  ServerName your_domain_or_ip  Redirect / https://your_domain_or_ip/&lt;/VirtualHost&gt;Save and close this file when you are finished, then test your configuration syntax again, and reload Apache: sudo apachectl configtestsudo systemctl reload httpdYou can test the new redirect functionality by visiting your site with plain http:// in front of the address. You should be redirected to https:// automatically. Conclusion: You have now configured Apache to serve encrypted requests using a self-signed SSL certificate, and to redirect unecrypted HTTP requests to HTTPS. If you are planning on using SSL for a public website, you should look into purchasing a domain name and using a widely supported certificate authority such as Let&rsquo;s Encrypt. For more information on using Let&rsquo;s Encrypt with Apache, please read our How To Secure Apache with Let&rsquo;s Encrypt on CentOS 8 tutorial. "
    }, {
    "id": 57,
    "url": "https://bright-softwares.com/blog/fr/ubuntu/how-to-create-a-new-sudoenabled-user-on-ubuntu-2004-quickstart",
    "title": "How To Create a New Sudo-enabled User on Ubuntu 20.04 [Quickstart]",
    "body": "2020/07/05 - Introduction: When managing a server, you’ll sometimes want to allow users to execute commands as “root,” the administrator-level user. The sudo command provides system administrators with a way to grant administrator privileges — ordinarily only available to the root user — to normal users.  In this tutorial, you’ll learn how to create a new user with sudo access on Ubuntu 20. 04 without having to modify your server&rsquo;s /etc/sudoers file.  Note: If you want to configure sudo for an existing user, skip to step 3. Step 1 — Logging Into Your Server: SSH in to your server as the root user: ssh root@your_server_ip_addressStep 2 — Adding a New User to the System: Use the adduser command to add a new user to your system: adduser sammyBe sure to replace sammy with the username that you want to create. You will be prompted to create and verify a password for the user: OutputEnter new UNIX password:Retype new UNIX password:passwd: password updated successfullyNext, you&rsquo;ll be asked to fill in some information about the new user. It is fine to accept the defaults and leave this information blank: OutputChanging the user information for sammyEnter the new value, or press ENTER for the default  Full Name []:  Room Number []:  Work Phone []:  Home Phone []:  Other []:Is the information correct? [Y/n]Step 3 — Adding the User to the sudo Group: Use the usermod command to add the user to the sudo group: usermod -aG sudo sammyAgain, be sure to replace sammy with the username you just added. By default on Ubuntu, all members of the sudo group have full sudo privileges. Step 4 — Testing sudo Access: To test that the new sudo permissions are working, first use the su command to switch to the new user account: su - sammyAs the new user, verify that you can use sudo by prepending sudo to the command that you want to run with superuser privileges: sudo command_to_runFor example, you can list the contents of the /root directory, which is normally only accessible to the root user: sudo ls -la /rootThe first time you use sudo in a session, you will be prompted for the password of that user’s account. Enter the password to proceed: Output:[sudo] password for sammy:Note: This is not asking for the root password! Enter the password of the sudo-enabled user you just created. If your user is in the proper group and you entered the password correctly, the command that you issued with sudo will run with root privileges. Conclusion: In this quickstart tutorial, we created a new user account and added it to the sudo group to enable sudo access.  For your new user to be granted external access, please follow our section on Enabling External Access for Your Regular User. If you need more detailed information on setting up an Ubuntu 20. 04 server, please read our Initial Server Setup with Ubuntu 20. 04 tutorial. "
    }, {
    "id": 58,
    "url": "https://bright-softwares.com/blog/fr/docker/how-to-configure-jenkins-with-ssl-using-an-nginx-reverse-proxy-on-ubuntu-2004",
    "title": "How To Configure Jenkins with SSL Using an Nginx Reverse Proxy on Ubuntu 20.04",
    "body": "2020/07/05 - Introduction: By default, Jenkins comes with its own built-in Winstone web server listening on port 8080, which is convenient for getting started. It&rsquo;s also a good idea, however, to secure Jenkins with SSL to protect passwords and sensitive data transmitted through the web interface.  In this tutorial, you will configure Nginx as a reverse proxy to direct client requests to Jenkins.  Prerequisites: To begin, you&rsquo;ll need the following: One Ubuntu 20. 04 server configured with a non-root sudo-enabled user and firewall, following the Ubuntu 20. 04 initial server setup guide. Jenkins installed, following the steps in How to Install Jenkins on Ubuntu 20. 04Nginx installed, following the steps in How to Install Nginx on Ubuntu 20. 04An SSL certificate for a domain provided by Let&rsquo;s Encrypt. Follow How to Secure Nginx with Let&rsquo;s Encrypt on Ubuntu 20. 04 to obtain this certificate. Note that you will need a registered domain name that you own or control. This tutorial will use the domain name example. com throughout. Step 1 — Configuring Nginx: In the prerequisite tutorial How to Secure Nginx with Let&rsquo;s Encrypt on Ubuntu 20. 04, you configured Nginx to use SSL in the /etc/nginx/sites-available/example. com file. Open this file to add your reverse proxy settings: sudo nano /etc/nginx/sites-available/example. comIn the server block with the SSL configuration settings, add Jenkins-specific access and error logs: /etc/nginx/sites-available/example. com. . . server {    . . .     # SSL Configuration    #    listen [::]:443 ssl ipv6only=on; # managed by Certbot    listen 443 ssl; # managed by Certbot    access_log      /var/log/nginx/jenkins. access. log;    error_log       /var/log/nginx/jenkins. error. log;    . . .     }Next let&rsquo;s configure the proxy settings. Since we&rsquo;re sending all requests to Jenkins, we&rsquo;ll comment out the default try_files line, which would otherwise return a 404 error before the request reaches Jenkins: /etc/nginx/sites-available/example. com. . .      location / {        # First attempt to serve request as file, then        # as directory, then fall back to displaying a 404.         # try_files $uri $uri/ =404;    }. . . Let&rsquo;s now add the proxy settings, which include: proxy_params: The /etc/nginx/proxy_params file is supplied by Nginx and ensures that important information, including the hostname, the protocol of the client request, and the client IP address, is retained and available in the log files. proxy_pass: This sets the protocol and address of the proxied server, which in this case will be the Jenkins server accessed via localhost on port 8080. proxy_read_timeout: This enables an increase from Nginx&rsquo;s 60 second default to the Jenkins-recommended 90 second value. proxy_redirect: This ensures that responses are correctly rewritten to include the proper host name. Be sure to substitute your SSL-secured domain name for example. com in the proxy_redirect line below: /etc/nginx/sites-available/example. comLocation / . . .      location / {        # First attempt to serve request as file, then        # as directory, then fall back to displaying a 404.         # try_files $uri $uri/ =404;        include /etc/nginx/proxy_params;        proxy_pass     http://localhost:8080;        proxy_read_timeout 90s;        # Fix potential  It appears that your reverse proxy setup is broken  error.         proxy_redirect   http://localhost:8080 https://example. com;Once you&rsquo;ve made these changes, save the file and exit the editor. We&rsquo;ll hold off on restarting Nginx until after we’ve configured Jenkins, but we can test our configuration now: sudo nginx -tIf all is well, the command will return: Outputnginx: the configuration file /etc/nginx/nginx. conf syntax is oknginx: configuration file /etc/nginx/nginx. conf test is successfulIf not, fix any reported errors until the test passes. &lt;p&gt;Note: If you misconfigure the proxy_pass (by adding a trailing slash, for example), you will get something similar to the following in your Jenkins Configuration page. &lt;/p&gt; If you see this error, double-check your proxy_pass and proxy_redirect settings in the Nginx configuration. &lt;/span&gt; Step 2 — Configuring Jenkins: For Jenkins to work with Nginx, you will need to update the Jenkins configuration so that the Jenkins server listens only on the localhost interface rather than on all interfaces (0. 0. 0. 0). If Jenkins listens on all interfaces, it&rsquo;s potentially accessible on its original, unencrypted port (8080).  Let&rsquo;s modify the /etc/default/jenkins configuration file to make these adjustments: sudo nano /etc/default/jenkinsLocate the JENKINS_ARGS line and add --httpListenAddress=127. 0. 0. 1 to the existing arguments: /etc/default/jenkins. . . JENKINS_ARGS= --webroot=/var/cache/$NAME/war --httpPort=$HTTP_PORT --httpListenAddress=127. 0. 0. 1 Save and exit the file. To use the new configuration settings, restart Jenkins: sudo systemctl restart jenkinsSince systemctl doesn&rsquo;t display output, check the status: sudo systemctl status jenkinsYou should see the active (exited) status in the Active line: Output● jenkins. service - LSB: Start Jenkins at boot time  Loaded: loaded (/etc/init. d/jenkins; generated)  Active: active (exited) since Mon 2018-07-09 20:26:25 UTC; 11s ago   Docs: man:systemd-sysv-generator(8) Process: 29766 ExecStop=/etc/init. d/jenkins stop (code=exited, status=0/SUCCESS) Process: 29812 ExecStart=/etc/init. d/jenkins start (code=exited, status=0/SUCCESS)Restart Nginx: sudo systemctl restart nginxCheck the status: sudo systemctl status nginxOutput● nginx. service - A high performance web server and a reverse proxy server  Loaded: loaded (/lib/systemd/system/nginx. service; enabled; vendor preset: enabled)  Active: active (running) since Mon 2018-07-09 20:27:23 UTC; 31s ago   Docs: man:nginx(8) Process: 29951 ExecStop=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx. pid (code=exited, status=0/SUCCESS) Process: 29963 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Process: 29952 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Main PID: 29967 (nginx)With both servers restarted, you should be able to visit the domain using either HTTP or HTTPS. HTTP requests will be redirected automatically to HTTPS, and the Jenkins site will be served securely. Step 3 — Testing the Configuration: Now that you have enabled encryption, you can test the configuration by resetting the administrative password. Let&rsquo;s start by visiting the site via HTTP to verify that you can reach Jenkins and are redirected to HTTPS. In your web browser, enter http://example. com, substituting your domain for example. com. After you press ENTER, the URL should start with https and the location bar should indicate that the connection is secure.  You can enter the administrative username you created in How To Install Jenkins on Ubuntu 20. 04 in the User field, and the password that you selected in the Password field. Once logged in, you can change the password to be sure it&rsquo;s secure.  Click on your username in the upper-right-hand corner of the screen. On the main profile page, select Configure from the list on the left side of the page: This will take you to a new page, where you can enter and confirm a new password: Confirm the new password by clicking Save. You can now use the Jenkins web interface securely. Conclusion: In this tutorial, you configured Nginx as a reverse proxy to Jenkins&rsquo; built-in web server to secure your credentials and other information transmitted via the web interface. Now that Jenkins is secure, you can learn how to set up a continuous integration pipeline to automatically test code changes.  Other resources to consider if you are new to Jenkins are the Jenkins project&rsquo;s &ldquo;Creating your first Pipeline&rdquo; tutorial or the library of community-contributed plugins. "
    }, {
    "id": 59,
    "url": "https://bright-softwares.com/blog/fr/slack/how-to-build-a-slackbot-in-python-on-ubuntu-2004",
    "title": "How To Build a Slackbot in Python on Ubuntu 20.04",
    "body": "2020/07/05 - The author selected the Tech Education Fund to receive a donation as part of the Write for DOnations program. Introduction: Slack is a communication platform designed for workplace productivity. It includes features such as direct messaging, public and private channels, voice and video calls, and bot integrations. A Slackbot is an automated program that can perform a variety of functions in Slack, from sending messages to triggering tasks to alerting on certain events. In this tutorial you will build a Slackbot in the Python programming language. Python is a popular language that prides itself on simplicity and readability. Slack provides a rich Python Slack API for integrating with Slack to perform common tasks such as sending messages, adding emojis to messages, and much more. Slack also provides a Python Slack Events API for integrating with events in Slack, allowing you to perform actions on events such as messages and mentions. As a fun proof-of-concept that will demonstrate the power of Python and its Slack APIs, you will build a CoinBot—a Slackbot that monitors a channel and, when triggered, will flip a coin for you. You can then modify your CoinBot to fulfill any number of slightly more practical applications. Note that this tutorial uses Python 3 and is not compatible with Python 2. Prerequisites: In order to follow this guide, you&rsquo;ll need: A Slack Workspace that you have the ability to install applications into. If you created the workspace you have this ability. If you don&rsquo;t already have one, you can create one on the Slack website. (Optional) A server or computer with a public IP address for development. We recommend a fresh installation of Ubuntu 20. 04, a non-root user with sudo privileges, and SSH enabled. You can follow this guide to initialize your server and complete these steps. You may want to test this tutorial on a server that has a public IP address. Slack will need to be able to send events such as messages to your bot. If you are testing on a local machine you will need to port forward traffic through your firewall to your local system. If you are looking for a way to develop on a cloud server, check out this tutorial on How To Use Visual Studio Code for Remote Development via the Remote-SSH Plugin. Step 1 — Creating the Slackbot in the Slack UI: First create your Slack app in the Slack API Control Panel. Log in to your workspace in Slack via a web browser and navigate to the API Control Panel. Now click on the Create an App button.  Next you&rsquo;ll be prompted for the name of your app and to select a development Slack workspace. For this tutorial, name your app CoinBot and select a workspace you have admin access to. Once you have done this click on the Create App button.  Once your app is created you&rsquo;ll be presented with the following default app dashboard. This dashboard is where you manage your app by setting permissions, subscribing to events, installing the app into workspaces, and more.  In order for your app to be able to post messages to a channel you need to grant the app permissions to send messages. To do this, click the Permissions button in the control panel.  When you arrive at the OAuth &amp; Permissions page, scroll down until you find the Scopes section of the page. Then find the Bot Token Scopes subsection in the scope and click on Add an OAuth Scope button.  Click on that button and then type chat:write. Select that permission to add it to your bot. This will allow the app to post messages to channels that it can access. For more information on the available permissions refer to Slack&rsquo;s Documentation.  Now that you&rsquo;ve added the appropriate permission it is time to install your app into your Slack workspace. Scroll back up on the OAuth &amp; Permissions page and click the Install App to Workspace button at the top.  Click this button and review the actions that the app can perform in the channel. Once you are satisfied, click the Allow button to finish the installation.  Once the bot is installed you&rsquo;ll be presented with a Bot User OAuth Access Token for your app to use when attempting to perform actions in the workspace. Go ahead and copy this token; you&rsquo;ll need it later.  Finally, add your newly installed bot into a channel within your workspace. If you haven&rsquo;t created a channel yet you can use the #general channel that is created by default in your Slack workspace. Locate the app in the Apps section of the navigation bar in your Slack client and click on it. Once you&rsquo;ve done that open the Details menu in the top right hand side. If your Slack client isn&rsquo;t full-screened it will look like an i in a circle.  To finish adding your app to a channel, click on the More button represented by three dots in the details page and select Add this app to a channel&hellip;. Type your channel into the modal that appears and click Add.  You&rsquo;ve now successfully created your app and added it to a channel within your Slack workspace. After you write the code for your app it will be able to post messages in that channel. In the next section you&rsquo;ll start writing the Python code that will power CoinBot. Step 2 — Setting Up Your Python Developer Environment: First let&rsquo;s set up your Python environment so you can develop the Slackbot. Open a terminal and install python3 and the relevant tools onto your system: sudo apt install python3 python3-venvNext you will create a virtual environment to isolate your Python packages from the system installation of Python. To do this, first create a directory into which you will create your virtual environment. Make a new directory at ~/. venvs: mkdir ~/. venvsNow create your Python virtual environment: python3 -m venv ~/. venvs/slackbotNext, activate your virtual environment so you can use its Python installation and install packages: source ~/. venvs/slackbot/bin/activateYour shell prompt will now show the virtual environment in parenthesis. It will look something like this: Now use pip to install the necessary Python packages into your virtual environment: pip install slackclient slackeventsapi Flaskslackclient and slackeventsapi facilitate Python&rsquo;s interaction with Slack&rsquo;s APIs. Flask is a popular micro web framework that you will use to deploy your app: Now that you have your developer environment set up, you can start writing your Python Slackbot: Step 3 — Creating the Slackbot Message Class in Python: Messages in Slack are sent via a specifically formatted JSON payload. This is an example of the JSON that your Slackbot will craft and send as a message: {   channel : channel ,   blocks :[   {      type : section ,      text :{       type : mrkdwn ,       text : Sure! Flipping a coin. . . . \n\n      }   },   {      type : section ,      text :{       type : mrkdwn ,       text : *flips coin* The result is Tails.       }   }  ]}You could manually craft this JSON and send it, but instead let&rsquo;s build a Python class that not only crafts this payload, but also simulates a coin flip. First use the touch command to create a file named coinbot. py: touch coinbot. pyNext, open this file with nano or your favorite text editor: nano coinbot. pyNow add the following lines of code to import the relevant libraries for your app. The only library you need for this class is the random library from the Python Standard Library. This library will allow us to simulate a coin flip. Add the following lines to coinbot. py to import all of the necessary libraries: coinbot. py# import the random library to help us generate the random numbersimport randomNext, create your CoinBot class and an instance of this classto craft the message payload. Add the following lines to coinbot. py to create the CoinBot class: coinbot. py. . . class CoinBot:Now indent by one and create the constants, constructors, and methods necessary for your class. First let&rsquo;s create the constant that will hold the base of your message payload. This section specifies that this constant is of the section type and that the text is formatted via markdown. It also specifies what text you wish to display. You can read more about the different payload options in the official Slack message payload documentation.  Append the following lines to coinbot. py to create the base template for the payload: coinbot. py. . .   # Create a constant that contains the default text for the message  COIN_BLOCK = {     type :  section ,     text : {       type :  mrkdwn ,       text : (         Sure! Flipping a coin. . . . \n\n       ),    },  }Next create a constructor for your class so that you can create a separate instance of your bot for every request. Don&rsquo;t worry about memory overhead here; the Python garbage collector will clean up these instances once they are no longer needed.  This code sets the recipient channel based on a parameter passed to the constructor. Append the following lines to coinbot. py to create the constructor: coinbot. py. . .   # The constructor for the class. It takes the channel name as the a  # parameter and sets it as an instance variable.   def __init__(self, channel):    self. channel = channelNow write the code that simulates to flip a coin. We&rsquo;ll randomly generate a one or zero, representing heads or tails respectively. Append the following lines to coinbot. py to simulate the coin flip and return the crafted payload: coinbot. py. . .   # Generate a random number to simulate flipping a coin. Then return the   # crafted slack payload with the coin flip message.   def _flip_coin(self):    rand_int = random. randint(0,1)    if rand_int == 0:      results =  Heads     else:      results =  Tails     text = f The result is {results}     return { type :  section ,  text : { type :  mrkdwn ,  text : text}},Finally, create a method that crafts and returns the entire message payload, including the data from your constructor, by calling your _flip_coin method. Append the following lines to coinbot. py to create the method that will generate the finished payload: coinbot. py. . .   # Craft and return the entire message payload as a dictionary.   def get_message_payload(self):    return {       channel : self. channel,       blocks : [        self. COIN_BLOCK,        *self. _flip_coin(),      ],    }You are now finished with the CoinBot class and it is ready for testing. Before continuing, verify that your finished file, coinbot. py, contains the following: coinbot. py# import the random library to help us generate the random numbersimport random# Create the CoinBot Classclass CoinBot:  # Create a constant that contains the default text for the message  COIN_BLOCK = {     type :  section ,     text : {       type :  mrkdwn ,       text : (         Sure! Flipping a coin. . . . \n\n       ),    },  }  # The constructor for the class. It takes the channel name as the a  # parameter and then sets it as an instance variable  def __init__(self, channel):    self. channel = channel  # Generate a random number to simulate flipping a coin. Then return the  # crafted slack payload with the coin flip message.   def _flip_coin(self):    rand_int = random. randint(0,1)    if rand_int == 0:      results =  Heads     else:      results =  Tails     text = f The result is {results}     return { type :  section ,  text : { type :  mrkdwn ,  text : text}},  # Craft and return the entire message payload as a dictionary.   def get_message_payload(self):    return {       channel : self. channel,       blocks : [        self. COIN_BLOCK,        *self. _flip_coin(),      ],    }Save and close the file. Now that you have a Python class ready to do the work for your Slackbot, let&rsquo;s ensure that this class produces a useful message payload and that you can send it to your workspace. Step 4 — Testing Your Message: Now let&rsquo;s test that this class produces a proper payload. Create a file namedcoinbot_test. py: nano coinbot_test. pyNow add the following code. Be sure to change the channel name in the instantiation of the coinbot class coin_bot = coinbot( #YOUR_CHANNEL_HERE ). This code will create a Slack client in Python that will send a message to the channel you specify that you have already installed the app into: coinbot_test. pyfrom slack import WebClientfrom coinbot import CoinBotimport os# Create a slack clientslack_web_client = WebClient(token=os. environ. get( SLACK_TOKEN ))# Get a new CoinBotcoin_bot = CoinBot( #YOUR_CHANNEL_HERE )# Get the onboarding message payloadmessage = coin_bot. get_message_payload()# Post the onboarding message in Slackslack_web_client. chat_postMessage(\*\*message)Save and close the file. Before you can run this file you will need to export the Slack token that you saved in Step 1 as an environment variable: export SLACK_TOKEN= your_bot_user_token Now test this file and verify that the payload is produced and sent by running the following script in your terminal. Make sure that your virtual environment is activated. You can verify this by seeing the (slackbot) text at the front of your bash prompt. Run this command you will receive a message from your Slackbot with the results of a coin flip: python coinbot_test. pyCheck the channel that you installed your app into and verify that your bot did indeed send the coin flip message. Your result will be heads or tails.  Now that you&rsquo;ve verified that your Slackbot can flip a coin, create a message, and deliver the message, let&rsquo;s create a Flask to perpetually run this app and make it simulate a coin flip and share the results whenever it sees certain text in messages sent in the channel. Step 5 — Creating a Flask Application to Run Your Slackbot: Now that you have a functioning application that can send messages to your Slack workspace, you need to create a long running process so your bot can listen to messages sent in the channel and reply to them if the text meets certain criteria. You&rsquo;re going to use the Python web framework Flask to run this process and listen for events in your channel. In this section you will be running your Flask application from a server with a public IP address so that the Slack API can send you events. If you are running this locally on your personal workstation you will need to forward the port from your personal firewall to the port that will be running on your workstation. These ports can be the same, and this tutorial will be set up to use port 3000. First adjust your firewall settings to allow traffic through port 3000: sudo ufw allow 3000Now check the status of ufw: sudo ufw statusYou will see an output like this: OutputStatus: activeTo Action From---OpenSSH ALLOW Anywhere3000 ALLOW AnywhereOpenSSH (v6) ALLOW Anywhere (v6)3000 (v6) ALLOW Anywhere (v6)Now create the file for your Flask app. Name this file app. py: touch app. pyNext, open this file in your favorite text editor: nano app. pyNow add the following import statements. You&rsquo;ll import the following libraries for the following reasons: import os - To access environment variablesimport logging - To log the events of the appfrom flask import Flask - To create a Flask appfrom slack import WebClient - To send messages via Slackfrom slackeventsapi import SlackEventAdapter - To receive events from Slack and process themfrom coinbot import CoinBot - To create an instance of your CoinBot and generate the message payload. Append the following lines to app. py to import all of the necessary libraries: app. pyimport osimport loggingfrom flask import Flaskfrom slack import WebClientfrom slackeventsapi import SlackEventAdapterfrom coinbot import CoinBotNow create your Flask app and register a Slack Event Adapter to your Slack app at the /slack/events endpoint. This will create a route in your Slack app where Slack events will be sent and ingested. To do this you will need to get another token from your Slack app, which you will do later in the tutorial. Once you get this variable you will export it as an environment variable named SLACK_EVENTS_TOKEN. Go ahead and write your code to read it in when creating the SlackEventAdapter, even though you haven&rsquo;t set the token yet. Append the following lines to app. py to create the Flask app and register the events adapter into this app: app. py. . . # Initialize a Flask app to host the events adapterapp = Flask(__name__)# Create an events adapter and register it to an endpoint in the slack app for event ingestion. slack_events_adapter = SlackEventAdapter(os. environ. get( SLACK_EVENTS_TOKEN ),  /slack/events , app)Next create a web client object that will allow your app to perform actions in the workspace, specifically to send messages. This is similar to what you did when you tested your coinbot. py file previously. Append the following line to app. py to create this slack_web_client: app. py. . . # Initialize a Web API clientslack_web_client = WebClient(token=os. environ. get( SLACK_TOKEN ))Now create a function that can be called that will create an instance of CoinBot, and then use this instance to create a message payload and pass the message payload to the Slack web client for delivery. This function will take in a single parameter, channel, which will specify what channel receives the message. Append the following lines to app. py to create this function: app. py. . . def flip_coin(channel):     Craft the CoinBot, flip the coin and send the message to the channel       # Create a new CoinBot  coin_bot = CoinBot(channel)  # Get the onboarding message payload  message = coin_bot. get_message_payload()  # Post the onboarding message in Slack  slack_web_client. chat_postMessage(**message)Now that you have created a function to handle the messaging aspects of your app, create one that monitors Slack events for a certain action and then executes your bot. You&rsquo;re going to configure your app to respond with the results of a simulated coin flip when it sees the phrase &ldquo;Hey Sammy, Flip a coin&rdquo;. You&rsquo;re going to accept any version of this—case won&rsquo;t prevent the app from responding. First decorate your function with the @slack_events_adapter. on syntax that allows your function to receive events. Specify that you only want the message events and have your function accept a payload parameter containing all of the necessary Slack information. Once you have this payload you will parse out the text and analyze it. Then, if it receives the activation phrase, your app will send the results of a simulated coin flip. Append the following code to app. py to receive, analyze, and act on incoming messages: app. py# When a 'message' event is detected by the events adapter, forward that payload# to this function. @slack_events_adapter. on( message )def message(payload):     Parse the message event, and if the activation string is in the text,  simulate a coin flip and send the result.        # Get the event data from the payload  event = payload. get( event , {})  # Get the text from the event that came through  text = event. get( text )  # Check and see if the activation phrase was in the text of the message.   # If so, execute the code to flip a coin.   if  hey sammy, flip a coin  in text. lower():    # Since the activation phrase was met, get the channel ID that the event    # was executed on    channel_id = event. get( channel )    # Execute the flip_coin function and send the results of    # flipping a coin to the channel    return flip_coin(channel_id)Finally, create a main section that will create a logger so you can see the internals of your application as well as launch the app on your external IP address on port 3000. In order to ingest the events from Slack, such as when a new message is sent, you must test your application on a public-facing IP address. Append the following lines to app. py to set up your main section: app. pyif __name__ ==  __main__ :  # Create the logging object  logger = logging. getLogger()  # Set the log level to DEBUG. This will increase verbosity of logging messages  logger. setLevel(logging. DEBUG)  # Add the StreamHandler as a logging handler  logger. addHandler(logging. StreamHandler())  # Run your app on your externally facing IP address on port 3000 instead of  # running it on localhost, which is traditional for development.   app. run(host='0. 0. 0. 0', port=3000)You are now finished with the Flask app and it is ready for testing. Before you move on verify that your finished file, app. py contains the following: app. pyimport osimport loggingfrom flask import Flaskfrom slack import WebClientfrom slackeventsapi import SlackEventAdapterfrom coinbot import CoinBot# Initialize a Flask app to host the events adapterapp = Flask(**name**)# Create an events adapter and register it to an endpoint in the slack app for event injestion. slack_events_adapter = SlackEventAdapter(os. environ. get( SLACK_EVENTS_TOKEN ),  /slack/events , app)# Initialize a Web API clientslack_web_client = WebClient(token=os. environ. get( SLACK_TOKEN ))def flip_coin(channel):   Craft the CoinBot, flip the coin and send the message to the channel    # Create a new CoinBotcoin_bot = CoinBot(channel)  # Get the onboarding message payload  message = coin_bot. get_message_payload()  # Post the onboarding message in Slack  slack_web_client. chat_postMessage(**message)# When a 'message' event is detected by the events adapter, forward that payload# to this function. @slack_events_adapter. on( message )def message(payload):   Parse the message event, and if the activation string is in the text,simulate a coin flip and send the result.      # Get the event data from the payload  event = payload. get( event , {})  # Get the text from the event that came through  text = event. get( text )  # Check and see if the activation phrase was in the text of the message.   # If so, execute the code to flip a coin.   if  hey sammy, flip a coin  in text. lower():    # Since the activation phrase was met, get the channel ID that the event    # was executed on    channel_id = event. get( channel )    # Execute the flip_coin function and send the results of    # flipping a coin to the channel    return flip_coin(channel_id)if **name** ==  **main** : # Create the logging objectlogger = logging. getLogger()  # Set the log level to DEBUG. This will increase verbosity of logging messages  logger. setLevel(logging. DEBUG)  # Add the StreamHandler as a logging handler  logger. addHandler(logging. StreamHandler())  # Run our app on our externally facing IP address on port 3000 instead of  # running it on localhost, which is traditional for development.   app. run(host='0. 0. 0. 0', port=3000)Save and close the file. Now that your Flask app is ready to serve your application let&rsquo;s test it out. Step 6 — Running Your Flask App: Finally, bring everything together and execute your app. First, add your running application as an authorized handler for your Slackbot. Navigate to the Basic Information section of your app in the Slack UI. Scroll down until you find the App Credentials section.  Copy the Signing Secret and export it as the environment variable SLACK_EVENTS_TOKEN: export SLACK_EVENTS_TOKEN= MY_SIGNING_SECRET_TOKEN With this you have all the necessary API tokens to run your app. Refer to Step 1 if you need a refresher on how to export your SLACK_TOKEN. Now you can start your app and verify that it is indeed running. Ensure that your virtual environment is activated and run the following command to start your Flask app: python3 app. pyYou will see an output like this: (slackbot) [20:04:03] sammy:coinbot$ python app. py * Serving Flask app  app  (lazy loading) * Environment: production  WARNING: This is a development server. Do not use it in a production deployment.  Use a production WSGI server instead. * Debug mode: off * Running on http://0. 0. 0. 0:3000/ (Press CTRL+C to quit)To verify that your app is up, open a new terminal window and curl the IP address of your server with the correct port at /slack/events: curl http://YOUR_IP_ADDRESS:3000/slack/eventscurl will return the following: OutputThese are not the slackbots you're looking for. Receiving the message These are not the slackbots you're looking for. , indicates that your app is up and running. Now leave this Flask application running while you finish configuring your app in the Slack UI. First grant your app the appropriate permissions so that it can listen to messages and respond accordingly. Click on Event Subscriptions in the UI sidebar and toggle the Enable Events radio button.  Once you&rsquo;ve done that, type in your IP address, port, and /slack/events endpoint into the Request URL field. Don&rsquo;t forget the HTTP protocol prefix. Slack will make an attempt to connect to your endpoint. Once it has successfully done so you&rsquo;ll see a green check mark with the word Verified next to it.  Next, expand the Subscribe to bot events and add the message. channels permission to your app. This will allow your app to receive messages from your channel and process them.  Once you&rsquo;ve done this you will see the event listed in your Subscribe to bot events section. Next click the green Save Changes button in the bottom right hand corner.  Once you do this you&rsquo;ll see a yellow banner across the top of the screen informing you that you need to reinstall your app for the following changes to apply. Every time you change permissions you&rsquo;ll need to reinstall your app. Click on the reinstall your app link in this banner to reinstall your app.  You&rsquo;ll be presented with a confirmation screen summarizing the permissions your bot will have and asking if you want to allow its installation. Click on the green Allow button to finish the installation process.  Now that you&rsquo;ve done this your app should be ready. Go back to the channel that you installed CoinBot into and send a message containing the phrase Hey Sammy, Flip a coin in it. Your bot will flip a coin and reply with the results. Congrats! You&rsquo;ve created a Slackbot! Conclusion: Once you are done developing your application and you are ready to move it to production, you&rsquo;ll need to deploy it to a server. This is necessary because the Flask development server is not a secure production environment. You&rsquo;ll be better served if you deploy your app using a WSGI and maybe even securing a domain name and giving your server a DNS record. There are many options for deploying Flask applications, some of which are listed below: Deploy your Flask application to Ubuntu 20. 04 using Gunicorn and NginxDeploy your Flask application to Ubuntu 20. 04 using uWSGI and NginxDeploy your Flask Application Using Docker on Ubuntu 18. 04There are many more ways to deploy your application than just these. As always, when it comes to deployments and infrastucture, do what works best for you. In any case, you now have a Slackbot that you can use to flip a coin to help you make decisions, like what to eat for lunch. You can also take this base code and modify it to fit your needs, whether it be automated support, resource management, pictures of cats, or whatever you can think of. You can view the complete Python Slack API docs here. "
    }, {
    "id": 60,
    "url": "https://bright-softwares.com/blog/en/javascript/understanding-template-literals-in-javascript",
    "title": "Understanding Template Literals in JavaScript",
    "body": "2020/07/05 - The author selected the COVID-19 Relief Fund to receive a donation as part of the Write for DOnations program. Introduction: The 2015 edition of the ECMAScript specification (ES6) added template literals to the JavaScript language. Template literals are a new form of making strings in JavaScript that add a lot of powerful new capabilities, such as creating multi-line strings more easily and using placeholders to embed expressions in a string. In addition, an advanced feature called tagged template literals allows you to perform operations on the expressions within a string. All of these capabilities increase your options for string manipulation as a developer, letting you generate dynamic strings that could be used for URLs or functions that customize HTML elements. In this article, you will go over the differences between single/double-quoted strings and template literals, running through the various ways to declare strings of different shape, including multi-line strings and dynamic strings that change depending on the value of a variable or expression. You will then learn about tagged templates and see some real-world examples of projects using them. Declaring Strings: This section will review how to declare strings with single quotes and double quotes, and will then show you how to do the same with template literals. In JavaScript, a string can be written with single quotes (' '): const single = 'Every day is a good day when you paint. 'A string can also be written with double quotes (   ): const double =  Be so very light. Be a gentle whisper.  There is no major difference in JavaScript between single- or double-quoted strings, unlike other languages that might allow interpolation in one type of string but not the other. In this context, interpolation refers to the evaluation of a placeholder as a dynamic part of a string. The use of single- or double-quoted strings mostly comes down to personal preference and convention, but used in conjunction, each type of string only needs to escape its own type of quote: // Escaping a single quote in a single-quoted stringconst single = ' We don\'t make mistakes. We just have happy accidents.   - Bob Ross'// Escaping a double quote in a double-quoted stringconst double =  \ We don't make mistakes. We just have happy accidents. \  - Bob Ross console. log(single);console. log(double);The result of the log() method here will print the same two strings to the console: Output We don't make mistakes. We just have happy accidents.   - Bob Ross We don't make mistakes. We just have happy accidents.   - Bob RossTemplate literals, on the other hand, are written by surrounding the string with the backtick character, or grave accent (`): const template = `Find freedom on this canvas. `They do not need to escape single or double quotes: const template = ` We don't make mistakes. We just have happy accidents.   - Bob Ross`However, they do still need to escape backticks: const template = `Template literals use the \` character. `Template literals can do everything that regular strings can, so you could possibly replace all strings in your project with them and have the same functionality. However, the most common convention in codebases is to only use template literals when using the additional capabilities of template literals, and consistently using the single or double quotes for all other simple strings. Following this standard will make your code easier to read if examined by another developer. Now that you&rsquo;ve seen how to declare strings with single quotes, double quotes, and backticks, you can move on to the first advantage of template literals: writing multi-line strings. Multi-line Strings: In this section, you will first run through the way strings with multiple lines were declared before ES6, then see how template literals make this easier. Originally, if you wanted to write a string that spans multiple lines in your text editor, you would use the concatenation operator. However, this was not always a straight-forward process. The following string concatenation seemed to run over multiple lines: const address =  'Homer J. Simpson' +  '742 Evergreen Terrace' +  'Springfield'This might allow you to break up the string into smaller lines and include it over multiple lines in the text editor, but it has no effect on the output of the string. In this case, the strings will all be on one line and not separated by newlines or spaces. If you logged address to the console, you would get the following: OutputHomer J. Simpson742 Evergreen TerraceSpringfieldYou can use the backslash character (\) to continue the string onto multiple lines: const address = 'Homer J. Simpson\ 742 Evergreen Terrace\ Springfield'This will retain any indentation as whitespace, but the string will still be on one line in the output: OutputHomer J. Simpson 742 Evergreen Terrace SpringfieldUsing the newline character (\n), you can create a true multi-line string: const address =  'Homer J. Simpson\n' +  '742 Evergreen Terrace\n' +  'Springfield'When logged to the console, this will display the following: OutputHomer J. Simpson742 Evergreen TerraceSpringfieldUsing newline characters to designate multi-line strings can be counterintuitive, however. In contrast, creating a multi-line string with template literals can be much more straight-forward. There is no need to concatenate, use newline characters, or use backslashes. Just pressing ENTER and writing the string across multiple lines works by default: const address = `Homer J. Simpson742 Evergreen TerraceSpringfield`The output of logging this to the console is the same as the input: OutputHomer J. Simpson742 Evergreen TerraceSpringfieldAny indentation will be preserved, so it&rsquo;s important not to indent any additional lines in the string if that is not desired. For example, consider the following: const address = `Homer J. Simpson         742 Evergreen Terrace         Springfield`Although this style of writing the line might make the code more human readable, the output will not be: OutputHomer J. Simpson         742 Evergreen Terrace         SpringfieldWith multi-line strings now covered, the next section will deal with how expressions are interpolated into their values with the different string declarations. Expression Interpolation: In strings before ES6, concatenation was used to create a dynamic string with variables or expressions: const method = 'concatenation'const dynamicString = 'This string is using ' + method + '. 'When logged to the console, this will yield the following: OutputThis string is using concatenation. With template literals, an expression can be embedded in a placeholder. A placeholder is represented by ${}, with anything within the curly brackets treated as JavaScript and anything outside the brackets treated as a string: const method = 'interpolation'const dynamicString = `This string is using ${method}. `When dynamicString is logged to the console, the console will show the following: OutputThis string is using interpolation. One common example of embedding values in a string might be for creating dynamic URLs. With concatenation, this can be cumbersome. For example, the following declares a function to generate an OAuth access string: function createOAuthString(host, clientId, scope) { return host + '/login/oauth/authorize?client_id=' + clientId + '&amp;scope=' + scope}createOAuthString('https://github. com', 'abc123', 'repo,user')Logging this function will yield the following URL to the console: Outputhttps://github. com/login/oauth/authorize?client_id=abc123&amp;scope=repo,userUsing string interpolation, you no longer have to keep track of opening and closing strings and concatenation operator placement. Here is the same example with template literals: function createOAuthString(host, clientId, scope) { return `${host}/login/oauth/authorize?client_id=${clientId}&amp;scope=${scope}`}createOAuthString('https://github. com', 'abc123', 'repo,user')This will have the same output as the concatenation example: Outputhttps://github. com/login/oauth/authorize?client_id=abc123&amp;scope=repo,userYou can also use the trim() method on a template literal to remove any whitespace at the beginning or end of the string. For example, the following uses an arrow function to create an HTML &lt;li&gt; element with a customized link: const menuItem = (url, link) =&gt; `&lt;li&gt; &lt;a href= ${url} &gt;${link}&lt;/a&gt;&lt;/li&gt;`. trim()menuItem('https://google. com', 'Google')The result will be trimmed of all the whitespace, ensuring that the element will be rendered correctly: Output&lt;li&gt; &lt;a href= https://google. com &gt;Google&lt;/a&gt;&lt;/li&gt;Entire expressions can be interpolated, not just variables, such as in this example of the sum of two numbers: const sum = (x, y) =&gt; x + yconst x = 5const y = 100const string = `The sum of ${x} and ${y} is ${sum(x, y)}. `console. log(string)This code defines the sum function and the variables x and y, then uses both the function and the variables in a string. The logged result will show the following: OutputThe sum of 5 and 100 is 105. This can be particularly useful with ternary operators, which allow conditionals within a string: const age = 19const message = `You can ${age &lt; 21 ? 'not' : ''} view this page`console. log(message)The logged message here will change depnding on whether the value of age is over or under 21. Since it is 19 in this example, the following output will be logged: OutputYou can not view this pageNow you have an idea of how template literals can be useful when used to interpolate expressions. The next section will take this a step further by examining tagged template literals to work with the expressions passed into placeholders. Tagged Template Literals: An advanced feature of template literals is the use of tagged template literals, sometimes referred to as template tags. A tagged template starts with a tag function that parses a template literal, allowing you more control over manipulating and returning a dynamic string. In this example, you&rsquo;ll create a tag function to use as the function operating on a tagged template. The string literals are the first parameter of the function, named strings here, and any expressions interpolated into the string are packed into the second parameter using rest parameters. You can console out the parameter to see what they will contain: function tag(strings, . . . expressions) { console. log(strings) console. log(expressions)}Use the tag function as the tagged template function and parse the string as follows: const string = tag`This is a string with ${true} and ${false} and ${100} interpolated inside. `Since you&rsquo;re console logging strings and expressions, this will be the output: Output(4) [ This is a string with  ,   and  ,   and  ,   interpolated inside.  (3) [true, false, 100]The first parameter, strings, is an array containing all the string literals:  This is a string with    and    and    interpolated inside.  There is also a raw property available on this argument at strings. raw, which contains the strings without any escape sequences being processed. For example, /n would just be the character /n and not be escaped into a newline. The second parameter, . . . expressions, is a rest parameter array consisting of all the expressions: truefalse100The string literals and expressions are passed as parameters to the tagged template function tag. Note that the tagged template does not need to return a string; it can operate on those values and return any type of value. For example, we can have the function ignore everything and return null, as in this returnsNull function: function returnsNull(strings, . . . expressions) { return null}const string = returnsNull`Does this work?`console. log(string)Logging the string variable will return: OutputnullAn example of an action that can be performed in tagged templates is applying some change to both sides of each expression, such as if you wanted to wrap each expression in an HTML tag. Create a bold function that will add &lt;strong&gt; and &lt;/strong&gt; to each expression: function bold(strings, . . . expressions) { let finalString = ''// Loop through all expressionsexpressions. forEach((value, i) =&gt; {finalString += `${strings[i]}&lt;strong&gt;${value}&lt;/strong&gt;`})// Add the last string literalfinalString += strings[strings. length - 1]return finalString}const string = bold`This is a string with ${true} and ${false} and ${100} interpolated inside. `console. log(string)This code uses the forEach method to loop over the expressions array and add the bolding element: OutputThis is a string with &lt;strong&gt;true&lt;/strong&gt; and &lt;strong&gt;false&lt;/strong&gt; and &lt;strong&gt;100&lt;/strong&gt; interpolated inside. There are a few examples of tagged template literals in popular JavaScript libraries. The graphql-tag library uses the gql tagged template to parse GraphQL query strings into the abstract syntax tree (AST) that GraphQL understands: import gql from 'graphql-tag'// A query to retrieve the first and last name from user 5const query = gql` { user(id: 5) { firstName lastName } }`Another library that uses tagged template functions is styled-components, which allows you to create new React components from regular DOM elements and apply additional CSS styles to them: import styled from 'styled-components'const Button = styled. button` color: magenta;`// &lt;Button&gt; can now be used as a custom componentYou can also use the built-in String. raw method on tagged template literals to prevent any escape sequences from being processed: const rawString = String. raw`I want to write /n without it being escaped. `console. log(rawString)This will log the following: OutputI want to write /n without it being escaped. Conclusion: In this article, you reviewed single- and double-quoted string literals and you learned about template literals and tagged template literals. Template literals make a lot of common string tasks simpler by interpolating expressions in strings and creating multi-line strings without any concatenation or escaping. Template tags are also a useful advanced feature of template literals that many popular libraries have used, such as GraphQL and styled-components. To learn more about strings in JavaScript, read How To Work with Strings in JavaScript and How To Index, Split, and Manipulate Strings in JavaScript. "
    }, {
    "id": 61,
    "url": "https://bright-softwares.com/blog/en/database/how-to-use-onetomany-database-relationships-with-flask-and-sqlite",
    "title": "How To Use One-to-Many Database Relationships with Flask and SQLite",
    "body": "2020/07/05 - The author selected the COVID-19 Relief Fund to receive a donation as part of the Write for DOnations program. Introduction: Flask is a framework for building web applications using the Python language, and SQLite is a database engine that can be used with Python to store application data. In this tutorial, you will use Flask with SQLite to create a to-do application where users can create lists of to-do items. You will learn how to use SQLite with Flask and how one-to-many database relationships work. A one-to-many database relationship is a relationship between two database tables where a record in one table can reference several records in another table. For example, in a blogging application, a table for storing posts can have a one-to-many relationship with a table for storing comments. Each post can reference many comments, and each comment references a single post; therefore, one post has a relationship with many comments. The post table is a parent table, while the comments table is a child table—a record in the parent table can reference many records in the child table. This is important to be able to have access to related data in each table. We&rsquo;ll use SQLite because it is portable and does not need any additional set up to work with Python. It is also great for prototyping an application before moving to a larger database such as MySQL or Postgres. For more on how to choose the right database system read our SQLite vs MySQL vs PostgreSQL: A Comparison Of Relational Database Management Systems article. Prerequisites: Before you start following this guide, you will need: A local Python 3 programming environment, follow the tutorial for your distribution in How To Install and Set Up a Local Programming Environment for Python 3 series for your local machine. In this tutorial we’ll call our project directory flask_todo. An understanding of basic Flask concepts such as creating routes, rendering HTML templates, and connecting to an SQLite database. You can follow How To Make a Web Application Using Flask in Python 3, if you are not familiar with these concepts, but it&rsquo;s not necessary. Step 1 — Creating the Database: In this step, you will activate your programming environment, install Flask, create the SQLite database, and populate it with sample data. You&rsquo;ll learn how to use foreign keys to create a one-to-many relationship between lists and items. A foreign key is a key used to associate a database table with another table, it is the link between the child table and its parent table. If you haven’t already activated your programming environment, make sure you’re in your project directory (flask_todo) and use this command to activate it: source env/bin/activateOnce your programming environment is activated, install Flask using the following command: pip install flaskOnce the installation is complete, you can now create the database schema file that contains SQL commands to create the tables you need to store your to-do data. You will need two tables: a table called lists to store to-do lists, and an items table to store the items of each list. Open a file called schema. sql inside your flask_todo directory: nano schema. sqlType the following SQL commands inside this file: flask_todo/schema. sqlDROP TABLE IF EXISTS lists;DROP TABLE IF EXISTS items;CREATE TABLE lists (id INTEGER PRIMARY KEY AUTOINCREMENT,created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,title TEXT NOT NULL);CREATE TABLE items (id INTEGER PRIMARY KEY AUTOINCREMENT,list_id INTEGER NOT NULL,created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,content TEXT NOT NULL,FOREIGN KEY (list_id) REFERENCES lists (id));Save and close the file. The first two SQL command are DROP TABLE IF EXISTS lists; and DROP TABLE IF EXISTS items;, these delete any already existing tables named lists and items so you don&rsquo;t see confusing behavior. Note that this will delete all of the content you have in the database whenever you use these SQL commands, so ensure you don&rsquo;t write any important content in the web application until you finish this tutorial and experiment with the final result. Next, you use CREATE TABLE lists to create the lists table that will store the to-do lists (such as a study list, work list, home list, and so on) with the following columns: id: An integer that represents a primary key, this will get assigned a unique value by the database for each entry (i. e. to-do list). created: The time the to-do list was created at. NOT NULL signifies that this column should not be empty and the DEFAULT value is the CURRENT_TIMESTAMP value, which is the time at which the list was added to the database. Just like id, you don&rsquo;t need to specify a value for this column, as it will be automatically filled in. title: The list title. Then, you create a table called items to store to-do items. This table has an ID, a list_id integer column to identify which list an item belongs to, a creation date, and the item&rsquo;s content. To link an item to a list in the database you use a foreign key constraint with the line FOREIGN KEY (list_id) REFERENCES lists (id). Here the lists table is a parent table, which is the table that is being referenced by the foreign key constraint, this indicates a list can have multiple items. The items table is a child table, which is the table the constraint applies to. This means items belong to a single list. The list_id column references the id column of the lists parent table. Since a list can have many items, and an item belongs to only one list, the relationship between the lists and items tables is a one-to-many relationship. Next, you will use the schema. sql file to create the database. Open a file named init_db. py inside the flask_todo directory: nano init_db. pyThen add the following code: flask_todo/init_db. pyimport sqlite3connection = sqlite3. connect('database. db')with open('schema. sql') as f:connection. executescript(f. read())cur = connection. cursor()cur. execute( INSERT INTO lists (title) VALUES (?) , ('Work',))cur. execute( INSERT INTO lists (title) VALUES (?) , ('Home',))cur. execute( INSERT INTO lists (title) VALUES (?) , ('Study',))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(1, 'Morning meeting'))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(2, 'Buy fruit'))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(2, 'Cook dinner'))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(3, 'Learn Flask'))cur. execute( INSERT INTO items (list_id, content) VALUES (?, ?) ,(3, 'Learn SQLite'))connection. commit()connection. close()Save and close the file. Here you connect to a file called database. db that will be created once you execute this program. You then open the schema. sql file and run it using the executescript() method that executes multiple SQL statements at once. Running schema. sql will create the lists and items tables. Next, using a Cursor object, you execute a few INSERT SQL statements to create three lists and five to-do items. You use the list_id column to link each item to a list via the list&rsquo;s id value. For example, the Work list was the first insertion into the database, so it will have the ID 1. This is how you can link the Morning meeting to-do item to Work—the same rule applies to the other lists and items. Finally, you commit the changes and close the connection. Run the program: python init_db. pyAfter execution, a new file called database. db will appear in your flask_todo directory. You&rsquo;ve activated your environment, installed Flask, and created the SQLite database. Next, you&rsquo;ll retrieve the lists and items from the database and display them in the application&rsquo;s homepage. Step 2 — Displaying To-do Items: In this step, you will connect the database you created in the previous step to a Flask application that displays the to-do lists and the items of each list. You will learn how to use SQLite joins to query data from two tables and how to group to-do items by their lists. First, you will create the application file. Open a file named app. py inside the flask_todo directory: nano app. pyAnd then add the following code to the file: flask_todo/app. pyfrom itertools import groupbyimport sqlite3from flask import Flask, render_template, request, flash, redirect, url_fordef get_db_connection():conn = sqlite3. connect('database. db')conn. row_factory = sqlite3. Rowreturn connapp = Flask(**name**)app. config['SECRET_KEY'] = 'this should be a secret random string'@app. route('/')def index():conn = get_db_connection()todos = conn. execute('SELECT i. content, l. title FROM items i JOIN lists l \ ON i. list_id = l. id ORDER BY l. title;'). fetchall()  lists = {}  for k, g in groupby(todos, key=lambda t: t['title']):    lists[k] = list(g)  conn. close()  return render_template('index. html', lists=lists)Save and close the file. The get_db_connection() function opens a connection to the database. db database file and then sets the row_factory attribute to sqlite3. Row. In this way you can have name-based access to columns; this means that the database connection will return rows that behave like regular Python dictionaries. Lastly, the function returns the conn connection object you&rsquo;ll be using to access the database. In the index() view function, you open a database connection and execute the following SQL query: SELECT i. content, l. title FROM items i JOIN lists l ON i. list_id = l. id ORDER BY l. title;You then retrieve its results by using the fetchall() method and save the data in a variable called todos. In this query, you use SELECT to get the content of the item and the title of the list it belongs to by joining both the items and lists tables (with the table aliases i for items and l for lists). With the join condition i. list_id = l. id after the ON keyword, you will get each row from the items table with every row from the lists table where the list_id column of the items table matches the id of the lists table. You then use ORDER BY to order the results by list titles. To understand this query better, open the Python REPL in your flask_todo directory: pythonTo understand the SQL query, examine the contents of the todos variable by running this small program: from app import get_db_connectionconn = get_db_connection()todos = conn. execute('SELECT i. content, l. title FROM items i JOIN lists l \ON i. list_id = l. id ORDER BY l. title;'). fetchall()for todo in todos:  print(todo['title'], ':', todo['content'])You first import the get_db_connection from the app. py file then open a connection and execute the query (note that this is the same SQL query you have in your app. py file). In the for loop you print the title of the list and the content of each to-do item. The output will be as follows: OutputHome : Buy fruitHome : Cook dinnerStudy : Learn FlaskStudy : Learn SQLiteWork : Morning meetingClose the REPL using CTRL + D. Now that you understand how SQL joins work and what the query achieves, let&rsquo;s return back to the index() view function in your app. py file. After declaring the todos variable, you group the results using the following code: lists = {}for k, g in groupby(todos, key=lambda t: t['title']):lists[k] = list(g)You first declare an empty dictionary called lists, then use a for loop to go through a grouping of the results in the todos variable by the list&rsquo;s title. You use the groupby() function you imported from the itertools standard library. This function will go through each item in the todos variable and generate a group of results for each key in the for loop. k represents list titles (that is, Home, Study, Work), which are extracted using the function you pass to the key parameter of the groupby() function. In this case the function is lambda t: t['title'] that takes a to-do item and returns the title of the list (as you have done before with todo['title'] in the previous for loop). g represents the group that contains the to-do items of each list title. For example, in the first iteration, k will be 'Home', while g is an iterable that will contain the items 'Buy fruit' and 'Cook dinner'. This gives us a representation of the one-to-many relationship between lists and items, where each list title has several to-do items. When running the app. py file, and after the for loop finishes execution, lists will be as follows: Output{'Home': [&lt;sqlite3. Row object at 0x7f9f58460950&gt;,     &lt;sqlite3. Row object at 0x7f9f58460c30&gt;], 'Study': [&lt;sqlite3. Row object at 0x7f9f58460b70&gt;,      &lt;sqlite3. Row object at 0x7f9f58460b50&gt;], 'Work': [&lt;sqlite3. Row object at 0x7f9f58460890&gt;]}Each sqlite3. Row object will contain the data you retrieved from the items table using the SQL query in the index() function. To represent this data better, let&rsquo;s make a program that goes through the lists dictionary and displays each list and its items. Open a file called list_example. py in your flask_todo directory: nano list_example. pyThen add the following code: flask_todo/list_example. pyfrom itertools import groupbyfrom app import get_db_connectionconn = get_db_connection()todos = conn. execute('SELECT i. content, l. title FROM items i JOIN lists l \ ON i. list_id = l. id ORDER BY l. title;'). fetchall()lists = {}for k, g in groupby(todos, key=lambda t: t['title']):lists[k] = list(g)for list*, items in lists. items():print(list*)for item in items:print(' ', item['content'])Save and close the file. This is very similar to the content in your index() view function. The last for loop here illustrates how the lists dictionary is structured. You first go through the dictionary&rsquo;s items, print the list title (which is in the list_ variable), then go through each group of to-do items that belong to the list and print the content value of the item. Run the list_example. py program: python list_example. pyHere is the output of list_example. py: OutputHome   Buy fruit   Cook dinnerStudy   Learn Flask   Learn SQLiteWork   Morning meetingNow that you understand each part of the index() function, let&rsquo;s create a base template and create the index. html file you rendered using the line return render_template('index. html', lists=lists). In your flask_todo directory, create a templates directory and open a file called base. html inside it: mkdir templatesnano templates/base. htmlAdd the following code inside base. html, note that you&rsquo;re using Bootstrap here. If you are not familiar with HTML templates in Flask, see Step 3 of How To Make a Web Application Using Flask in Python 3: flask_todo/templates/base. html&lt;!doctype html&gt;&lt;html lang= en &gt; &lt;head&gt;  &lt;!-- Required meta tags --&gt;  &lt;meta charset= utf-8 &gt;  &lt;meta name= viewport  content= width=device-width, initial-scale=1, shrink-to-fit=no &gt;  &lt;!-- Bootstrap CSS --&gt;  &lt;link rel= stylesheet  href= https://stackpath. bootstrapcdn. com/bootstrap/4. 3. 1/css/bootstrap. min. css  integrity= sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T  crossorigin= anonymous &gt;  &lt;title&gt;{ block title } { endblock }&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;nav class= navbar navbar-expand-md navbar-light bg-light &gt;&lt;a class= navbar-brand  href=  &gt;FlaskTodo&lt;/a&gt;&lt;button class= navbar-toggler  type= button  data-toggle= collapse  data-target= #navbarNav  aria-controls= navbarNav  aria-expanded= false  aria-label= Toggle navigation &gt;&lt;span class= navbar-toggler-icon &gt;&lt;/span&gt;&lt;/button&gt;&lt;div class= collapse navbar-collapse  id= navbarNav &gt;&lt;ul class= navbar-nav &gt;&lt;li class= nav-item active &gt;&lt;a class= nav-link  href= # &gt;About&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/nav&gt;&lt;div class= container &gt;{ block content } { endblock }&lt;/div&gt;  &lt;!-- Optional JavaScript --&gt;  &lt;!-- jQuery first, then Popper. js, then Bootstrap JS --&gt;  &lt;script src= https://code. jquery. com/jquery-3. 3. 1. slim. min. js  integrity= sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo  crossorigin= anonymous &gt;&lt;/script&gt;  &lt;script src= https://cdnjs. cloudflare. com/ajax/libs/popper. js/1. 14. 7/umd/popper. min. js  integrity= sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1  crossorigin= anonymous &gt;&lt;/script&gt;  &lt;script src= https://stackpath. bootstrapcdn. com/bootstrap/4. 3. 1/js/bootstrap. min. js  integrity= sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM  crossorigin= anonymous &gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;Save and close the file. Most of the code in the preceding block is standard HTML and code required for Bootstrap. The &lt;meta&gt; tags provide information for the web browser, the &lt;link&gt; tag links the Bootstrap CSS files, and the &lt;script&gt; tags are links to JavaScript code that allows some additional Bootstrap features. Check out the Bootstrap documentation for more information. Next, create the index. html file that will extend this base. html file: nano templates/index. htmlAdd the following code to index. html: flask_todo/templates/index. html{ extends 'base. html' }{ block content }&lt;h1&gt;{ block title } Welcome to FlaskTodo { endblock }&lt;/h1&gt;{ for list, items in lists. items() }&lt;div class= card  style= width: 18rem; margin-bottom: 50px; &gt;&lt;div class= card-header &gt;&lt;h3&gt;&lt;/h3&gt;&lt;/div&gt;&lt;ul class= list-group list-group-flush &gt;{ for item in items }&lt;li class= list-group-item &gt;&lt;/li&gt;{ endfor }&lt;/ul&gt;&lt;/div&gt;{ endfor }{ endblock }Here you use a for loop to go through each item of the lists dictionary, you display the list title as a card header inside an &lt;h3&gt; tag, and then use a list group to display each to-do item that belongs to the list in an &lt;li&gt; tag. This follows the same rules explained in the list_example. py program. You will now set the environment variables Flask needs and run the application using the following commands: export FLASK_APP=appexport FLASK_ENV=developmentflask runOnce the development server is running, you can visit the URL http://127. 0. 0. 1:5000/ in your browser. You will see a web page with the &ldquo;Welcome to FlaskTodo&rdquo; and your list items.  You can now type CTRL + C to stop your development server. You&rsquo;ve created a Flask application that displays the to-do lists and the items of each list. In the next step, you will add a new page for creating new to-do items. Step 3 — Adding New To-do Items: In this step, you will make a new route for creating to-do items, you will insert data into database tables, and associate items with the lists they belong to. First, open the app. py file: nano app. pyThen, add a new /create route with a view function called create() at the end of the file: flask_todo/app. py. . . @app. route('/create/', methods=('GET', 'POST'))def create():  conn = get_db_connection()  lists = conn. execute('SELECT title FROM lists;'). fetchall()  conn. close()  return render_template('create. html', lists=lists)Save and close the file. Because you will use this route to insert new data to the database via a web form, you allow both GET and POST requests using methods=('GET', 'POST') in the app. route() decorator. In the create() view function, you open a database connection then get all the list titles available in the database, close the connection, and render a create. html template passing it the list titles. Next, open a new template file called create. html: nano templates/create. htmlAdd the following HTML code to create. html: flask_todo/templates/create. html{ extends 'base. html' }{ block content }&lt;h1&gt;{ block title } Create a New Item { endblock }&lt;/h1&gt;&lt;form method= post &gt;&lt;div class= form-group &gt;&lt;label for= content &gt;Content&lt;/label&gt;&lt;input type= text  name= content placeholder= Todo content  class= form-control value=  &gt;&lt;/input&gt;&lt;/div&gt;  &lt;div class= form-group &gt;    &lt;label for= list &gt;List&lt;/label&gt;    &lt;select class= form-control  name= list &gt;      { for list in lists }        { if list['title'] == request. form['list'] }          &lt;option value=   selected&gt;                      &lt;/option&gt;        { else }          &lt;option value=  &gt;                      &lt;/option&gt;        { endif }      { endfor }    &lt;/select&gt;  &lt;/div&gt;  &lt;div class= form-group &gt;    &lt;button type= submit  class= btn btn-primary &gt;Submit&lt;/button&gt;  &lt;/div&gt;&lt;/form&gt;{ endblock }Save and close the file. You use request. form to access the form data that is stored in case something goes wrong with your form submission (for example, if no to-do content was provided). In the &lt;select&gt; element, you loop through the lists you retrieved from the database in the create() function. If the list title is equal to what is stored in request. form then the selected option is that list title, otherwise, you display the list title in a normal non-selected &lt;option&gt; tag. Now, in the terminal, run your Flask application: flask runThen visit http://127. 0. 0. 1:5000/create in your browser, you will see a form for creating a new to-do item, note that the form doesn&rsquo;t work yet because you have no code to handle POST requests that get sent by the browser when submitting the form. Type CTRL + C to stop your development server. Next, let&rsquo;s add the code for handling POST requests to the create() function and make the form function properly, open app. py: Save and close the file. Inside the request. method == 'POST' condition you get the to-do item&rsquo;s content and the list&rsquo;s title from the form data. If no content was submitted, you send the user a message using the flash() function and redirect to the index page. If this condition was not triggered, then you execute a SELECT statement to get the list ID from the provided list title and save it in a variable called list_id. You then execute an INSERT INTO statement to insert the new to-do item into the items table. You use the list_id variable to link the item to the list it belongs to. Finally, you commit the transaction, close the connection, and redirect to the index page. As a last step, you will add a link to /create in the navigation bar and display flashed messages below it, to do this, open base. html: Edit the file by adding a new &lt;li&gt; navigation item that links to the create() view function. Then display the flashed messages using a for loop above the content block. These are available in the get_flashed_messages() Flask function: flask_todo/templates/base. html&lt;nav class= navbar navbar-expand-md navbar-light bg-light &gt;  &lt;a class= navbar-brand  href=  &gt;FlaskTodo&lt;/a&gt;  &lt;button class= navbar-toggler  type= button  data-toggle= collapse  data-target= #navbarNav  aria-controls= navbarNav  aria-expanded= false  aria-label= Toggle navigation &gt;    &lt;span class= navbar-toggler-icon &gt;&lt;/span&gt;  &lt;/button&gt;  &lt;div class= collapse navbar-collapse  id= navbarNav &gt;    &lt;ul class= navbar-nav &gt;    &lt;li class= nav-item active &gt;      &lt;a class= nav-link  href=  &gt;New&lt;/a&gt;    &lt;/li&gt;    &lt;li class= nav-item active &gt;      &lt;a class= nav-link  href= # &gt;About&lt;/a&gt;    &lt;/li&gt;    &lt;/ul&gt;  &lt;/div&gt;&lt;/nav&gt;&lt;div class= container &gt;{ for message in get_flashed_messages() } &lt;div class= alert alert-danger &gt;&lt;/div&gt;{ endfor }{block content } { endblock }&lt;/div&gt;Save and close the file. Now, in the terminal, run your Flask application: flask runA new link to /create will appear in the navigation bar. If you navigate to this page and try to add a new to-do item with no content, you&rsquo;ll receive a flashed message saying Content is required!. If you fill in the content form, a new to-do item will appear on the index page. In this step, you have added the ability to create new to-do items and save them to the database. You can find the source code for this project in this repository. Conclusion: You now have an application to manage to-do lists and items. Each list has several to-do items and each to-do item belongs to a single list in a one-to-many relationship. You learned how to use Flask and SQLite to manage multiple related database tables, how to use foreign keys and how to retrieve and display related data from two tables in a web application using SQLite joins. Furthermore, you grouped results using the groupby() function, inserted new data to the database, and associated database table rows with the tables they are related to. You can learn more about foreign keys and database relationships from the SQLite documentation. You can also read more of our Python Framework content. If you want to check out the sqlite3 Python module, read our tutorial on How To Use the sqlite3 Module in Python 3. "
    }, {
    "id": 62,
    "url": "https://bright-softwares.com/blog/en/javascript/how-to-manage-state-on-react-class-components",
    "title": "How To Manage State on React Class Components",
    "body": "2020/07/05 - The author selected Creative Commons to receive a donation as part of the Write for DOnations program. Introduction: In React, state refers to a structure that keeps track of how data changes over time in your application. Managing state is a crucial skill in React because it allows you to make interactive components and dynamic web applications. State is used for everything from tracking form inputs to capturing dynamic data from an API. In this tutorial, you&rsquo;ll run through an example of managing state on class-based components. As of the writing of this tutorial, the official React documentation encourages developers to adopt React Hooks to manage state with functional components when writing new code, rather than using class-based components. Although the use of React Hooks is considered a more modern practice, it&rsquo;s important to understand how to manage state on class-based components as well. Learning the concepts behind state management will help you navigate and troubleshoot class-based state management in existing code bases and help you decide when class-based state management is more appropriate. There&rsquo;s also a class-based method called componentDidCatch that is not available in Hooks and will require setting state using class methods. This tutorial will first show you how to set state using a static value, which is useful for cases where the next state does not depend on the first state, such as setting data from an API that overrides old values. Then it will run through how to set a state as the current state, which is useful when the next state depends on the current state, such as toggling a value. To explore these different ways of setting state, you&rsquo;ll create a product page component that you&rsquo;ll update by adding purchases from a list of options. Prerequisites: You will need a development environment running Node. js; this tutorial was tested on Node. js version 10. 20. 1 and npm version 6. 14. 4. To install this on macOS or Ubuntu 18. 04, follow the steps in How to Install Node. js and Create a Local Development Environment on macOS or the Installing Using a PPA section of How To Install Node. js on Ubuntu 18. 04. In this tutorial, you will create apps with Create React App. You can find instructions for installing an application with Create React App at How To Set Up a React Project with Create React App. You will also need a basic knowledge of JavaScript, which you can find in How To Code in JavaScript, along with a basic knowledge of HTML and CSS. A good resource for HTML and CSS is the Mozilla Developer Network. Step 1 — Creating an Empty Project: In this step, you&rsquo;ll create a new project using Create React App. Then you will delete the sample project and related files that are installed when you bootstrap the project. Finally, you will create a simple file structure to organize your components. This will give you a solid basis on which to build this tutorial&rsquo;s sample application for managing state on class-based components. To start, make a new project. In your terminal, run the following script to install a fresh project using create-react-app: npx create-react-app state-class-tutorialAfter the project is finished, change into the directory: cd state-class-tutorialIn a new terminal tab or window, start the project using the Create React App start script. The browser will auto-refresh on changes, so leave this script running while you work: npm startYou will get a running local server. If the project did not open in a browser window, you can open it with http://localhost:3000/. If you are running this from a remote server, the address will be http://your_domain:3000. Your browser will load with a simple React application included as part of Create React App: You will be building a completely new set of custom components, so you&rsquo;ll need to start by clearing out some boilerplate code so that you can have an empty project. To start, open src/App. js in a text editor. This is the root component that is injected into the page. All components will start from here. You can find more information about App. js at How To Set Up a React Project with Create React App. Open src/App. js with the following command: nano src/App. jsYou will see a file like this: state-class-tutorial/src/App. jsimport React from 'react';import logo from '. /logo. svg';import '. /App. css';function App() {return (&lt;div className= App &gt;&lt;header className= App-header &gt;&lt;img src={logo} className= App-logo  alt= logo  /&gt;&lt;p&gt;Edit &lt;code&gt;src/App. js&lt;/code&gt; and save to reload. &lt;/p&gt;&lt;aclassName= App-link href= https://reactjs. org target= \_blank rel= noopener noreferrer &gt;Learn React&lt;/a&gt;&lt;/header&gt;&lt;/div&gt;);}export default App;Delete the line import logo from '. /logo. svg';. Then replace everything in the return statement to return a set of empty tags: &lt;&gt;&lt;/&gt;. This will give you a valid page that returns nothing. The final code will look like this: state-class-tutorial/src/App. jsimport React from 'react';import '. /App. css';function App() {return &lt;&gt;&lt;/&gt;;}export default App;Save and exit the text editor. Finally, delete the logo. You won&rsquo;t be using it in your application and you should remove unused files as you work. It will save you from confusion in the long run. In the terminal window type the following command: rm src/logo. svgIf you look at your browser, you will see a blank screen.  Now that you have cleared out the sample Create React App project, create a simple file structure. This will help you keep your components isolated and independent. Create a directory called components in the src directory. This will hold all of your custom components. mkdir src/componentsEach component will have its own directory to store the component file along with the styles, images, and tests. Create a directory for App: mkdir src/components/AppMove all of the App files into that directory. Use the wildcard, *, to select any files that start with App. regardless of file extension. Then use the mv command to put them into the new directory: mv src/App. * src/components/AppNext, update the relative import path in index. js, which is the root component that bootstraps the whole process: nano src/index. jsThe import statement needs to point to the App. js file in the App directory, so make the following highlighted change: state-class-tutorial/src/index. jsimport React from 'react';import ReactDOM from 'react-dom';import '. /index. css';import App from '. /components/App/App';import * as serviceWorker from '. /serviceWorker';ReactDOM. render(&lt;React. StrictMode&gt;&lt;App /&gt;&lt;/React. StrictMode&gt;,document. getElementById('root'));// If you want your app to work offline and load faster, you can change// unregister() to register() below. Note this comes with some pitfalls. // Learn more about service workers: https://bit. ly/CRA-PWAserviceWorker. unregister();Save and exit the file. Now that the project is set up, you can create your first component. Step 2 — Using State in a Component: In this step, you&rsquo;ll set the initial state of a component on its class and reference the state to display a value. You&rsquo;ll then make a product page with a shopping cart that displays the total items in the cart using the state value. By the end of the step, you&rsquo;ll know the different ways to hold a value and when you should use state rather than a prop or a static value. Building the Components: Start by creating a directory for Product: mkdir src/components/ProductNext, open up Product. js in that directory: nano src/components/Product/Product. jsStart by creating a component with no state. The component will have two parts: The cart, which has the number of items and the total price, and the product, which has a button to add and remove an item. For now, the buttons will have no actions. Add the following code to Product. js: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: 0 total items. &lt;/div&gt;&lt;div&gt;Total: 0&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button&gt;Add&lt;/button&gt; &lt;button&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}You have also included a couple of div elements that have JSX class names so you can add some basic styling. Save and close the file, then open Product. css: nano src/components/Product/Product. cssGive some light styling to increase the font-size for the text and the emoji: state-class-tutorial/src/components/Product/Product. css. product span {  font-size: 100px;}. wrapper {padding: 20px;font-size: 20px;}. wrapper button {font-size: 20px;background: none;}The emoji will need a much larger font size than the text, since it&rsquo;s acting as the product image in this example. In addition, you are removing the default gradient background on buttons by setting the background to none.  Save and close the file. Now, render the Product component in the App component so you can see the results in the browser. Open App. js: nano src/components/App/App. jsImport the component and render it. You can also delete the CSS import since you won&rsquo;t be using it in this tutorial: state-class-tutorial/src/components/App/App. jsimport React from 'react';import Product from '. . /Product/Product';function App() {return &lt;Product /&gt;}export default App;Save and close the file. When you do, the browser will refresh and you&rsquo;ll see the Product component.  Setting the Initial State on a Class Component: There are two values in your component values that are going to change in your display: total number of items and total cost. Instead of hard coding them, in this step you&rsquo;ll move them into an object called state. The state of a React class is a special property that controls the rendering of a page. When you change the state, React knows that the component is out-of-date and will automatically re-render. When a component re-renders, it modifies the rendered output to include the most up-to-date information in state. In this example, the component will re-render whenever you add a product to the cart or remove it from the cart. You can add other properties to a React class, but they won&rsquo;t have the same ability to trigger re-rendering. Open Product. js: nano src/components/Product/Product. jsAdd a property called state to the Product class. Then add two values to the state object: cart and total. The cart will be an array, since it may eventually hold many items. The total will be a number. After assigning these, replace references to the values with this. state. property: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {state = {cart: [],total: 0}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. state. total}&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button&gt;Add&lt;/button&gt; &lt;button&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}Notice that in both cases, since you are referencing JavaScript inside of your JSX, you need to wrap the code in curly braces. You are also displaying the length of the cart array to get a count of the number of items in the array. Save the file. When you do, the browser will refresh and you&rsquo;ll see the same page as before.  The state property is a standard class property, which means that it is accessible in other methods, not just the render method. Next, instead of displaying the price as a static value, convert it to a string using the toLocaleString method, which will convert the number to a string that matches the way numbers are displayed in the browser&rsquo;s region. Create a method called getTotal() that takes the state and converts it to a localized string using an array of currencyOptions. Then, replace the reference to state in the JSX with a method call: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {state = {cart: [],total: 0}currencyOptions = {minimumFractionDigits: 2,maximumFractionDigits: 2,}getTotal = () =&gt; {return this. state. total. toLocaleString(undefined, this. currencyOptions)}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button&gt;Add&lt;/button&gt; &lt;button&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}Since total is a price for goods, you are passing currencyOptions that set the maximum and minimum decimal places for your total to two. Note that this is set as a separate property. Often, beginner React developers will put information like this in the state object, but it is best to only add information to state that you expect to change. This way, the information in state will be easier to keep strack of as your application scales. Another important change you made was to create the getTotal() method by assigning an arrow function to a class property. Without using the arrow function, this method would create a new this binding, which would interfere with the current this binding and introduce a bug into our code. You&rsquo;ll see more on this in the next step. Save the file. When you do, the page will refresh and you&rsquo;ll see the value converted to a decimal.  You&rsquo;ve now added state to a component and referenced it in your class. You also accessed values in the render method and in other class methods. Next, you&rsquo;ll create methods to update the state and show dynamic values.  Step 3 — Setting State from a Static Value: So far you&rsquo;ve created a base state for the component and you&rsquo;ve referenced that state in your functions and your JSX code. In this step, you&rsquo;ll update your product page to modify the state on button clicks. You&rsquo;ll learn how to pass a new object containing updated values to a special method called setState, which will then set the state with the updated data. To update state, React developers use a special method called setState that is inherited from the base Component class. The setState method can take either an object or a function as the first argument. If you have a static value that doesn&rsquo;t need to reference the state, it&rsquo;s best to pass an object containing the new value, since it&rsquo;s easier to read. If you need to reference the current state, you pass a function to avoid any references to out-of-date state. Start by adding an event to the buttons. If your user clicks Add, then the program will add the item to the cart and update the total. If they click Remove, it will reset the cart to an empty array and the total to 0. For example purposes, the program will not allow a user to add an item more then once. Open Product. js: nano src/components/Product/Product. jsInside the component, create a new method called add, then pass the method to the onClick prop for the Add button: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {state = {cart: [],total: 0}add = () =&gt; {this. setState({cart: ['ice cream'],total: 5})}currencyOptions = {minimumFractionDigits: 2,maximumFractionDigits: 2,}getTotal = () =&gt; {return this. state. total. toLocaleString(undefined, this. currencyOptions)}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button onClick={this. add}&gt;Add&lt;/button&gt;    &lt;button&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}Inside the add method, you call the setState method and pass an object containing the updated cart with a single item ice cream and the updated price of 5. Notice that you again used an arrow function to create the add method. As mentioned before, this will ensure the function has the proper this context when running the update. If you add the function as a method without using the arrow function, the setState would not exist without binding the function to the current context. For example, if you created the add function this way: export default class Product extends Component {. . .  add() {  this. setState({   cart: ['ice cream'],   total: 5  }) }. . . }The user would get an error when they click on the Add button.  Using an arrow function ensures that you&rsquo;ll have the proper context to avoid this error. Save the file. When you do, the browser will reload, and when you click on the Add button the cart will update with the current amount.  With the add method, you passed both properties of the state object: cart and total. However, you do not always need to pass a complete object. You only need to pass an object containing the properties that you want to update, and everything else will stay the same. To see how React can handle a smaller object, create a new function called remove. Pass a new object containing just the cart with an empty array, then add the method to the onClick property of the Remove button: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';export default class Product extends Component {. . . remove = () =&gt; {this. setState({cart: []})}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;    &lt;div className= product &gt;&lt;span role= img  aria-label= ice cream &gt;🍦&lt;/span&gt;&lt;/div&gt;    &lt;button onClick={this. add}&gt;Add&lt;/button&gt;    &lt;button onClick={this. remove}&gt;Remove&lt;/button&gt;   &lt;/div&gt;  )}}Save the file. When the browser refreshes, click on the Add and Remove buttons. You&rsquo;ll see the cart update, but not the price. The total state value is preserved during the update. This value is only preserved for example purposes; with this application, you would want to update both properties of the state object. But you will often have components with stateful properties that have different responsibilities, and you can make them persist by leaving them out of the updated object. The change in this step was static. You knew exactly what the values would be ahead of time, and they didn&rsquo;t need to be recalculated from state. But if the product page had many products and you wanted to be able to add them multiple times, passing a static object would provide no guarantee of referencing the most up-to-date state, even if your object used a this. state value. In this case, you could instead use a function. In the next step, you&rsquo;ll update state using functions that reference the current state. Step 4 — Setting State Using Current State: There are many times when you&rsquo;ll need to reference a previous state to update a current state, such as updating an array, adding a number, or modifying an object. To be as accurate as possible, you need to reference the most up-to-date state object. Unlike updating state with a predefined value, in this step you&rsquo;ll pass a function to the setState method, which will take the current state as an argument. Using this method, you will update a component&rsquo;s state using the current state. Another benefit of setting state with a function is increased reliability. To improve performance, React may batch setState calls, which means that this. state. value may not be fully reliable. For example, if you update state quickly in several places, it is possible that a value could be out of date. This can happen during data fetches, form validations, or any situation where several actions are occurring in parallel. But using a function with the most up-to-date state as the argument ensures that this bug will not enter your code. To demonstrate this form of state management, add some more items to the product page. First, open the Product. js file: nano src/components/Product/Product. jsNext, create an array of objects for different products. The array will contain the product emoji, name, and price. Then loop over the array to display each product with an Add and Remove button: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';const products = [{emoji: '🍦',name: 'ice cream',price: 5},{emoji: '🍩',name: 'donuts',price: 2. 5,},{emoji: '🍉',name: 'watermelon',price: 4}];export default class Product extends Component {. . . render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;&lt;div&gt;{products. map(product =&gt; (&lt;div key={product. name}&gt;&lt;div className= product &gt;&lt;span role= img  aria-label={product. name}&gt;{product. emoji}&lt;/span&gt;&lt;/div&gt;&lt;button onClick={this. add}&gt;Add&lt;/button&gt;&lt;button onClick={this. remove}&gt;Remove&lt;/button&gt;&lt;/div&gt;))}&lt;/div&gt;&lt;/div&gt;)}}In this code, you are using the map() array method to loop over the products array and return the JSX that will display each element in your browser. Save the file. When the browser reloads, you&rsquo;ll see an updated product list: Now you need to update your methods. First, change the add() method to take the product as an argument. Then instead of passing an object to setState(), pass a function that takes the state as an argument and returns an object that has the cart updated with the new product and the total updated with the new price: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';. . . export default class Product extends Component {state = {cart: [],total: 0}add = (product) =&gt; {this. setState(state =&gt; ({cart: [. . . state. cart, product. name],total: state. total + product. price}))}currencyOptions = {minimumFractionDigits: 2,maximumFractionDigits: 2,}getTotal = () =&gt; {return this. state. total. toLocaleString(undefined, this. currencyOptions)}remove = () =&gt; {this. setState({cart: []})}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;    &lt;div&gt;     {products. map(product =&gt; (      &lt;div key={product. name}&gt;       &lt;div className= product &gt;        &lt;span role= img  aria-label={product. name}&gt;{product. emoji}&lt;/span&gt;       &lt;/div&gt;       &lt;button onClick={() =&gt; this. add(product)}&gt;Add&lt;/button&gt;       &lt;button onClick={this. remove}&gt;Remove&lt;/button&gt;      &lt;/div&gt;     ))}    &lt;/div&gt;   &lt;/div&gt;  )}}Inside the anonymous function that you pass to setState(), make sure you reference the argument—state—and not the component&rsquo;s state—this. state. Otherwise, you still run a risk of getting an out-of-date state object. The state in your function will be otherwise identical. Take care not to directly mutate state. Instead, when adding a new value to the cart, you can add the new product to the state by using the spread syntax on the current value and adding the new value onto the end. Finally, update the call to this. add by changing the onClick() prop to take an anonymous function that calls this. add() with the relevant product. Save the file. When you do, the browser will reload and you&rsquo;ll be able to add multiple products.  Next, update the remove() method. Follow the same steps: convert setState to take a function, update the values without mutating, and update the onChange() prop: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';. . . export default class Product extends Component {. . . remove = (product) =&gt; {this. setState(state =&gt; {const cart = [. . . state. cart];cart. splice(cart. indexOf(product. name))return ({cart,total: state. total - product. price})})}render() {return(&lt;div className= wrapper &gt;&lt;div&gt;Shopping Cart: {this. state. cart. length} total items. &lt;/div&gt;&lt;div&gt;Total {this. getTotal()}&lt;/div&gt;&lt;div&gt;{products. map(product =&gt; (&lt;div key={product. name}&gt;&lt;div className= product &gt;&lt;span role= img  aria-label={product. name}&gt;{product. emoji}&lt;/span&gt;&lt;/div&gt;&lt;button onClick={() =&gt; this. add(product)}&gt;Add&lt;/button&gt;&lt;button onClick={() =&gt; this. remove(product)}&gt;Remove&lt;/button&gt;&lt;/div&gt;))}&lt;/div&gt;&lt;/div&gt;)}}To avoid mutating the state object, you must first make a copy of it using the spread operator. Then you can splice out the item you want from the copy and return the copy in the new object. By copying state as the first step, you can be sure that you will not mutate the state object. Save the file. When you do, the browser will refresh and you&rsquo;ll be able to add and remove items: There is still a bug in this application: In the remove method, a user can subtract from the total even if the item is not in the cart. If you click Remove on the ice cream without adding it to your cart, your total will be -5. 00. You can fix the bug by checking for an item&rsquo;s existence before subtracting, but an easier way is to keep your state object small by only keeping references to the products and not separating references to products and total cost. Try to avoid double references to the same data. Instead, store the raw data in state— in this case the whole product object—then perform the calculations outside of the state. Refactor the component so that the add() method adds the whole object, the remove() method removes the whole object, and the getTotal method uses the cart: state-class-tutorial/src/components/Product/Product. jsimport React, { Component } from 'react';import '. /Product. css';. . . export default class Product extends Component {state = {cart: [],}add = (product) =&gt; {this. setState(state =&gt; ({cart: [. . . state. cart, product],}))}currencyOptions = {minimumFractionDigits: 2,maximumFractionDigits: 2,}getTotal = () =&gt; {const total = this. state. cart. reduce((totalCost, item) =&gt; totalCost + item. price, 0);return total. toLocaleString(undefined, this. currencyOptions)}remove = (product) =&gt; {this. setState(state =&gt; {const cart = [. . . state. cart];const productIndex = cart. findIndex(p =&gt; p. name === product. name);if(productIndex &lt; 0) {return;}cart. splice(productIndex, 1)return ({cart})})}render() {. . . }}The add() method is similar to what it was before, except that reference to the total property has been removed. In the remove() method, you find the index of the product with findByIndex. If the index doesn&rsquo;t exist, you&rsquo;ll get a -1. In that case, you use a conditional statement toreturn nothing. By returning nothing, React will know the state didn&rsquo;t change and won&rsquo;t trigger a re-render. If you return state or an empty object, it will still trigger a re-render. When using the splice() method, you are now passing 1 as the second argument, which will remove one value and keep the rest. Finally, you calculate the total using the reduce() array method. Save the file. When you do, the browser will refresh and you&rsquo;ll have your final cart: The setState function you pass can have an additional argument of the current props, which can be helpful if you have state that needs to reference the current props. You can also pass a callback function to setState as the second argument, regardless of if you pass an object or function for the first argument. This is particularly useful when you are setting state after fetching data from an API and you need to perform a new action after the state update is complete. In this step, you learned how to update a new state based on the current state. You passed a function to the setState function and calculated new values without mutating the current state. You also learned how to exit a setState function if there is no update in a manner that will prevent a re-render, adding a slight performance enhancement. Conclusion: In this tutorial, you have developed a class-based component with a dynamic state that you&rsquo;ve updated statically and using the current state. You now have the tools to make complex projects that respond to users and dynamic information.  React does have a way to manage state with Hooks, but it is helpful to understand how to use state on components if you need to work with components that must be class-based, such as those that use the componentDidCatch method. Managing state is key to nearly all components and is necessary for creating interactive applications. With this knowledge you can recreate many common web components, such as sliders, accordions, forms, and more. You will then use the same concepts as you build applications using hooks or develop components that pull data dynamically from APIs. If you would like to look at more React tutorials, check out our React Topic page, or return to the How To Code in React. js series page. "
    }, {
    "id": 63,
    "url": "https://bright-softwares.com/blog/en/ubuntu/how-to-create-a-selfsigned-ssl-certificate-for-apache-on-centos-8",
    "title": "How To Create a Self-Signed SSL Certificate for Apache on CentOS 8",
    "body": "2020/07/05 - Introduction: TLS, or &ldquo;transport layer security&rdquo; — and its predecessor SSL — are protocols used to wrap normal traffic in a protected, encrypted wrapper. Using this technology, servers can safely send information to their clients without their messages being intercepted or read by an outside party. In this guide, we will show you how to create and use a self-signed SSL certificate with the Apache web server on a CentOS 8 machine. &lt;p&gt;Note: A self-signed certificate will encrypt communication between your server and its clients. However, because it is not signed by any of the trusted certificate authorities included with web browsers and operating systems, users cannot use the certificate to automatically validate the identity of your server. As a result, your users will see a security error when visiting your site. &lt;/p&gt; Because of this limitation, self-signed certificates are not appropriate for a production environment serving the public. They are typically used for testing, or for securing non-critical services used by a single user or a small group of users that can establish trust in the certificate&rsquo;s validity through alternate communication channels. For a more production-ready certificate solution, check out Let&rsquo;s Encrypt, a free certificate authority. You can learn how to download and configure a Let&rsquo;s Encrypt certificate in our How To Secure Apache with Let&rsquo;s Encrypt on CentOS 8 tutorial. &lt;/span&gt; Prerequisites: Before starting this tutorial, you&rsquo;ll need the following: Access to a CentOS 8 server with a non-root, sudo-enabled user. Our Initial Server Setup with CentOS 8 guide can show you how to create this account. You will also need to have Apache installed. You can install Apache using dnf: sudo dnf install httpdEnable Apache and start it using systemctl: sudo systemctl enable httpdsudo systemctl start httpdAnd finally, if you have a firewalld firewall set up, open up the http and https ports: sudo firewall-cmd --permanent --add-service=httpsudo firewall-cmd --permanent --add-service=httpssudo firewall-cmd --reloadAfter these steps are complete, be sure you are logged in as your non-root user and continue with the tutorial. Step 1 — Installing mod_ssl: We first need to install mod_ssl, an Apache module that provides support for SSL encryption. Install mod_ssl with the dnf command: sudo dnf install mod_sslBecause of a packaging bug, we need to restart Apache once to properly generate the default SSL certificate and key, otherwise we&rsquo;ll get an error reading '/etc/pki/tls/certs/localhost. crt' does not exist or is empty. sudo systemctl restart httpdThe mod_ssl module is now enabled and ready for use. Step 2 — Creating the SSL Certificate: Now that Apache is ready to use encryption, we can move on to generating a new SSL certificate. The certificate will store some basic information about your site, and will be accompanied by a key file that allows the server to securely handle encrypted data. We can create the SSL key and certificate files with the openssl command: sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/pki/tls/private/apache-selfsigned. key -out /etc/pki/tls/certs/apache-selfsigned. crtAfter you enter the command, you will be taken to a prompt where you can enter information about your website. Before we go over that, let&rsquo;s take a look at what is happening in the command we are issuing: openssl: This is the command line tool for creating and managing OpenSSL certificates, keys, and other files. req -x509: This specifies that we want to use X. 509 certificate signing request (CSR) management. X. 509 is a public key infrastructure standard that SSL and TLS adhere to for key and certificate management. -nodes: This tells OpenSSL to skip the option to secure our certificate with a passphrase. We need Apache to be able to read the file, without user intervention, when the server starts up. A passphrase would prevent this from happening, since we would have to enter it after every restart. -days 365: This option sets the length of time that the certificate will be considered valid. We set it for one year here. Many modern browsers will reject any certificates that are valid for longer than one year. -newkey rsa:2048: This specifies that we want to generate a new certificate and a new key at the same time. We did not create the key that is required to sign the certificate in a previous step, so we need to create it along with the certificate. The rsa:2048 portion tells it to make an RSA key that is 2048 bits long. -keyout: This line tells OpenSSL where to place the generated private key file that we are creating. -out: This tells OpenSSL where to place the certificate that we are creating. Fill out the prompts appropriately. The most important line is the one that requests the Common Name. You need to enter either the hostname you&rsquo;ll use to access the server by, or the public IP of the server. It&rsquo;s important that this field matches whatever you&rsquo;ll put into your browser&rsquo;s address bar to access the site, as a mismatch will cause more security errors. The full list of prompts will look something like this: Country Name (2 letter code) [XX]:USState or Province Name (full name) []:ExampleLocality Name (eg, city) [Default City]:Example Organization Name (eg, company) [Default Company Ltd]:Example IncOrganizational Unit Name (eg, section) []:Example DeptCommon Name (eg, your name or your server's hostname) []:your_domain_or_ipEmail Address []:webmaster@example. comBoth of the files you created will be placed in the appropriate subdirectories of the /etc/pki/tls directory. This is a standard directory provided by CentOS for this purpose. Next we will update our Apache configuration to use the new certificate and key. Step 3 — Configuring Apache to Use SSL: Now that we have our self-signed certificate and key available, we need to update our Apache configuration to use them. On CentOS, you can place new Apache configuration files (they must end in . conf) into /etc/httpd/conf. d and they will be loaded the next time the Apache process is reloaded or restarted. For this tutorial we will create a new minimal configuration file. If you already have an Apache &lt;Virtualhost&gt; set up and just need to add SSL to it, you will likely need to copy over the configuration lines that start with SSL, and switch the VirtualHost port from 80 to 443. We will take care of port 80 in the next step. Open a new file in the /etc/httpd/conf. d directory: sudo vi /etc/httpd/conf. d/your_domain_or_ip. confPaste in the following minimal VirtualHost configuration: /etc/httpd/conf. d/your_domain_or_ip. conf&lt;VirtualHost *:443&gt;  ServerName your_domain_or_ip  DocumentRoot /var/www/ssl-test  SSLEngine on  SSLCertificateFile /etc/pki/tls/certs/apache-selfsigned. crt  SSLCertificateKeyFile /etc/pki/tls/private/apache-selfsigned. key&lt;/VirtualHost&gt;Be sure to update the ServerName line to however you intend to address your server. This can be a hostname, full domain name, or an IP address. Make sure whatever you choose matches the Common Name you chose when making the certificate. The remaining lines specify a DocumentRoot directory to serve files from, and the SSL options needed to point Apache to our newly-created certificate and key. Now let&rsquo;s create our DocumentRoot and put an HTML file in it just for testing purposes: sudo mkdir /var/www/ssl-testOpen a new index. html file with your text editor: sudo vi /var/www/ssl-test/index. htmlPaste the following into the blank file: /var/www/ssl-test/index. html&lt;h1&gt;it worked!&lt;/h1&gt;This is not a full HTML file, of course, but browsers are lenient and it will be enough to verify our configuration. Save and close the file, then check your Apache configuration for syntax errors by typing: sudo apachectl configtestYou may see some warnings, but as long as the output ends with Syntax OK, you are safe to continue. If this is not part of your output, check the syntax of your files and try again. When all is well, reload Apache to pick up the configuration changes: sudo systemctl reload httpdNow load your site in a browser, being sure to use https:// at the beginning.  You should see an error. This is normal for a self-signed certificate! The browser is warning you that it can&rsquo;t verify the identity of the server, because our certificate is not signed by any of the browser&rsquo;s known certificate authorities. For testing purposes and personal use this can be fine. You should be able to click through to advanced or more information and choose to proceed. After you do so, your browser will load the it worked! message. &lt;p&gt;Note: if your browser doesn’t connect at all to the server, make sure your connection isn’t being blocked by a firewall. If you are using firewalld, the following commands will open ports 80 and 443:&lt;/p&gt; sudo firewall-cmd --permanent --add-service=httpsudo firewall-cmd --permanent --add-service=httpssudo firewall-cmd --reload &lt;/span&gt; Next we will add another VirtualHost section to our configuration to serve plain HTTP requests and redirect them to HTTPS. Step 4 — Redirecting HTTP to HTTPS: Currently, our configuration will only respond to HTTPS requests on port 443. It is good practice to also respond on port 80, even if you want to force all traffic to be encrypted. Let&rsquo;s set up a VirtualHost to respond to these unencrypted requests and redirect them to HTTPS. Open the same Apache configuration file we started in previous steps: sudo vi /etc/httpd/conf. d/your_domain_or_ip. confAt the bottom, create another VirtualHost block to match requests on port 80. Use the ServerName directive to again match your domain name or IP address. Then, use Redirect to match any requests and send them to the SSL VirtualHost. Make sure to include the trailing slash: /etc/httpd/conf. d/your_domain_or_ip. conf&lt;VirtualHost *:80&gt;  ServerName your_domain_or_ip  Redirect / https://your_domain_or_ip/&lt;/VirtualHost&gt;Save and close this file when you are finished, then test your configuration syntax again, and reload Apache: sudo apachectl configtestsudo systemctl reload httpdYou can test the new redirect functionality by visiting your site with plain http:// in front of the address. You should be redirected to https:// automatically. Conclusion: You have now configured Apache to serve encrypted requests using a self-signed SSL certificate, and to redirect unecrypted HTTP requests to HTTPS. If you are planning on using SSL for a public website, you should look into purchasing a domain name and using a widely supported certificate authority such as Let&rsquo;s Encrypt. For more information on using Let&rsquo;s Encrypt with Apache, please read our How To Secure Apache with Let&rsquo;s Encrypt on CentOS 8 tutorial. "
    }, {
    "id": 64,
    "url": "https://bright-softwares.com/blog/en/ubuntu/how-to-create-a-new-sudoenabled-user-on-ubuntu-2004-quickstart",
    "title": "How To Create a New Sudo-enabled User on Ubuntu 20.04 [Quickstart]",
    "body": "2020/07/05 - Introduction: When managing a server, you’ll sometimes want to allow users to execute commands as “root,” the administrator-level user. The sudo command provides system administrators with a way to grant administrator privileges — ordinarily only available to the root user — to normal users.  In this tutorial, you’ll learn how to create a new user with sudo access on Ubuntu 20. 04 without having to modify your server&rsquo;s /etc/sudoers file.  Note: If you want to configure sudo for an existing user, skip to step 3. Step 1 — Logging Into Your Server: SSH in to your server as the root user: ssh root@your_server_ip_addressStep 2 — Adding a New User to the System: Use the adduser command to add a new user to your system: adduser sammyBe sure to replace sammy with the username that you want to create. You will be prompted to create and verify a password for the user: OutputEnter new UNIX password:Retype new UNIX password:passwd: password updated successfullyNext, you&rsquo;ll be asked to fill in some information about the new user. It is fine to accept the defaults and leave this information blank: OutputChanging the user information for sammyEnter the new value, or press ENTER for the default  Full Name []:  Room Number []:  Work Phone []:  Home Phone []:  Other []:Is the information correct? [Y/n]Step 3 — Adding the User to the sudo Group: Use the usermod command to add the user to the sudo group: usermod -aG sudo sammyAgain, be sure to replace sammy with the username you just added. By default on Ubuntu, all members of the sudo group have full sudo privileges. Step 4 — Testing sudo Access: To test that the new sudo permissions are working, first use the su command to switch to the new user account: su - sammyAs the new user, verify that you can use sudo by prepending sudo to the command that you want to run with superuser privileges: sudo command_to_runFor example, you can list the contents of the /root directory, which is normally only accessible to the root user: sudo ls -la /rootThe first time you use sudo in a session, you will be prompted for the password of that user’s account. Enter the password to proceed: Output:[sudo] password for sammy:Note: This is not asking for the root password! Enter the password of the sudo-enabled user you just created. If your user is in the proper group and you entered the password correctly, the command that you issued with sudo will run with root privileges. Conclusion: In this quickstart tutorial, we created a new user account and added it to the sudo group to enable sudo access.  For your new user to be granted external access, please follow our section on Enabling External Access for Your Regular User. If you need more detailed information on setting up an Ubuntu 20. 04 server, please read our Initial Server Setup with Ubuntu 20. 04 tutorial. "
    }, {
    "id": 65,
    "url": "https://bright-softwares.com/blog/en/docker/how-to-configure-jenkins-with-ssl-using-an-nginx-reverse-proxy-on-ubuntu-2004",
    "title": "How To Configure Jenkins with SSL Using an Nginx Reverse Proxy on Ubuntu 20.04",
    "body": "2020/07/05 - Introduction: By default, Jenkins comes with its own built-in Winstone web server listening on port 8080, which is convenient for getting started. It&rsquo;s also a good idea, however, to secure Jenkins with SSL to protect passwords and sensitive data transmitted through the web interface.  In this tutorial, you will configure Nginx as a reverse proxy to direct client requests to Jenkins.  Prerequisites: To begin, you&rsquo;ll need the following: One Ubuntu 20. 04 server configured with a non-root sudo-enabled user and firewall, following the Ubuntu 20. 04 initial server setup guide. Jenkins installed, following the steps in How to Install Jenkins on Ubuntu 20. 04Nginx installed, following the steps in How to Install Nginx on Ubuntu 20. 04An SSL certificate for a domain provided by Let&rsquo;s Encrypt. Follow How to Secure Nginx with Let&rsquo;s Encrypt on Ubuntu 20. 04 to obtain this certificate. Note that you will need a registered domain name that you own or control. This tutorial will use the domain name example. com throughout. Step 1 — Configuring Nginx: In the prerequisite tutorial How to Secure Nginx with Let&rsquo;s Encrypt on Ubuntu 20. 04, you configured Nginx to use SSL in the /etc/nginx/sites-available/example. com file. Open this file to add your reverse proxy settings: sudo nano /etc/nginx/sites-available/example. comIn the server block with the SSL configuration settings, add Jenkins-specific access and error logs: /etc/nginx/sites-available/example. com. . . server {    . . .     # SSL Configuration    #    listen [::]:443 ssl ipv6only=on; # managed by Certbot    listen 443 ssl; # managed by Certbot    access_log      /var/log/nginx/jenkins. access. log;    error_log       /var/log/nginx/jenkins. error. log;    . . .     }Next let&rsquo;s configure the proxy settings. Since we&rsquo;re sending all requests to Jenkins, we&rsquo;ll comment out the default try_files line, which would otherwise return a 404 error before the request reaches Jenkins: /etc/nginx/sites-available/example. com. . .      location / {        # First attempt to serve request as file, then        # as directory, then fall back to displaying a 404.         # try_files $uri $uri/ =404;    }. . . Let&rsquo;s now add the proxy settings, which include: proxy_params: The /etc/nginx/proxy_params file is supplied by Nginx and ensures that important information, including the hostname, the protocol of the client request, and the client IP address, is retained and available in the log files. proxy_pass: This sets the protocol and address of the proxied server, which in this case will be the Jenkins server accessed via localhost on port 8080. proxy_read_timeout: This enables an increase from Nginx&rsquo;s 60 second default to the Jenkins-recommended 90 second value. proxy_redirect: This ensures that responses are correctly rewritten to include the proper host name. Be sure to substitute your SSL-secured domain name for example. com in the proxy_redirect line below: /etc/nginx/sites-available/example. comLocation / . . .      location / {        # First attempt to serve request as file, then        # as directory, then fall back to displaying a 404.         # try_files $uri $uri/ =404;        include /etc/nginx/proxy_params;        proxy_pass     http://localhost:8080;        proxy_read_timeout 90s;        # Fix potential  It appears that your reverse proxy setup is broken  error.         proxy_redirect   http://localhost:8080 https://example. com;Once you&rsquo;ve made these changes, save the file and exit the editor. We&rsquo;ll hold off on restarting Nginx until after we’ve configured Jenkins, but we can test our configuration now: sudo nginx -tIf all is well, the command will return: Outputnginx: the configuration file /etc/nginx/nginx. conf syntax is oknginx: configuration file /etc/nginx/nginx. conf test is successfulIf not, fix any reported errors until the test passes. &lt;p&gt;Note: If you misconfigure the proxy_pass (by adding a trailing slash, for example), you will get something similar to the following in your Jenkins Configuration page. &lt;/p&gt; If you see this error, double-check your proxy_pass and proxy_redirect settings in the Nginx configuration. &lt;/span&gt; Step 2 — Configuring Jenkins: For Jenkins to work with Nginx, you will need to update the Jenkins configuration so that the Jenkins server listens only on the localhost interface rather than on all interfaces (0. 0. 0. 0). If Jenkins listens on all interfaces, it&rsquo;s potentially accessible on its original, unencrypted port (8080).  Let&rsquo;s modify the /etc/default/jenkins configuration file to make these adjustments: sudo nano /etc/default/jenkinsLocate the JENKINS_ARGS line and add --httpListenAddress=127. 0. 0. 1 to the existing arguments: /etc/default/jenkins. . . JENKINS_ARGS= --webroot=/var/cache/$NAME/war --httpPort=$HTTP_PORT --httpListenAddress=127. 0. 0. 1 Save and exit the file. To use the new configuration settings, restart Jenkins: sudo systemctl restart jenkinsSince systemctl doesn&rsquo;t display output, check the status: sudo systemctl status jenkinsYou should see the active (exited) status in the Active line: Output● jenkins. service - LSB: Start Jenkins at boot time  Loaded: loaded (/etc/init. d/jenkins; generated)  Active: active (exited) since Mon 2018-07-09 20:26:25 UTC; 11s ago   Docs: man:systemd-sysv-generator(8) Process: 29766 ExecStop=/etc/init. d/jenkins stop (code=exited, status=0/SUCCESS) Process: 29812 ExecStart=/etc/init. d/jenkins start (code=exited, status=0/SUCCESS)Restart Nginx: sudo systemctl restart nginxCheck the status: sudo systemctl status nginxOutput● nginx. service - A high performance web server and a reverse proxy server  Loaded: loaded (/lib/systemd/system/nginx. service; enabled; vendor preset: enabled)  Active: active (running) since Mon 2018-07-09 20:27:23 UTC; 31s ago   Docs: man:nginx(8) Process: 29951 ExecStop=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx. pid (code=exited, status=0/SUCCESS) Process: 29963 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Process: 29952 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Main PID: 29967 (nginx)With both servers restarted, you should be able to visit the domain using either HTTP or HTTPS. HTTP requests will be redirected automatically to HTTPS, and the Jenkins site will be served securely. Step 3 — Testing the Configuration: Now that you have enabled encryption, you can test the configuration by resetting the administrative password. Let&rsquo;s start by visiting the site via HTTP to verify that you can reach Jenkins and are redirected to HTTPS. In your web browser, enter http://example. com, substituting your domain for example. com. After you press ENTER, the URL should start with https and the location bar should indicate that the connection is secure.  You can enter the administrative username you created in How To Install Jenkins on Ubuntu 20. 04 in the User field, and the password that you selected in the Password field. Once logged in, you can change the password to be sure it&rsquo;s secure.  Click on your username in the upper-right-hand corner of the screen. On the main profile page, select Configure from the list on the left side of the page: This will take you to a new page, where you can enter and confirm a new password: Confirm the new password by clicking Save. You can now use the Jenkins web interface securely. Conclusion: In this tutorial, you configured Nginx as a reverse proxy to Jenkins&rsquo; built-in web server to secure your credentials and other information transmitted via the web interface. Now that Jenkins is secure, you can learn how to set up a continuous integration pipeline to automatically test code changes.  Other resources to consider if you are new to Jenkins are the Jenkins project&rsquo;s &ldquo;Creating your first Pipeline&rdquo; tutorial or the library of community-contributed plugins. "
    }, {
    "id": 66,
    "url": "https://bright-softwares.com/blog/fr/wordpress/how-to-set-up-a-remote-database-to-optimize-site-performance-with-mysql-on-ubuntu-1604",
    "title": "How To Set Up a Remote Database to Optimize Site Performance with MySQL on Ubuntu 16.04",
    "body": "2020/04/04 - "
    }, {
    "id": 67,
    "url": "https://bright-softwares.com/blog/fr/wordpress/how-to-install-wordpress-with-lemp-on-ubuntu-1604",
    "title": "How To Install WordPress with LEMP on Ubuntu 16.04",
    "body": "2020/04/04 - "
    }, {
    "id": 68,
    "url": "https://bright-softwares.com/blog/fr/web%20development/how-to-install-wordpress-with-lamp-on-ubuntu-1604",
    "title": "How To Install WordPress with LAMP on Ubuntu 16.04",
    "body": "2020/04/04 - "
    }, {
    "id": 69,
    "url": "https://bright-softwares.com/blog/en/wordpress/how-to-set-up-a-remote-database-to-optimize-site-performance-with-mysql-on-ubuntu-1604",
    "title": "How To Set Up a Remote Database to Optimize Site Performance with MySQL on Ubuntu 16.04",
    "body": "2020/04/04 - "
    }, {
    "id": 70,
    "url": "https://bright-softwares.com/blog/en/wordpress/how-to-install-wordpress-with-lemp-on-ubuntu-1604",
    "title": "How To Install WordPress with LEMP on Ubuntu 16.04",
    "body": "2020/04/04 - "
    }, {
    "id": 71,
    "url": "https://bright-softwares.com/blog/en/web%20development/how-to-install-wordpress-with-lamp-on-ubuntu-1604",
    "title": "How To Install WordPress with LAMP on Ubuntu 16.04",
    "body": "2020/04/04 - "
    }, {
    "id": 72,
    "url": "https://bright-softwares.com/blog/fr/docker/top-5-questions-from-how-to-become-a-docker-power-user-session-at-dockercon-2020",
    "title": "Top 5 Questions from “How to become a Docker Power User” session at DockerCon 2020",
    "body": "2020/04/03 - This is a guest post from Brian Christner. Brian is a Docker Captain since 2016, host of The Byte podcast, and Co-Founder &amp; Site Reliability Engineer at 56K. Cloud. At 56K. Cloud, he helps companies to adapt technologies and concepts like Cloud, Containers, and DevOps. 56K. Cloud is a Technology company from Switzerland focusing on Automation, IoT, Containerization, and DevOps. It was a fantastic experience hosting my first ever virtual conference session. The commute to my home office was great, and I even picked up a coffee on the way before my session started. No more waiting in lines, queueing for food, or sitting on the conference floor somewhere in a corner to check emails.   The “DockerCon 2020 that&#8217;s a wrap” blog post highlighted my session &#8220;How to Become a Docker Power User using VS Code&#8221; session was one of the most popular sessions from DockerCon. Docker asked if I could write a recap and summarize some of the top questions that appeared in the chat. Absolutely. Honestly, I liked the presented/audience interaction more than an in-person conference. Typically, a presenter broadcasts their content to a room full of participants, and if you are lucky and plan your session tempo well enough, you still have 5-10 minutes for Q&amp;A at the end. Even with 5-10 minutes, I find it is never enough time to answer questions, and people always walk away as they have to hurry to the next session. Virtual Events allow the presenters to answer questions in real-time in the chat. Real-time chat is brilliant as I found a lot more questions were being asked compared to in-person sessions. However, we averaged about 5,500 people online during the session, so the chat became fast and furious with Q&amp;A.    I quickly summarized the Chat transcript of people saying hello from countries/cities around the world. The chat kicked off with people from around the world chiming in to say &#8220;Hello from my home country/city. Just from the chat transcripts and people saying hello, I counted the following: &nbsp;&nbsp;Argentina 1&nbsp;&nbsp; &nbsp;&nbsp;Austria	 2&nbsp;&nbsp; &nbsp;&nbsp;Belgium	 1&nbsp;&nbsp; &nbsp;&nbsp;Brazil	 4&nbsp;&nbsp; &nbsp;&nbsp;Canada	 3&nbsp;&nbsp; &nbsp;&nbsp;Chile	 1&nbsp;&nbsp; &nbsp;&nbsp;Colombia	 1&nbsp;&nbsp; &nbsp;&nbsp;Denmark	 3&nbsp;&nbsp; &nbsp;&nbsp;France	 3&nbsp;&nbsp; &nbsp;&nbsp;Germany	 3&nbsp;&nbsp; &nbsp;&nbsp;Greece	 2&nbsp;&nbsp; &nbsp;&nbsp;Guatemala	 1&nbsp;&nbsp; &nbsp;&nbsp;Italy	 	 1&nbsp;&nbsp; &nbsp;&nbsp;Korea	 	 1&nbsp;&nbsp; &nbsp;&nbsp;Mexico	 1&nbsp;&nbsp; &nbsp;&nbsp;My chair	 1&nbsp;&nbsp; &nbsp;&nbsp;Netherlands	 2&nbsp;&nbsp; &nbsp;&nbsp;Poland	 2&nbsp;&nbsp; &nbsp;&nbsp;Portugal	 2&nbsp;&nbsp; &nbsp;&nbsp;Saudi Arabia	 1&nbsp;&nbsp; &nbsp;&nbsp;South Africa	 4&nbsp;&nbsp; &nbsp;&nbsp;Spain	 	 1&nbsp;&nbsp; &nbsp;&nbsp;Switzerland	 3&nbsp;&nbsp; &nbsp;&nbsp;UK		 3&nbsp;&nbsp; &nbsp;&nbsp;USA		 15&nbsp;&nbsp;   TOTAL  62  Top 5 Questions Based on the Chat transcript, we summarized the top 5 questions/requests. The number one asked question was for the link to the demo code. VS Code demo Repo &#8211; https://github. com/vegasbrianc/vscode-docker-demoDoes VS Code support VIM/Emacs keybindings? Yes, and yes. You can either install the VIM or Emacs keybinding emulation to transform VS Code to your favorite editor keybinding shortcuts. We had several docker-compose questions ranging from can I run X with docker-compose to can I run docker-compose in production. Honestly, you can run docker-compose in production, but it depends on your application and use case. Please have a look at the Docker Voting Application, highlighting the different ways you can run the same application stack. Additionally, docker-compose documentation is an excellent resource. VS Code Debugging &#8211; This is a really powerful tool. If you select the Debug option when bootstrapping your project Debug is built in by default. Otherwise, you can add the debug code manually&nbsp;Docker context is one of the latest features to arrive in the VS Code Docker extension. A few questions asked how to setup Docker contexts and how to use it. At the moment, you still need to set up a Docker Context using the terminal. I would highly recommend reading the blog post by Anca Lordache wrote about using Docker Context as it provides a complete end-to-end set up of using Context with remote hostsBonus question! The most requested question during the session is a link to the Cat GIF’s so here you go. via GIPHY More Information That’s a wrap Blog post:- https://www. docker. com/blog/dockercon-2020-and-thats-a-wrap/Become a Docker Power User With Microsoft Visual Studio Code &#8211; https://docker. events. cube365. net/docker/dockercon/content/Videos/4YkHYPnoQshkmnc26&nbsp;&nbsp;Code used in the talk and demo &#8211; https://github. com/vegasbrianc/vscode-docker-demoVIM Keybinding &#8211; https://marketplace. visualstudio. com/items?itemName=vscodevim. vimEmacs Keybinding &#8211; https://marketplace. visualstudio. com/items?itemName=vscodeemacs. emacsDocker Voting Application &#8211; https://github. com/dockersamples/example-voting-appdocker-compose documentation &#8211; https://docs. docker. com/compose/VS Code Debug &#8211; https://code. visualstudio. com/docs/containers/debug-commonHow to deploy on remote Docker hosts with docker-compose &#8211; https://www. docker. com/blog/how-to-deploy-on-remote-docker-hosts-with-docker-compose/Additional links mentioned during the session 2020 Stackoverflow Survey &#8211; https://insights. stackoverflow. com/survey/2020#technology-most-loved-dreaded-and-wanted-platforms-loved5VS Code Containers overview documentation &#8211; https://code. visualstudio. com/docs/containers/overviewAwesome VS Code List &#8211; https://code. visualstudio. com/docs/containers/overviewCompose Spec &#8211; https://www. compose-spec. io/Find out more about 56K. Cloud We love Cloud, IoT, Containers, DevOps, and Infrastructure as Code. If you are interested in chatting connect with us on Twitter or drop us an email: info@56K. Cloud. We hope you found this article helpful. If there is anything you would like to contribute or you have questions, please let us know! The post Top 5 Questions from “How to become a Docker Power User” session at DockerCon 2020 appeared first on Docker Blog. "
    }, {
    "id": 73,
    "url": "https://bright-softwares.com/blog/fr/docker/how-to-setup-your-local-nodejs-development-environment-using-docker",
    "title": "How To Setup Your Local Node.js Development Environment Using Docker",
    "body": "2020/04/03 - Docker is the defacto toolset for building modern applications and setting up a CI/CD pipeline &#8211; helping you build, ship and run your applications in containers on-prem and in the cloud. &nbsp; Whether you&#8217;re running on simple compute instances such as AWS EC2 or Azure VMs or something a little more fancy like a hosted Kubernetes service like AWS EKS or Azure AKS, Docker’s toolset is your new BFF. &nbsp; But what about your local development environment? Setting up local dev environments can be frustrating to say the least. Remember the last time you joined a new development team? You needed to configure your local machine, install development tools, pull repositories, fight through out-of-date onboarding docs and READMEs, get everything running and working locally without knowing a thing about the code and it’s architecture. Oh and don’t forget about databases, caching layers and message queues. These are notoriously hard to set up and develop on locally. I’ve never worked at a place where we didn’t expect at least a week or more of on-boarding for new developers. &nbsp; So what are we to do? Well, there is no silver bullet and these things are hard to do (that’s why you get paid the big bucks) but with the help of Docker and it’s toolset, we can make things a whole lot easier. In Part I of this tutorial we’ll walk through setting up a local development environment for a relatively complex application that uses React for it’s front end, Node and Express for a couple of micro-services and MongoDb for our datastore. We’ll use Docker to build our images and Docker Compose to make everything a whole lot easier. If you have any questions, comments or just want to connect. You can reach me in our Community Slack or on twitter at @pmckee. Let’s get started. Prerequisites: To complete this tutorial, you will need: Docker installed on your development machine. You can download and install Docker Desktop from the links below:Docker Desktop for MacDocker Desktop for WindowsGit installed on your development machine. An IDE or text editor to use for editing files. I would recommend VSCodeFork the Code Repository: The first thing we want to do is download the code to our local development machine. Let’s do this using the following git command: git clone git@github. com:pmckeetx/memphis. git Now that we have the code local, let’s take a look at the project structure. Open the code in your favorite IDE and expand the root level directories. You’ll see the following file structure. ├── docker-compose. yml├── notes-service│ &nbsp; ├── config│ &nbsp; ├── node_modules│ &nbsp; ├── nodemon. json│ &nbsp; ├── package-lock. json│ &nbsp; ├── package. json│ &nbsp; └── server. js├── reading-list-service│ &nbsp; ├── config│ &nbsp; ├── node_modules│ &nbsp; ├── nodemon. json│ &nbsp; ├── package-lock. json│ &nbsp; ├── package. json│ &nbsp; └── server. js├── users-service│ &nbsp; ├── Dockerfile│ &nbsp; ├── config│ &nbsp; ├── node_modules│ &nbsp; ├── nodemon. json│ &nbsp; ├── package-lock. json│ &nbsp; ├── package. json│ &nbsp; └── server. js└── yoda-ui&nbsp; &nbsp; ├── README. md&nbsp; &nbsp; ├── node_modules&nbsp; &nbsp; ├── package. json&nbsp; &nbsp; ├── public&nbsp; &nbsp; ├── src &nbsp; └── yarn. lock The application is made up of a couple simple microservices and a front-end written in React. js. It uses MongoDB as it’s datastore. Typically at this point, we would start a local version of MongoDB or start looking through the project to find out where our applications will be looking for MongoDB. Then we would start each of our microservices independently and then finally start the UI and hope that the default configuration just works. This can be very complicated and frustrating. Especially if our micro-services are using different versions of node. js and are configured differently. So let&#8217;s walk through making this process easier by dockerizing our application and putting our database into a container. &nbsp; Dockerizing Applications: Docker is a great way to provide consistent development environments. It will allow us to run each of our services and UI in a container. We’ll also set up things so that we can develop locally and start our dependencies with one docker command. The first thing we want to do is dockerize each of our applications. Let’s start with the microservices because they are all written in node. js and we’ll be able to use the same Dockerfile. Create Dockerfiles: Create a Dockerfile in the notes-services directory and add the following commands. This is a very basic Dockerfile to use with node. js. If you are not familiar with the commands, you can start with our getting started guide. Also take a look at our reference documentation. Building Docker Images: Now that we’ve created our Dockerfile, let’s build our image. Make sure you’re still located in the notes-services directory and run the following command: docker build -t notes-service. Now that we have our image built,&nbsp; let’s run it as a container and test that it’s working. docker run &#8211;rm -p 8081:8081 &#8211;name notes notes-service Looks like we have an issue connecting to the mongodb. Two things are broken at this point. We didn’t provide a connection string to the application. The second is that we do not have MongoDB running locally. At this point we could provide a connection string to a shared instance of our database but we want to be able to manage our database locally and not have to worry about messing up our colleagues&#8217; data they might be using to develop. &nbsp; Local Database and Containers: Instead of downloading MongoDB, installing, configuring and then running the Mongo database service. We can use the Docker Official Image for MongoDB and run it in a container. Before we run MongoDB in a container, we want to create a couple of volumes that Docker can manage to store our persistent data and configuration. I like to use the managed volumes that docker provides instead of using bind mounts. You can read all about volumes in our documentation. Let’s create our volumes now. We’ll create one for the data and one for configuration of MongoDB. docker volume create mongodbdocker volume create mongodb_config Now we’ll create a network that our application and database will use to talk with each other. The network is called a user defined bridge network and gives us a nice DNS lookup service which we can use when creating our connection string. docker network create mongodb Now we can run MongoDB in a container and attach to the volumes and network we created above. Docker will pull the image from Hub and run it for you locally. docker run -it &#8211;rm -d -v mongodb:/data/db -v mongodb_config:/data/configdb -p 27017:27017 &#8211;network mongodb &#8211;name mongodb mongo Okay, now that we have a&nbsp; running mongodb, we also need to set a couple of environment variables so our application knows what port to listen on and what connection string to use to access the database. We’ll do this right in the docker run command. docker run \-it &#8211;rm -d \&#8211;network mongodb \&#8211;name notes \-p 8081:8081 \-e SERVER_PORT=8081 \-e SERVER_PORT=8081 \-e DATABASE_CONNECTIONSTRING=mongodb://mongodb:27017/yoda_notes \ notes-service Let’s test that our application is connected to the database and is able to add a note. curl &#8211;request POST \&#8211;url http://localhost:8081/services/m/notes \&nbsp;&nbsp;&#8211;header &#8216;content-type: application/json&#8217; \&nbsp;&nbsp;&#8211;data &#8216;{      &#8220;name&#8221;: &#8220;this is a note&#8221;,      &#8220;text&#8221;: &#8220;this is a note that I wanted to take while I was working on writing a blog post. &#8221;, &#8220;owner&#8221;: &#8220;peter&#8221;} You should receive the following json back from our service. {&#8220;code&#8221;:&#8221;success&#8221;,&#8221;payload&#8221;:{&#8220;_id&#8221;:&#8221;5efd0a1552cd422b59d4f994&#8243;,&#8221;name&#8221;:&#8221;this is a note&#8221;,&#8221;text&#8221;:&#8221;this is a note that I wanted to take while I was working on writing a blog post. &#8221;,&#8221;owner&#8221;:&#8221;peter&#8221;,&#8221;createDate&#8221;:&#8221;2020-07-01T22:11:33. 256Z&#8221;}} Conclusion: Awesome! We’ve completed the first steps in Dockerizing our local development environment for Node. js. In Part II of the series, we’ll take a look at how we can use Docker Compose to simplify the process we just went through. In the meantime, you can read more about networking, volumes and Dockerfile best practices with the below links: Docker NetworkingVolumesBest practices for writing DockerfilesThe post How To Setup Your Local Node. js Development Environment Using Docker appeared first on Docker Blog. "
    }, {
    "id": 74,
    "url": "https://bright-softwares.com/blog/fr/kubernetes/how-to-set-up-the-codeserver-cloud-ide-platform-on-digitalocean-kubernetes",
    "title": "How To Set Up the code-server Cloud IDE Platform on DigitalOcean Kubernetes",
    "body": "2020/04/03 - IntroductionWith developer tools moving to the cloud, creation and adoption of cloud IDE (Integrated Development Environment) platforms is growing. Cloud IDEs allow for real-time collaboration between developer teams to work in a unified development environment that minimizes incompatibilities and enhances productivity. Accessible through web browsers, cloud IDEs are available from every type of modern device. Another advantage of a cloud IDE is the possibility to leverage the power of a cluster, which can greatly exceed the processing power of a single development computer. code-server is Microsoft Visual Studio Code running on a remote server and accessible directly from your browser. Visual Studio Code is a modern code editor with integrated Git support, a code debugger, smart autocompletion, and customizable and extensible features. This means that you can use various devices, running different operating systems, and always have a consistent development environment on hand. In this tutorial, you will set up the code-server cloud IDE platform on your DigitalOcean Kubernetes cluster and expose it at your domain, secured with Let’s Encrypt certificates. In the end, you’ll have Microsoft Visual Studio Code running on your Kubernetes cluster, available via HTTPS and protected by a password. PrerequisitesA DigitalOcean Kubernetes cluster with your connection configured as the kubectl default. Instructions on how to configure kubectl are shown under the Connect to your Cluster step when you create your cluster. To create a Kubernetes cluster on DigitalOcean, see Kubernetes Quickstart. The Helm package manager installed on your local machine, and Tiller installed on your cluster. To do this, complete Steps 1 and 2 of the How To Install Software on Kubernetes Clusters with the Helm Package Manager tutorial. The Nginx Ingress Controller and Cert-Manager installed on your cluster using Helm in order to expose code-server using Ingress Resources. To do this, follow How to Set Up an Nginx Ingress on DigitalOcean Kubernetes Using Helm. A fully registered domain name to host code-server, pointed at the Load Balancer used by the Nginx Ingress. This tutorial will use code-server. your_domain throughout. You can purchase a domain name on Namecheap, get one for free on Freenom, or use the domain registrar of your choice. This domain name must differ from the one used in the How To Set Up an Nginx Ingress on DigitalOcean Kubernetes prerequisite tutorial. Step 1 — Installing And Exposing code-server: In this section, you’ll install code-server to your DigitalOcean Kubernetes cluster and expose it at your domain, using the Nginx Ingress controller. You will also set up a password for admittance. You’ll store the deployment configuration on your local machine, in a file named code-server. yaml. Create it using the following command:nano code-server. yaml Add the following lines to the file: code-server. yaml apiVersion: v1kind: Namespacemetadata: name: code-server---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: code-server namespace: code-server annotations:  kubernetes. io/ingress. class: nginxspec: rules: - host: code-server. your_domain  http:   paths:   - backend:     serviceName: code-server     servicePort: 80---apiVersion: v1kind: Servicemetadata: name: code-server namespace: code-serverspec: ports: - port: 80  targetPort: 8443 selector:  app: code-server---apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels:  app: code-server name: code-server namespace: code-serverspec: selector:  matchLabels:   app: code-server replicas: 1 template:  metadata:   labels:    app: code-server  spec:   containers:   - image: codercom/code-server    imagePullPolicy: Always    name: code-server    args: [ --allow-http ]    ports:    - containerPort: 8443    env:    - name: PASSWORD     value:  your_password This configuration defines a Namespace, a Deployment, a Service, and an Ingress. The Namespace is called code-server and separates the code-server installation from the rest of your cluster. The Deployment consists of one replica of the codercom/code-server Docker image, and an environment variable named PASSWORD that specifies the password for access. The code-server Service internally exposes the pod (created as a part of the Deployment) at port 80. The Ingress defined in the file specifies that the Ingress Controller is nginx, and that the code-server. your_domain domain will be served from the Service. Remember to replace your_password with your desired password, and code-server. your_domain with your desired domain, pointed to the Load Balancer of the Nginx Ingress Controller. Then, create the configuration in Kubernetes by running the following command: kubectl create -f code-server. yamlYou’ll see the following output:Output namespace/code-server createdingress. extensions/code-server createdservice/code-server createddeployment. extensions/code-server createdYou can watch the code-server pod become available by running: kubectl get pods -w -n code-serverThe output will look like:Output NAME             READY  STATUS       RESTARTS  AGEcode-server-f85d9bfc9-j7hq6  0/1   ContainerCreating  0     1mAs soon as the status becomes Running, code-server has finished installing to your cluster. Navigate to your domain in your browser. You’ll see the login prompt for code-server. Enter the password you set in code-server. yaml and press Enter IDE. You’ll enter code-server and immediately see its editor GUI. You’ve installed code-server to your Kubernetes cluster and made it available at your domain. You have also verified that it requires you to log in with a password. Now, you’ll move on to secure it with free Let’s Encrypt certificates using Cert-Manager. Step 2 — Securing the code-server Deployment: In this section, you will secure your code-server installation by applying Let’s Encrypt certificates to your Ingress, which Cert-Manager will automatically create. After completing this step, your code-server installation will be accessible via HTTPS. Open code-server. yaml for editing: nano code-server. yamlAdd the highlighted lines to your file, making sure to replace the example domain with your own:code-server. yaml apiVersion: v1kind: Namespacemetadata: name: code-server---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: code-server namespace: code-server annotations:  kubernetes. io/ingress. class: nginx  certmanager. k8s. io/cluster-issuer: letsencrypt-prodspec: tls: - hosts:  - code-server. your_domain  secretName: codeserver-prod rules: - host: code-server. your_domain  http:   paths:   - backend:     serviceName: code-server     servicePort: 80. . . First, you specify that the cluster-issuer that this Ingress will use to provision certificates will be letsencrypt-prod, created as a part of the prerequisites. Then, you specify the domains that will be secured under the tls section, as well as your name for the Secret holding them. Apply the changes to your Kubernetes cluster by running the following command: kubectl apply -f code-server. yamlYou’ll need to wait a few minutes for Let’s Encrypt to provision your certificate. In the meantime, you can track its progress by looking at the output of the following command:kubectl describe certificate codeserver-prod -n code-server When it finishes, the end of the output will look similar to this:Output Events: Type  Reason       Age  From     Message ----  ------       ----  ----     ------- Normal Generated      2m49s cert-manager Generated new private key Normal GenerateSelfSigned 2m49s cert-manager Generated temporary self signed certificate Normal OrderCreated    2m49s cert-manager Created Order resource  codeserver-prod-4279678953  Normal OrderComplete    2m14s cert-manager Order  codeserver-prod-4279678953  completed successfully Normal CertIssued     2m14s cert-manager Certificate issued successfullyYou can now refresh your domain in your browser. You’ll see the padlock to the left of the address bar in your browser signifying that the connection is secure. In this step, you have configured the Ingress to secure your code-server deployment. Now, you can review the code-server user interface. Step 3 — Exploring the code-server Interface: In this section, you’ll explore some of the features of the code-server interface. Since code-server is Visual Studio Code running in the cloud, it has the same interface as the standalone desktop edition. On the left-hand side of the IDE, there is a vertical row of six buttons opening the most commonly used features in a side panel known as the Activity Bar. This bar is customizable so you can move these views to a different order or remove them from the bar. By default, the first view opens the Explorer panel that provides tree-like navigation of the project’s structure. You can manage your folders and files here—creating, deleting, moving, and renaming them as necessary. The next view provides access to a search and replace functionality. Following this, in the default order, is your view of the source control systems, like Git. Visual Studio code also supports other source control providers and you can find further instructions for source control workflows with the editor in this documentation. The debugger option on the Activity Bar provides all the common actions for debugging in the panel. Visual Studio Code comes with built-in support for the Node. js runtime debugger and any language that transpiles to Javascript. For other languages you can install extensions for the required debugger. You can save debugging configurations in the launch. json file. The final view in the Activity Bar provides a menu to access available extensions on the Marketplace. The central part of the GUI is your editor, which you can separate by tabs for your code editing. You can change your editing view to a grid system or to side-by-side files. After creating a new file through the File menu, an empty file will open in a new tab, and once saved, the file’s name will be viewable in the Explorer side panel. Creating folders can be done by right clicking on the Explorer sidebar and pressing on New Folder. You can expand a folder by clicking on its name as well as dragging and dropping files and folders to upper parts of the hierarchy to move them to a new location. You can gain access to a terminal by pressing CTRL+SHIFT+\, or by pressing on Terminal in the upper menu, and selecting New Terminal. The terminal will open in a lower panel and its working directory will be set to the project’s workspace, which contains the files and folders shown in the Explorer side panel. You’ve explored a high-level overview of the code-server interface and reviewed some of the most commonly used features. ConclusionYou now have code-server, a versatile cloud IDE, installed on your DigitalOcean Kubernetes cluster. You can work on your source code and documents with it individually or collaborate with your team. Running a cloud IDE on your cluster provides more power for testing, downloading, and more thorough or rigorous computing. For further information see the Visual Studio Code documentation on additional features and detailed instructions on other components of code-server. "
    }, {
    "id": 75,
    "url": "https://bright-softwares.com/blog/en/docker/top-5-questions-from-how-to-become-a-docker-power-user-session-at-dockercon-2020",
    "title": "Top 5 Questions from “How to become a Docker Power User” session at DockerCon 2020",
    "body": "2020/04/03 - This is a guest post from Brian Christner. Brian is a Docker Captain since 2016, host of The Byte podcast, and Co-Founder &amp; Site Reliability Engineer at 56K. Cloud. At 56K. Cloud, he helps companies to adapt technologies and concepts like Cloud, Containers, and DevOps. 56K. Cloud is a Technology company from Switzerland focusing on Automation, IoT, Containerization, and DevOps. It was a fantastic experience hosting my first ever virtual conference session. The commute to my home office was great, and I even picked up a coffee on the way before my session started. No more waiting in lines, queueing for food, or sitting on the conference floor somewhere in a corner to check emails.   The “DockerCon 2020 that&#8217;s a wrap” blog post highlighted my session &#8220;How to Become a Docker Power User using VS Code&#8221; session was one of the most popular sessions from DockerCon. Docker asked if I could write a recap and summarize some of the top questions that appeared in the chat. Absolutely. Honestly, I liked the presented/audience interaction more than an in-person conference. Typically, a presenter broadcasts their content to a room full of participants, and if you are lucky and plan your session tempo well enough, you still have 5-10 minutes for Q&amp;A at the end. Even with 5-10 minutes, I find it is never enough time to answer questions, and people always walk away as they have to hurry to the next session. Virtual Events allow the presenters to answer questions in real-time in the chat. Real-time chat is brilliant as I found a lot more questions were being asked compared to in-person sessions. However, we averaged about 5,500 people online during the session, so the chat became fast and furious with Q&amp;A.    I quickly summarized the Chat transcript of people saying hello from countries/cities around the world. The chat kicked off with people from around the world chiming in to say &#8220;Hello from my home country/city. Just from the chat transcripts and people saying hello, I counted the following: &nbsp;&nbsp;Argentina 1&nbsp;&nbsp; &nbsp;&nbsp;Austria	 2&nbsp;&nbsp; &nbsp;&nbsp;Belgium	 1&nbsp;&nbsp; &nbsp;&nbsp;Brazil	 4&nbsp;&nbsp; &nbsp;&nbsp;Canada	 3&nbsp;&nbsp; &nbsp;&nbsp;Chile	 1&nbsp;&nbsp; &nbsp;&nbsp;Colombia	 1&nbsp;&nbsp; &nbsp;&nbsp;Denmark	 3&nbsp;&nbsp; &nbsp;&nbsp;France	 3&nbsp;&nbsp; &nbsp;&nbsp;Germany	 3&nbsp;&nbsp; &nbsp;&nbsp;Greece	 2&nbsp;&nbsp; &nbsp;&nbsp;Guatemala	 1&nbsp;&nbsp; &nbsp;&nbsp;Italy	 	 1&nbsp;&nbsp; &nbsp;&nbsp;Korea	 	 1&nbsp;&nbsp; &nbsp;&nbsp;Mexico	 1&nbsp;&nbsp; &nbsp;&nbsp;My chair	 1&nbsp;&nbsp; &nbsp;&nbsp;Netherlands	 2&nbsp;&nbsp; &nbsp;&nbsp;Poland	 2&nbsp;&nbsp; &nbsp;&nbsp;Portugal	 2&nbsp;&nbsp; &nbsp;&nbsp;Saudi Arabia	 1&nbsp;&nbsp; &nbsp;&nbsp;South Africa	 4&nbsp;&nbsp; &nbsp;&nbsp;Spain	 	 1&nbsp;&nbsp; &nbsp;&nbsp;Switzerland	 3&nbsp;&nbsp; &nbsp;&nbsp;UK		 3&nbsp;&nbsp; &nbsp;&nbsp;USA		 15&nbsp;&nbsp;   TOTAL  62  Top 5 Questions Based on the Chat transcript, we summarized the top 5 questions/requests. The number one asked question was for the link to the demo code. VS Code demo Repo &#8211; https://github. com/vegasbrianc/vscode-docker-demoDoes VS Code support VIM/Emacs keybindings? Yes, and yes. You can either install the VIM or Emacs keybinding emulation to transform VS Code to your favorite editor keybinding shortcuts. We had several docker-compose questions ranging from can I run X with docker-compose to can I run docker-compose in production. Honestly, you can run docker-compose in production, but it depends on your application and use case. Please have a look at the Docker Voting Application, highlighting the different ways you can run the same application stack. Additionally, docker-compose documentation is an excellent resource. VS Code Debugging &#8211; This is a really powerful tool. If you select the Debug option when bootstrapping your project Debug is built in by default. Otherwise, you can add the debug code manually&nbsp;Docker context is one of the latest features to arrive in the VS Code Docker extension. A few questions asked how to setup Docker contexts and how to use it. At the moment, you still need to set up a Docker Context using the terminal. I would highly recommend reading the blog post by Anca Lordache wrote about using Docker Context as it provides a complete end-to-end set up of using Context with remote hostsBonus question! The most requested question during the session is a link to the Cat GIF’s so here you go. via GIPHY More Information That’s a wrap Blog post:- https://www. docker. com/blog/dockercon-2020-and-thats-a-wrap/Become a Docker Power User With Microsoft Visual Studio Code &#8211; https://docker. events. cube365. net/docker/dockercon/content/Videos/4YkHYPnoQshkmnc26&nbsp;&nbsp;Code used in the talk and demo &#8211; https://github. com/vegasbrianc/vscode-docker-demoVIM Keybinding &#8211; https://marketplace. visualstudio. com/items?itemName=vscodevim. vimEmacs Keybinding &#8211; https://marketplace. visualstudio. com/items?itemName=vscodeemacs. emacsDocker Voting Application &#8211; https://github. com/dockersamples/example-voting-appdocker-compose documentation &#8211; https://docs. docker. com/compose/VS Code Debug &#8211; https://code. visualstudio. com/docs/containers/debug-commonHow to deploy on remote Docker hosts with docker-compose &#8211; https://www. docker. com/blog/how-to-deploy-on-remote-docker-hosts-with-docker-compose/Additional links mentioned during the session 2020 Stackoverflow Survey &#8211; https://insights. stackoverflow. com/survey/2020#technology-most-loved-dreaded-and-wanted-platforms-loved5VS Code Containers overview documentation &#8211; https://code. visualstudio. com/docs/containers/overviewAwesome VS Code List &#8211; https://code. visualstudio. com/docs/containers/overviewCompose Spec &#8211; https://www. compose-spec. io/Find out more about 56K. Cloud We love Cloud, IoT, Containers, DevOps, and Infrastructure as Code. If you are interested in chatting connect with us on Twitter or drop us an email: info@56K. Cloud. We hope you found this article helpful. If there is anything you would like to contribute or you have questions, please let us know! The post Top 5 Questions from “How to become a Docker Power User” session at DockerCon 2020 appeared first on Docker Blog. "
    }, {
    "id": 76,
    "url": "https://bright-softwares.com/blog/en/docker/how-to-setup-your-local-nodejs-development-environment-using-docker",
    "title": "How To Setup Your Local Node.js Development Environment Using Docker",
    "body": "2020/04/03 - Docker is the defacto toolset for building modern applications and setting up a CI/CD pipeline &#8211; helping you build, ship and run your applications in containers on-prem and in the cloud. &nbsp; Whether you&#8217;re running on simple compute instances such as AWS EC2 or Azure VMs or something a little more fancy like a hosted Kubernetes service like AWS EKS or Azure AKS, Docker’s toolset is your new BFF. &nbsp; But what about your local development environment? Setting up local dev environments can be frustrating to say the least. Remember the last time you joined a new development team? You needed to configure your local machine, install development tools, pull repositories, fight through out-of-date onboarding docs and READMEs, get everything running and working locally without knowing a thing about the code and it’s architecture. Oh and don’t forget about databases, caching layers and message queues. These are notoriously hard to set up and develop on locally. I’ve never worked at a place where we didn’t expect at least a week or more of on-boarding for new developers. &nbsp; So what are we to do? Well, there is no silver bullet and these things are hard to do (that’s why you get paid the big bucks) but with the help of Docker and it’s toolset, we can make things a whole lot easier. In Part I of this tutorial we’ll walk through setting up a local development environment for a relatively complex application that uses React for it’s front end, Node and Express for a couple of micro-services and MongoDb for our datastore. We’ll use Docker to build our images and Docker Compose to make everything a whole lot easier. If you have any questions, comments or just want to connect. You can reach me in our Community Slack or on twitter at @pmckee. Let’s get started. Prerequisites: To complete this tutorial, you will need: Docker installed on your development machine. You can download and install Docker Desktop from the links below:Docker Desktop for MacDocker Desktop for WindowsGit installed on your development machine. An IDE or text editor to use for editing files. I would recommend VSCodeFork the Code Repository: The first thing we want to do is download the code to our local development machine. Let’s do this using the following git command: git clone git@github. com:pmckeetx/memphis. git Now that we have the code local, let’s take a look at the project structure. Open the code in your favorite IDE and expand the root level directories. You’ll see the following file structure. ├── docker-compose. yml├── notes-service│ &nbsp; ├── config│ &nbsp; ├── node_modules│ &nbsp; ├── nodemon. json│ &nbsp; ├── package-lock. json│ &nbsp; ├── package. json│ &nbsp; └── server. js├── reading-list-service│ &nbsp; ├── config│ &nbsp; ├── node_modules│ &nbsp; ├── nodemon. json│ &nbsp; ├── package-lock. json│ &nbsp; ├── package. json│ &nbsp; └── server. js├── users-service│ &nbsp; ├── Dockerfile│ &nbsp; ├── config│ &nbsp; ├── node_modules│ &nbsp; ├── nodemon. json│ &nbsp; ├── package-lock. json│ &nbsp; ├── package. json│ &nbsp; └── server. js└── yoda-ui&nbsp; &nbsp; ├── README. md&nbsp; &nbsp; ├── node_modules&nbsp; &nbsp; ├── package. json&nbsp; &nbsp; ├── public&nbsp; &nbsp; ├── src &nbsp; └── yarn. lock The application is made up of a couple simple microservices and a front-end written in React. js. It uses MongoDB as it’s datastore. Typically at this point, we would start a local version of MongoDB or start looking through the project to find out where our applications will be looking for MongoDB. Then we would start each of our microservices independently and then finally start the UI and hope that the default configuration just works. This can be very complicated and frustrating. Especially if our micro-services are using different versions of node. js and are configured differently. So let&#8217;s walk through making this process easier by dockerizing our application and putting our database into a container. &nbsp; Dockerizing Applications: Docker is a great way to provide consistent development environments. It will allow us to run each of our services and UI in a container. We’ll also set up things so that we can develop locally and start our dependencies with one docker command. The first thing we want to do is dockerize each of our applications. Let’s start with the microservices because they are all written in node. js and we’ll be able to use the same Dockerfile. Create Dockerfiles: Create a Dockerfile in the notes-services directory and add the following commands. This is a very basic Dockerfile to use with node. js. If you are not familiar with the commands, you can start with our getting started guide. Also take a look at our reference documentation. Building Docker Images: Now that we’ve created our Dockerfile, let’s build our image. Make sure you’re still located in the notes-services directory and run the following command: docker build -t notes-service. Now that we have our image built,&nbsp; let’s run it as a container and test that it’s working. docker run &#8211;rm -p 8081:8081 &#8211;name notes notes-service Looks like we have an issue connecting to the mongodb. Two things are broken at this point. We didn’t provide a connection string to the application. The second is that we do not have MongoDB running locally. At this point we could provide a connection string to a shared instance of our database but we want to be able to manage our database locally and not have to worry about messing up our colleagues&#8217; data they might be using to develop. &nbsp; Local Database and Containers: Instead of downloading MongoDB, installing, configuring and then running the Mongo database service. We can use the Docker Official Image for MongoDB and run it in a container. Before we run MongoDB in a container, we want to create a couple of volumes that Docker can manage to store our persistent data and configuration. I like to use the managed volumes that docker provides instead of using bind mounts. You can read all about volumes in our documentation. Let’s create our volumes now. We’ll create one for the data and one for configuration of MongoDB. docker volume create mongodbdocker volume create mongodb_config Now we’ll create a network that our application and database will use to talk with each other. The network is called a user defined bridge network and gives us a nice DNS lookup service which we can use when creating our connection string. docker network create mongodb Now we can run MongoDB in a container and attach to the volumes and network we created above. Docker will pull the image from Hub and run it for you locally. docker run -it &#8211;rm -d -v mongodb:/data/db -v mongodb_config:/data/configdb -p 27017:27017 &#8211;network mongodb &#8211;name mongodb mongo Okay, now that we have a&nbsp; running mongodb, we also need to set a couple of environment variables so our application knows what port to listen on and what connection string to use to access the database. We’ll do this right in the docker run command. docker run \-it &#8211;rm -d \&#8211;network mongodb \&#8211;name notes \-p 8081:8081 \-e SERVER_PORT=8081 \-e SERVER_PORT=8081 \-e DATABASE_CONNECTIONSTRING=mongodb://mongodb:27017/yoda_notes \ notes-service Let’s test that our application is connected to the database and is able to add a note. curl &#8211;request POST \&#8211;url http://localhost:8081/services/m/notes \&nbsp;&nbsp;&#8211;header &#8216;content-type: application/json&#8217; \&nbsp;&nbsp;&#8211;data &#8216;{      &#8220;name&#8221;: &#8220;this is a note&#8221;,      &#8220;text&#8221;: &#8220;this is a note that I wanted to take while I was working on writing a blog post. &#8221;, &#8220;owner&#8221;: &#8220;peter&#8221;} You should receive the following json back from our service. {&#8220;code&#8221;:&#8221;success&#8221;,&#8221;payload&#8221;:{&#8220;_id&#8221;:&#8221;5efd0a1552cd422b59d4f994&#8243;,&#8221;name&#8221;:&#8221;this is a note&#8221;,&#8221;text&#8221;:&#8221;this is a note that I wanted to take while I was working on writing a blog post. &#8221;,&#8221;owner&#8221;:&#8221;peter&#8221;,&#8221;createDate&#8221;:&#8221;2020-07-01T22:11:33. 256Z&#8221;}} Conclusion: Awesome! We’ve completed the first steps in Dockerizing our local development environment for Node. js. In Part II of the series, we’ll take a look at how we can use Docker Compose to simplify the process we just went through. In the meantime, you can read more about networking, volumes and Dockerfile best practices with the below links: Docker NetworkingVolumesBest practices for writing DockerfilesThe post How To Setup Your Local Node. js Development Environment Using Docker appeared first on Docker Blog. "
    }, {
    "id": 77,
    "url": "https://bright-softwares.com/blog/en/aws/migrate-your-current-vps-linode-rackspace-aws-ec2-to-digitalocean",
    "title": "Migrate Your Current VPS (Linode, Rackspace, AWS EC2) to DigitalOcean",
    "body": "2020/04/02 - IntroductionMigrating between VPS providers can seem like a daunting task. Like DigitalOcean, other VPS providers, such as Linode and Rackspace, provide root access. This allows you to transfer all of the necessary files to your new DigitalOcean VPS. For this guide, we will demonstrate how to transfer a simple WordPress blog from Linode to a DigitalOcean cloud server. Both instances will be running Ubuntu. These instructions can be adapted to migrate other services from other providers. We will be working as root in both VPS instances. What the Red MeansThe lines that the user needs to enter or customize will be in red in this tutorial! The rest should mostly be copy-and-pastable. Preliminary DigitalOcean Cloud Server ConfigurationLAMP InstallationTo begin, you will want to install a LAMP (Linux, Apache, MySQL, PHP) stack on your DigitalOcean cloud server. This can be accomplished in a few different ways. The easiest way to get LAMP up and running on Ubuntu is to choose the pre-configured “LAMP on Ubuntu” image when you are initially creating your droplet. In the “Select Image” portion of the Droplet creation page, choose the “Applications” tab. Select “LAMP on Ubuntu 14. 04”. If you already have a droplet that you would like to use, you can install a LAMP stack on Ubuntu by following this link. Rsync InstallationWe will be doing our file transfers using ssh and rsync. Make sure rsync is installed on your DigitalOcean VPS using the following command:rsync –versionIf this command returns a “command not found” message, then you need to install rsync with apt-get:apt-get install rsyncCommunicating Between VPS ServersThe following steps will take place on your old VPS. If you aren’t already, log in as root. Your old VPS also needs to have rsync installed. Re-run the check for rsync on this system:rsync –versionIf necessary, install rsync:apt-get install rsyncIn order to transfer the relevant information from our previous VPS to our DigitalOcean cloud server, rsync needs to be able to log into our new server from our old server. We will be using SSH to do this. If you do not have SSH keys generated on your old VPS, create them now with the following command:ssh-keygen -t rsa -b 4096 -vAnswer the prompts as required. Feel free to press “Enter” through all of the prompts to accept the default values. Next, transfer the SSH key to our new VPS using the following command. Change the portion in red to reflect your DigitalOcean VPS IP address:ssh-copy-id 111. 222. 333. 444Transferring Site FilesFirst, we will be transferring the files in our old server’s web root to our new cloud server. We will find where our WordPress web root is by examining the configuration file. We will look at the sites-enabled directory to find the correct VirtualHost file:cd /etc/apache2/sites-enabledlsli606-185. members. linode. comHere, our file is called “li606-185. members. linode. com”, but yours may be something different. Open the file in nano:nano li606-185. members. linode. comWe are looking for the “DocumentRoot” line to tell us which directory our web content is being served from. In our example, the line reads:DocumentRoot /srv/www/li606-185. members. linode. com/public_html/Close the file and cd into that directory:cd /srv/www/li606-185. members. linode. com/public_html/ls -FOutputlatest. tar. gz wordpress/As you can see, we have a directory for our WordPress site. This contains all of the web content for our site. We will transfer this entire directory, along with its permissions and sub-directories, from this location into the web root of our DigitalOcean cloud server. By default, apache2 on Ubuntu 14. 04 serves its content out of “/var/www/html”, so that is where we will place this content. We will add some options to rsync in order for it to transfer properly. The “-a” option stands for archive, which allows us to transfer recursively while preserving many of the underlying file properties like permissions and ownership. We also use the “-v” flag for verbose output, and the “-P” flag, which shows us transfer progress and allows rsync to resume in the event of a transfer problem. rsync -avP wordpress 111. 222. 333. 444:/var/www/html/Note how there is no trailing slash after “wordpress”, but that there is in “/var/www/html/”. This will transfer the “wordpress” directory itself to the destination instead of only transferring the directory contents. Our entire WordPress directory structure has now been transferred to the web root of the new cloud server. At this point, if we direct our web browser to our new cloud server’s IP address and try to access our WordPress site, we will get a MySQL error:111. 222. 333. 444/wordpressError establishing a database connection This is because WordPress stores its data in a MySQL database that has not been transferred yet. We will handle the MySQL transfer next. MySQL Database TransferThe best way to transfer MySQL databases is to use MySQL’s internal database dumping utility. First, we will see what databases we need to dump. Log into MySQL:cdmysql -u root -pEnter the database administrator’s password to continue. List the MySQL databases with the following command:show databases;+——————–+| Database |+——————–+| information_schema || mysql || wordpress |+——————–+3 rows in set (0. 00 sec)We would like to transfer our “wordpress” database, which contains our site information, and also our “mysql” database, which will transfer all of our user info, etc. The “information_schema” is just data structure information, and we don’t need to hold onto that. Get an idea of which databases you want to transfer for the next step. Exit out of MySql:exitWe will be dumping the database information with “mysqldump” and then compressing it with “bzip2”. We will use a number of parameters to make our databases import cleanly. Replace the red with your database names:mysqldump -u root -p -QqeR –add-drop-table –databases mysql wordpress | bzip2 -v9 - &gt; siteData. sql. bz2Again, enter your database administrator’s password to continue. We now have a zipped database file that we can transfer to our new cloud server. We will use rsync again. Change the IP address to reflect your DigitalOcean server’s IP address:rsync -avP siteData. sql. bz2 111. 222. 333. 444:/rootImporting the DatabasesOur database file is compressed and transferred to our new DigitalOcean cloud server. We must import it into MySQL on our new server so that WordPress can utilize it. Log into your DigitalOcean cloud server as root for the following steps. Our database file was transferred to the root user’s home directory, so move to that directory now. We will uncompress the file using “bunzip2”:cd /rootbunzip2 siteData. sql. bz2Now we can import the file into our new MySQL database:mysql -u root -p &lt; siteData. sqlLet’s check to see that MySQL has imported correctly:mysql -u root -pshow databases;+——————–+| Database |+——————–+| information_schema || mysql || performance_schema || test || wordpress |+——————–+5 rows in set (0. 00 sec)As you can see, our “wordpress” database is present. The previous “mysql” database has been replaced with the one from our old VPS. Exit out of MySQL:exitNow we will restart our database and our server for good measure:service mysql restartservice apache2 restartNow, if we navigate to our DigitalOcean VPS IP address followed by “/wordpress”, we will see the WordPress site that was previously hosted on our old VPS:111. 222. 333. 444/wordpress Final ConsiderationsBefore changing over your domain name to point to your new site location, it is important to test your setup extensively. It is a good idea to reference the services that were running on your old VPS, and then check their configuration files. You can see the services that were running on your old VPS by logging in and typing:netstat -pluntActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127. 0. 0. 1:3306 0. 0. 0. 0:_ LISTEN 13791/mysqldtcp 0 0 0. 0. 0. 0:22 0. 0. 0. 0:_ LISTEN 10538/sshdtcp 0 0 127. 0. 0. 1:25 0. 0. 0. 0:_ LISTEN 13963/mastertcp6 0 0 :::80 :::_ LISTEN 13771/apache2tcp6 0 0 :::22 :::_ LISTEN 10538/sshdudp 0 0 0. 0. 0. 0:68 0. 0. 0. 0:_ 2287/dhclient3Here, we can see the services in the last column that we will want to configure on our new server. Your list will probably be different. Each service has its own configuration syntax and configuration location, so you will need to check the documentation on a case-by-case basis. As an example, if we wanted to replicate the configuration of our SSH daemon on our new VPS we could transfer the configuration file to the home directory of our new VPS using rsync:rsync -avP /etc/ssh/sshd_config 111. 222. 333. 444:/rootAfter we’ve transferred the file, we do not want to simply replace the default file with the one from our old VPS. Different versions of programs can introduce changes to the syntax. Issues can also arise from configuration options that were specific to your old VPS. Options referencing hostnames, IP addresses, or file paths will need to be changed to reflect your new setup. It is safer to produce a diff of the files so that you can adjust your new cloud server’s native configuration file as necessary. There are a number of different programs that can give you the differences between two files. One is simply diff:diff /root/sshd_conf /etc/ssh/sshd_configThis will produce a list of all of the differences between the two files. You can examine the differences and take them into consideration. Some configuration options you might want to incorporate from your old setup, while others you might want to modify or discard. By Justin Ellingwood "
    }, {
    "id": 78,
    "url": "https://bright-softwares.com/blog/fr/data-analysis/how-to-copy-a-formula-down-an-entire-column-in-google-sheets",
    "title": "How to Copy a Formula Down an Entire Column in Google Sheets",
    "body": "2020/02/12 - You are working inside a Google Spreadsheet where a formula needs to copied down to the last row of the sheet. You also need the formula to be added automatically when a new row is added to the Google Sheet. There are several ways to solve this problem. Copy Formula Down in Google SheetsThe easiest approach to copy down formulas is to use the fill handle in Google Sheets. Write your formula in the first row of your spreadsheet, and then point your mouse to the lower right corner of the formula cell. The pointer changes into a fill handle (black plus symbol) that you can drag to the last row of the sheet. The fill handle will not just copy down the formulas to all the adjacent cells but also copies the visual formatting. If you need to copy the formulas across cells but sans any formatting, select the cell that contains the formatting and press Ctrl+C to copy it to the clipboard. Next, select the range where that formula needs to applied, right-click, choose Paste Special and Paste Formula only. Apply Formula to the Entire Column in Google SheetsIf you have hundreds of rows in a Google Spreadsheet and you want to apply the same formula to all rows of a particular column, there’s a more efficient solution than copy-paste - Array Formulas. Highlight the first cell in the column and type the formula as earlier. However, instead of specifying a single cell as a parameter, we’ll specify the entire column using the B2:B notation (start from cell B2 and go all the way down to the last row of column B). Then press Ctrl+Shift+Enter, or Cmd+Shift+Enter on Mac, and Google Sheets will automatically surround your formula with ARRAYFORMULA function. Thus, we could apply the formula to the entire column of the spreadsheet with only a single cell. Array Formulas are more efficient as they process a batch of rows in one go. They are also easier to maintain as you only need to modify a single cell to edit the formula. One issue that you may have noticed with the above formulae is that it applies to every row in the column where you have only want to add formulas to rows that contain data and skip the blank rows. This can be done by adding an IF contain to our ARRAYFORMULA so that it doesn’t apply the formula the any of the blank rows. Google Spreadsheet offers two functions to help test whether a cell is empty or now. ISBLANK(A1) - Returns TRUE if the referenced cell is empty. LEN(A1) &lt;&gt; 0 - Returns TRUE if the referenced cell not empty, FALSE otherwiseOur modified Array Formulas would therefore read: Using ISBLANK(Cell Reference):: There are several other ways to test if a cell is blank or not: =ArrayFormula(IF(ISBLANK(B2:B), “”, ROUND(B2:B18%, 2)))=ArrayFormula(IF(LEN(B2:B)&lt;&gt;0, ROUND(B2:B18%, 2), “”))=ArrayFormula(IF(B2:B=””, “”, ROUND(B2:B*18%, 2))) Use Array Formulas inside Column Headers: In our previous examples, the text of the column titles (like Tax, Total Amount) was pre-populated and the formulas were only added to the first row of the dataset. We can further improve our formula so that they can be applied to the column header itself. If the index of the current row is 1, calculated using the ROW() function, the formula outputs the column title else it performs the calculation using the formula. =ArrayFormula(IF(ROW(B:B)=1,”Tax”,IF(ISBLANK(B:B),”“,ROUND(B:B*18%, 2)))) Auto Fill Formulas into Google Form Submissions: ARRAYFORMULA functions are particularly useful for Google Forms when the form responses are getting saved inside a Google Sheet. You cannot do live calculations inside Google Forms but they can be performed inside the spreadsheet that is collecting the responses. You can create new columns inside the Google Spreadsheet and apply the ARRAYFORMULA to the first row of the added columns. When a new form submission is received, a new row would be added to the Google Sheet and the formulas would be cloned and automatically applied to the new rows without you have to copy-paste stuff. How to Use VLOOKUP inside ARRAYFORMULA: You can combine ARRAYFORMULA with VLOOKUP to quickly perform a lookup across an entire column. Say you have a “Fruits” sheet that lists the fruit names in column A and the corresponding prices in column B. The second sheet “Orders” has fruit names in column A, the quantity in column B and you are supposed to calculate the order amount in column C. =ArrayFormula(IF(ROW(A:A)=1,“Total”,IF(NOT(ISBLANK(A:A)), VLOOKUP(A:A, Fruits!A2:B6, 2, FALSE) * B:B, “”))) In simple English, if the row of the current cell is 1, output the column title in plain text. If the row is greater than 1 and the column A of the current row is not empty, perform a VLOOKUP to fetch the price of the item from the Fruits sheet. Then multiply that price with the quantity in cell B and output the value in cell C. If your VLOOKUP range is in another Google Spreadsheet, use the IMPORTRANGE() function with the ID of the other Google Sheet. Please note that you may have to use semicolons in the spreadsheet formulas instead of commas for some locales. "
    }, {
    "id": 79,
    "url": "https://bright-softwares.com/blog/fr/aws/amazons-t3-qui-doit-utiliser-quand-comment-et-pourquoi",
    "title": "Amazon’s T3 – Qui peut l'utiliser, quand, comment et pourquoi?",
    "body": "2020/02/12 -                                 Amazon’s T3 – Qui peut l'utiliser, quand, comment et pourquoi?                                       Published on August 23, 2018                                              Share this on                                                                                                                                                                                              Amazon just announced on August 21st a the third generation of the T series – T3. This article will look at: Comment fonctionnent les T3?Is T3 class an instance type to use in your EC2 arsenal list?If so, when does it make sense to enable T3?T2 standard got vastly misunderstood due to its CPU throttling over baseline, to which Amazon introduced T2 unlimited – with a way to overcome the CPU throttling with a pay for credit mechanism for the period the EC2 ran over the baseline. The new introduction – T3, is a T2 unlimited with some subtle variations and has use cases in your EC2 arsenal. Each T3 instances has a fixed set of memory and a baseline threshold specified by AWS. Let’s compare some of the attributes of T3 classes with that of T2. &nbsp;Notice how the number of vCPUs given at the lower ends has doubled for t3. nano, t3. micro, t3. small as compared to t2. nano, t2. micro, and t2. small. The pricing once you exceed the baseline threshold is at $0. 05 / CPU hour when bursting above baseline. So while AWS can make an unprecedented claim for the t3. nano can operate as low as $3. 796 / month, inherent in bursting above the baseline lies some unexpected price surprises for the naive user, who hasn’t understood how the T3 class works.  The baseline characteristics of T3 are similar to that of T2 unlimited in many ways. CPU utilization is measured at the millisecond level by AWS for this instance type. When your T3 class system is operating at levels below the baseline for that EC2, it is earning credits at an established rate; at baseline, neither credits are earned nor depleted; at thresholds over baseline, you are paying for the credits based on the number of vCPUs for that instance type.  T3. nano may be an alternative pitched at the low end of the hosting market where WordPress sites are hosted on shared platforms for $5-10 / month. If the usage is a handful of visitors a day, this may be sufficient. If you have some batch or a runaway process and exceed the baseline credits across 2 vCPUs, this is where the borrowed credits and payment kicks in. One CPU credit is equal to one vCPU running at 100% utilization for one minute. For example, one CPU credit is equal to one vCPU running at 50% utilization for two minutes, or two vCPUs running at 25% utilization for two minutes. T3 class earns credits when operating below baseline with a maximum credit level which is accrued for up to 7 days, unlike T2 unlimited which appears to have a daily limit.  CPU credits have a charge at $0. 05 per vCPU-Hour for Linux and at $0. 096 per vCPU-Hour for Windows. This is where the new smaller end T3. nano customer can find a surprise in their bill. Yes the t3. nano is only $3. 796 / month (without EBS storage and IP cost), but if one managed to run this instance at 100% utilization for a month the bill could be as high as $3. 976 + ($0. 05 * 2 * 24 * 30) = $75. 976 / month. If you aren’t able to monitor credits earned the lower end T3 class operating in the unlimited mode is bound to get a few people with bill shocks on expectation set at $4/month. At these surprise price points of $75 / month, there are far better instance types like the C5. large that has 2 vCPUs and 4GiBs (8 times more memory than the T3. nano) at $61 / month (Linux, US-East-1 region). A $4/month cost for T3. nano can turn as high as $76/month, whereas the C5. large has 8 times the memory T3. nano can run at $61/month. Tweet thisT2 standard, on the other hand, has the restriction similar to if a web server is hosted along with thousands of others, that the performance just gets throttled. Turning on T2 unlimited in such situations has the benefit in that you know your application isn’t getting throttled if you get a sudden unexpected burst. Taking out the surprise factor for the T3 at the smaller ends of the spectrum, at the 4, 8, 16, 32 GiB memory thresholds there is good price performance for this T3 class in general. The vast majority of the analysis the public cloud deployments we have seen, which Amazon would likely have done as well they would know that most EC2 instances are operating well under these baseline thresholds of 30-40% range. There are typical daily usage patterns (9-5 type) and weekend usage patterns (low relative to weekdays) for public cloud and general cloud deployments for a myriad of reasons where T3 has a better value proposition in many use cases. To understand the value proposition let’s look at the cost/vCPU for memory for Linux and Microsoft. For these prices, I am using pricing as of this date for the US-East-1 region, and for the most part, the relative price ratios hold across regions: Notice first the burst price point credit price – this is the price for borrowed CPU credits from Amazon when earned credits are exhausted. At the lower end of the GB spectrum (under 4GB) the choices in other classes are limited. The options available are is an older M class (m1. small) system and C5 class that have higher price points. If your memory requirements are at 0. 5, 1, 2 GB range and your workload can stay for the most part inside the baseline range T2 or T3 are good choices. If price predictability is more important than performance consider T2 standard. If performance is more important than price T2 unlimited and T3 may be good in the short term. If you are consistently exhausting credits earned and borrowing credits to maintain your system performance then M1. small (1vCPU, 1. 7GB) and C5. large(2vCPU, 4GB) become systems to consider over the T3 or T2 unlimited. If price predictability is more important than performance for an application needing 0. 5, 1, 2 GB of memory consider the T2 standard. If performance is important over price and you are relatively certain the utilization is well under the 20% baseline then consider T3 or T2 unlimited. If performance is important relative to price variability then consider m1. small or c5. large. Tweet thisThe price for Microsoft burst credits is at $0. 096/vCPU hour. It has similar patterns to Unix in that under 4GB memory T3 class is an excellent choice. In both, the above graphs note the clusters around 4GB, 8GB, 16GB, and 32GB. The close cluster offerings come from newer generation C5 and M5 systems. Let’s look at these prices relative to each other and include a 1-year convertible no cash upfront option. The reason to consider convertible reservation type is it gives you more choices – example if during the term of the agreement T4 type comes up which is likely at a far lower cost with a performance enhancement – it has to do with the nature of technology! If you look at the first generation or even second generation classes of C, M and if you purchased the standard 3-year reservation, vs. cost savings on C5, M5 – you will likely be better off on newer class system types from performance enhancements and cost savings inherent in these newer generation systems. The convertible reservation type gives you more choices than the standard reservation to change as technology changes occur in the cloud. Tweet this At the 4GB threshold, if your baseline usage is in the 20% range then T3 is a great choice. In the C class, the C5. large is perhaps the only choice as in the M5 class the lowest end starts at 8GB of memory. If your T3 instance starts borrowing credits frequently the performance of C5 class turns out to be almost double the price (103%) increase. At the 4GB memory level, the cost savings you get from T3. medium is significant even if you are bursting ocassionally compared to a C5 instance type, that is the closest in performance value combination. Tweet thisAt 8,16, 32 GB of memory, if you are relatively certain that your baseline performance is in the 30-40% range T3, is a good bet. But with C5 and M5 operating at 1. 6% higher than the T3 price point when you get fractions of CPU isn’t all that attractive! Most systems deployed in the AWS cloud rarely sustain 30-40% utilization consistently over a 24 hour period. However, if you haven’t actively monitored CPU utilization patterns and performance is important, the risk with T3 and unlimited turned on as default is the possibility of a surprise element of cost relative to a C5 or M5 instance type that has overall good performance and value. At 8,16, 32GB memory usage levels, T3 family isnt an attractive choice if your application utilization baseline is over the 30-40% levels. Most applications rarely sustain these thresholds consistently over a 24 hour period. If performance is important at these memory thresholds the C5 and M5 systems provide a good overall value proposition relative to the T3 family. Tweet thisOn the readers posted a survey response on the ways to overcome this default setting and how to run T3 in prior standard mode that I would like to address as an addendum to this initial post. Yes, running a T3 in standard mode overcomes this potential issue of an unexpected bill when credits needed are charged. The downside of this is the cost surprise if you are not looking at CloudWatch logs on CPU credits earned, consumed specific to the T2/T3 class. In the EC2 console you should find a change T2/T3 unlimited setting. The default on T3 is enabled state that can be toggled easily: If you are changing a T2 instance to T3 ensure the ENA requirement is met on the T3. This is referenced in greater detail under the AWS tutorials on how to change an EC2 instance type. &nbsp; You can change most instance types to the T class, and specifically how to change an instance type to T2 is described, and the steps are nearly the same, just select T3. &nbsp;If you attempt to change the instance type without this ENA requirement met, you will see an error like this below (do not panic): In summary, T3 instance family with its default behavior similar to that of T2 unlimited is likely to surprise some who haven’t looked at application utilization characteristics of memory and CPU. If you use T3, ensure you are actively monitoring CPU utilization, earned credits and borrowed credits from CloudWatch. The T2 standard type may be something to consider when CPU throttling is an acceptable way to limit application anomalies – example rouge run away code loops that consume CPU unexpectedly or some malware installs change the way CPU is consumed. If performance is relatively important the C5 and M5 families can be better choices over the newly introduced T3 class. The T3 class while it has a low price point, the default behavior of unlimited burst credits along with more vCPUs over T2 has the potential to surprise someone expecting a $4/month bill to turn to $76/month (Windows system to cost additional over Linux).                                                               Please enable JavaScript to view the comments powered by Disqus.         "
    }, {
    "id": 80,
    "url": "https://bright-softwares.com/blog/en/data-analysis/how-to-copy-a-formula-down-an-entire-column-in-google-sheets",
    "title": "How to Copy a Formula Down an Entire Column in Google Sheets",
    "body": "2020/02/12 - You are working inside a Google Spreadsheet where a formula needs to copied down to the last row of the sheet. You also need the formula to be added automatically when a new row is added to the Google Sheet. There are several ways to solve this problem. Copy Formula Down in Google SheetsThe easiest approach to copy down formulas is to use the fill handle in Google Sheets. Write your formula in the first row of your spreadsheet, and then point your mouse to the lower right corner of the formula cell. The pointer changes into a fill handle (black plus symbol) that you can drag to the last row of the sheet. The fill handle will not just copy down the formulas to all the adjacent cells but also copies the visual formatting. If you need to copy the formulas across cells but sans any formatting, select the cell that contains the formatting and press Ctrl+C to copy it to the clipboard. Next, select the range where that formula needs to applied, right-click, choose Paste Special and Paste Formula only. Apply Formula to the Entire Column in Google SheetsIf you have hundreds of rows in a Google Spreadsheet and you want to apply the same formula to all rows of a particular column, there’s a more efficient solution than copy-paste - Array Formulas. Highlight the first cell in the column and type the formula as earlier. However, instead of specifying a single cell as a parameter, we’ll specify the entire column using the B2:B notation (start from cell B2 and go all the way down to the last row of column B). Then press Ctrl+Shift+Enter, or Cmd+Shift+Enter on Mac, and Google Sheets will automatically surround your formula with ARRAYFORMULA function. Thus, we could apply the formula to the entire column of the spreadsheet with only a single cell. Array Formulas are more efficient as they process a batch of rows in one go. They are also easier to maintain as you only need to modify a single cell to edit the formula. One issue that you may have noticed with the above formulae is that it applies to every row in the column where you have only want to add formulas to rows that contain data and skip the blank rows. This can be done by adding an IF contain to our ARRAYFORMULA so that it doesn’t apply the formula the any of the blank rows. Google Spreadsheet offers two functions to help test whether a cell is empty or now. ISBLANK(A1) - Returns TRUE if the referenced cell is empty. LEN(A1) &lt;&gt; 0 - Returns TRUE if the referenced cell not empty, FALSE otherwiseOur modified Array Formulas would therefore read: Using ISBLANK(Cell Reference):: There are several other ways to test if a cell is blank or not: =ArrayFormula(IF(ISBLANK(B2:B), “”, ROUND(B2:B18%, 2)))=ArrayFormula(IF(LEN(B2:B)&lt;&gt;0, ROUND(B2:B18%, 2), “”))=ArrayFormula(IF(B2:B=””, “”, ROUND(B2:B*18%, 2))) Use Array Formulas inside Column Headers: In our previous examples, the text of the column titles (like Tax, Total Amount) was pre-populated and the formulas were only added to the first row of the dataset. We can further improve our formula so that they can be applied to the column header itself. If the index of the current row is 1, calculated using the ROW() function, the formula outputs the column title else it performs the calculation using the formula. =ArrayFormula(IF(ROW(B:B)=1,”Tax”,IF(ISBLANK(B:B),”“,ROUND(B:B*18%, 2)))) Auto Fill Formulas into Google Form Submissions: ARRAYFORMULA functions are particularly useful for Google Forms when the form responses are getting saved inside a Google Sheet. You cannot do live calculations inside Google Forms but they can be performed inside the spreadsheet that is collecting the responses. You can create new columns inside the Google Spreadsheet and apply the ARRAYFORMULA to the first row of the added columns. When a new form submission is received, a new row would be added to the Google Sheet and the formulas would be cloned and automatically applied to the new rows without you have to copy-paste stuff. How to Use VLOOKUP inside ARRAYFORMULA: You can combine ARRAYFORMULA with VLOOKUP to quickly perform a lookup across an entire column. Say you have a “Fruits” sheet that lists the fruit names in column A and the corresponding prices in column B. The second sheet “Orders” has fruit names in column A, the quantity in column B and you are supposed to calculate the order amount in column C. =ArrayFormula(IF(ROW(A:A)=1,“Total”,IF(NOT(ISBLANK(A:A)), VLOOKUP(A:A, Fruits!A2:B6, 2, FALSE) * B:B, “”))) In simple English, if the row of the current cell is 1, output the column title in plain text. If the row is greater than 1 and the column A of the current row is not empty, perform a VLOOKUP to fetch the price of the item from the Fruits sheet. Then multiply that price with the quantity in cell B and output the value in cell C. If your VLOOKUP range is in another Google Spreadsheet, use the IMPORTRANGE() function with the ID of the other Google Sheet. Please note that you may have to use semicolons in the spreadsheet formulas instead of commas for some locales. "
    }, {
    "id": 81,
    "url": "https://bright-softwares.com/blog/en/aws/amazons-t3-who-should-use-it-when-how-and-the-why",
    "title": "Amazon’s T3 – Who should use it, when, how and the why?",
    "body": "2020/02/12 -                                 Amazon’s T3 – Who should use it, when, how and the why?                                       Published on August 23, 2018                                              Share this on                                                                                                                                                                                              Amazon just announced on August 21st a the third generation of the T series – T3. This article will look at: How does the T3 class works?Is T3 class an instance type to use in your EC2 arsenal list?If so, when does it make sense to enable T3?T2 standard got vastly misunderstood due to its CPU throttling over baseline, to which Amazon introduced T2 unlimited – with a way to overcome the CPU throttling with a pay for credit mechanism for the period the EC2 ran over the baseline. The new introduction – T3, is a T2 unlimited with some subtle variations and has use cases in your EC2 arsenal. Each T3 instances has a fixed set of memory and a baseline threshold specified by AWS. Let’s compare some of the attributes of T3 classes with that of T2. &nbsp;Notice how the number of vCPUs given at the lower ends has doubled for t3. nano, t3. micro, t3. small as compared to t2. nano, t2. micro, and t2. small. The pricing once you exceed the baseline threshold is at $0. 05 / CPU hour when bursting above baseline. So while AWS can make an unprecedented claim for the t3. nano can operate as low as $3. 796 / month, inherent in bursting above the baseline lies some unexpected price surprises for the naive user, who hasn’t understood how the T3 class works.  The baseline characteristics of T3 are similar to that of T2 unlimited in many ways. CPU utilization is measured at the millisecond level by AWS for this instance type. When your T3 class system is operating at levels below the baseline for that EC2, it is earning credits at an established rate; at baseline, neither credits are earned nor depleted; at thresholds over baseline, you are paying for the credits based on the number of vCPUs for that instance type.  T3. nano may be an alternative pitched at the low end of the hosting market where WordPress sites are hosted on shared platforms for $5-10 / month. If the usage is a handful of visitors a day, this may be sufficient. If you have some batch or a runaway process and exceed the baseline credits across 2 vCPUs, this is where the borrowed credits and payment kicks in. One CPU credit is equal to one vCPU running at 100% utilization for one minute. For example, one CPU credit is equal to one vCPU running at 50% utilization for two minutes, or two vCPUs running at 25% utilization for two minutes. T3 class earns credits when operating below baseline with a maximum credit level which is accrued for up to 7 days, unlike T2 unlimited which appears to have a daily limit.  CPU credits have a charge at $0. 05 per vCPU-Hour for Linux and at $0. 096 per vCPU-Hour for Windows. This is where the new smaller end T3. nano customer can find a surprise in their bill. Yes the t3. nano is only $3. 796 / month (without EBS storage and IP cost), but if one managed to run this instance at 100% utilization for a month the bill could be as high as $3. 976 + ($0. 05 * 2 * 24 * 30) = $75. 976 / month. If you aren’t able to monitor credits earned the lower end T3 class operating in the unlimited mode is bound to get a few people with bill shocks on expectation set at $4/month. At these surprise price points of $75 / month, there are far better instance types like the C5. large that has 2 vCPUs and 4GiBs (8 times more memory than the T3. nano) at $61 / month (Linux, US-East-1 region). A $4/month cost for T3. nano can turn as high as $76/month, whereas the C5. large has 8 times the memory T3. nano can run at $61/month. Tweet thisT2 standard, on the other hand, has the restriction similar to if a web server is hosted along with thousands of others, that the performance just gets throttled. Turning on T2 unlimited in such situations has the benefit in that you know your application isn’t getting throttled if you get a sudden unexpected burst. Taking out the surprise factor for the T3 at the smaller ends of the spectrum, at the 4, 8, 16, 32 GiB memory thresholds there is good price performance for this T3 class in general. The vast majority of the analysis the public cloud deployments we have seen, which Amazon would likely have done as well they would know that most EC2 instances are operating well under these baseline thresholds of 30-40% range. There are typical daily usage patterns (9-5 type) and weekend usage patterns (low relative to weekdays) for public cloud and general cloud deployments for a myriad of reasons where T3 has a better value proposition in many use cases. To understand the value proposition let’s look at the cost/vCPU for memory for Linux and Microsoft. For these prices, I am using pricing as of this date for the US-East-1 region, and for the most part, the relative price ratios hold across regions: Notice first the burst price point credit price – this is the price for borrowed CPU credits from Amazon when earned credits are exhausted. At the lower end of the GB spectrum (under 4GB) the choices in other classes are limited. The options available are is an older M class (m1. small) system and C5 class that have higher price points. If your memory requirements are at 0. 5, 1, 2 GB range and your workload can stay for the most part inside the baseline range T2 or T3 are good choices. If price predictability is more important than performance consider T2 standard. If performance is more important than price T2 unlimited and T3 may be good in the short term. If you are consistently exhausting credits earned and borrowing credits to maintain your system performance then M1. small (1vCPU, 1. 7GB) and C5. large(2vCPU, 4GB) become systems to consider over the T3 or T2 unlimited. If price predictability is more important than performance for an application needing 0. 5, 1, 2 GB of memory consider the T2 standard. If performance is important over price and you are relatively certain the utilization is well under the 20% baseline then consider T3 or T2 unlimited. If performance is important relative to price variability then consider m1. small or c5. large. Tweet thisThe price for Microsoft burst credits is at $0. 096/vCPU hour. It has similar patterns to Unix in that under 4GB memory T3 class is an excellent choice. In both, the above graphs note the clusters around 4GB, 8GB, 16GB, and 32GB. The close cluster offerings come from newer generation C5 and M5 systems. Let’s look at these prices relative to each other and include a 1-year convertible no cash upfront option. The reason to consider convertible reservation type is it gives you more choices – example if during the term of the agreement T4 type comes up which is likely at a far lower cost with a performance enhancement – it has to do with the nature of technology! If you look at the first generation or even second generation classes of C, M and if you purchased the standard 3-year reservation, vs. cost savings on C5, M5 – you will likely be better off on newer class system types from performance enhancements and cost savings inherent in these newer generation systems. The convertible reservation type gives you more choices than the standard reservation to change as technology changes occur in the cloud. Tweet this At the 4GB threshold, if your baseline usage is in the 20% range then T3 is a great choice. In the C class, the C5. large is perhaps the only choice as in the M5 class the lowest end starts at 8GB of memory. If your T3 instance starts borrowing credits frequently the performance of C5 class turns out to be almost double the price (103%) increase. At the 4GB memory level, the cost savings you get from T3. medium is significant even if you are bursting ocassionally compared to a C5 instance type, that is the closest in performance value combination. Tweet thisAt 8,16, 32 GB of memory, if you are relatively certain that your baseline performance is in the 30-40% range T3, is a good bet. But with C5 and M5 operating at 1. 6% higher than the T3 price point when you get fractions of CPU isn’t all that attractive! Most systems deployed in the AWS cloud rarely sustain 30-40% utilization consistently over a 24 hour period. However, if you haven’t actively monitored CPU utilization patterns and performance is important, the risk with T3 and unlimited turned on as default is the possibility of a surprise element of cost relative to a C5 or M5 instance type that has overall good performance and value. At 8,16, 32GB memory usage levels, T3 family isnt an attractive choice if your application utilization baseline is over the 30-40% levels. Most applications rarely sustain these thresholds consistently over a 24 hour period. If performance is important at these memory thresholds the C5 and M5 systems provide a good overall value proposition relative to the T3 family. Tweet thisOn the readers posted a survey response on the ways to overcome this default setting and how to run T3 in prior standard mode that I would like to address as an addendum to this initial post. Yes, running a T3 in standard mode overcomes this potential issue of an unexpected bill when credits needed are charged. The downside of this is the cost surprise if you are not looking at CloudWatch logs on CPU credits earned, consumed specific to the T2/T3 class. In the EC2 console you should find a change T2/T3 unlimited setting. The default on T3 is enabled state that can be toggled easily: If you are changing a T2 instance to T3 ensure the ENA requirement is met on the T3. This is referenced in greater detail under the AWS tutorials on how to change an EC2 instance type. &nbsp; You can change most instance types to the T class, and specifically how to change an instance type to T2 is described, and the steps are nearly the same, just select T3. &nbsp;If you attempt to change the instance type without this ENA requirement met, you will see an error like this below (do not panic): In summary, T3 instance family with its default behavior similar to that of T2 unlimited is likely to surprise some who haven’t looked at application utilization characteristics of memory and CPU. If you use T3, ensure you are actively monitoring CPU utilization, earned credits and borrowed credits from CloudWatch. The T2 standard type may be something to consider when CPU throttling is an acceptable way to limit application anomalies – example rouge run away code loops that consume CPU unexpectedly or some malware installs change the way CPU is consumed. If performance is relatively important the C5 and M5 families can be better choices over the newly introduced T3 class. The T3 class while it has a low price point, the default behavior of unlimited burst credits along with more vCPUs over T2 has the potential to surprise someone expecting a $4/month bill to turn to $76/month (Windows system to cost additional over Linux).                                                               Please enable JavaScript to view the comments powered by Disqus.         "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});